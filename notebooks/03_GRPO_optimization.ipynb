{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L637dg8aVl1t"
   },
   "source": [
    "# Step 1: Mounting Google Drive and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25689,
     "status": "ok",
     "timestamp": 1766857703396,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Qzxre_ImVbHp",
    "outputId": "3dd8d125-1b45-408c-e519-b8ce21c0d56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/grpo-verified-reasoner\n",
      "data\t\t\t      notebooks  unsloth_compiled_cache\n",
      "huggingface_tokenizers_cache  outputs\t _unsloth_sentencepiece_temp\n",
      "LICENSE\t\t\t      README.md\n",
      "models\t\t\t      src\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd /content/drive/MyDrive/grpo-verified-reasoner\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3IoN0USiBmR"
   },
   "outputs": [],
   "source": [
    "# Install UV (Faster pip)\n",
    "!pip install --upgrade -qqq uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8iXM_15a_mn"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U evalplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1766857719298,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Oq8Azhmyiu6E"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1766857719348,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "lv-Ki6Vcrntd"
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1766857719360,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "NeqIAhUsd5Dg"
   },
   "outputs": [],
   "source": [
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1766857719373,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "kTXAQ4pB0NsE"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"mbpp-rl-project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38172,
     "status": "ok",
     "timestamp": 1766857757550,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "ZuucNkH5imSg",
    "outputId": "c12f0779-8c72-4e60-8efb-8222b907d0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m18 packages\u001b[0m \u001b[2min 24ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 435ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 312ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 50ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.24.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.22.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Environment Logic (Colab vs Local)\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # Version Matching\n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    try: is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "    except: is_t4 = False\n",
    "\n",
    "    # A100 gets vllm 0.10.2 (Fast), T4 gets 0.9.2 (Stable)\n",
    "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
    "\n",
    "    # Install Everything\n",
    "    !uv pip install -qqq --upgrade \\\n",
    "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
    "    !uv pip install -qqq {get_triton}\n",
    "\n",
    "# Install TRL\n",
    "!uv pip install transformers==4.56.2\n",
    "!uv pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73340,
     "status": "ok",
     "timestamp": 1766857831059,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "HPDizgKUc3Fx",
    "outputId": "c34ce002-653d-4c86-8050-64e4d888cd1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py:2525: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  * regex for parameter names, must start with `re:`, e.g. `re:language\\.layers\\..+\\.q_proj.weight`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-27 17:49:51 [__init__.py:216] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import evalplus\n",
    "import traceback\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from evalplus.data import get_mbpp_plus\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from vllm import SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiKxGqbQx_r6"
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bFALRzXVpfD"
   },
   "source": [
    "# Step 2: Verifying GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1766857904388,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "j6gsvZOiavZf",
    "outputId": "2718adac-8688-4ed8-9be3-f8c2dcb38145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-PeyUR5dE0m"
   },
   "source": [
    "# Step 3: Loading Base Model and LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766857906975,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "OGf88PMBdN3B"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models/qwen3-4b-sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLoe3PBmsHXz"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_PATH,\n",
    "    max_seq_length = 3072,      # Aligned with GRPO + schema\n",
    "    load_in_4bit = False,       # Full precision for RL stability\n",
    "    fast_inference = True,      # Required for vLLM\n",
    "    gpu_memory_utilization = 0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766858517643,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "3wgYFoDcv283",
    "outputId": "04d3f505-637d-4c00-9811-2cef54efad75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen3ForCausalLM(\n",
      "      (model): Qwen3Model(\n",
      "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
      "        (layers): ModuleList(\n",
      "          (0-35): 36 x Qwen3DecoderLayer(\n",
      "            (self_attn): Qwen3Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen3MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AtswtDWt3T4"
   },
   "source": [
    "# Step 4: Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766858520968,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "pt0hmcEetnJ7"
   },
   "outputs": [],
   "source": [
    "# This is the same prompt that we used during SFT\n",
    "system_prompt = \"\"\"You are a code-generation engine.\n",
    "You must output your response in the following exact format:\n",
    "<START_WORKING_OUT>\n",
    "Concise reasoning steps required to solve the problem.\n",
    "</END_WORKING_OUT>\n",
    "<SOLUTION>\n",
    "Valid Python code only.\n",
    "</SOLUTION>\n",
    "Do not output anything outside these tags.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766858521771,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "FnsS5mubtnGM"
   },
   "outputs": [],
   "source": [
    "user_prompt = \"Write a Python function that returns the factorial of a number.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858522207,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "9buanb82vgtZ"
   },
   "outputs": [],
   "source": [
    "# Move the dictionary to GPU manually\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 9086,
     "status": "ok",
     "timestamp": 1766858531496,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "m1gvTQsdtnEa"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Temporarily enable inference mode for the test\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.0, # Deterministic check\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858531499,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "xglp4DortnCS"
   },
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1766858531502,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zxb_myK-tnAO",
    "outputId": "a661534d-5fc3-4240-8850-0bdcbb4ec3cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MODEL OUTPUT ---\n",
      "<START_WORKING_OUT>\n",
      "Define a function factorial(n) that calculates the product of all positive integers up to n.\n",
      "Handle non-positive integers by returning 1 (factorial of 0 and negative is 1).\n",
      "Implement iterative approach for efficiency.\n",
      "Return the computed factorial.\n",
      "</END_WORKING_OUT>\n",
      "<SOLUTION>\n",
      "def factorial(n):\n",
      "    if n <= 0:\n",
      "        return 1\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "</SOLUTION>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- MODEL OUTPUT ---\")\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "print(tokenizer.decode(output[0][input_len:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8MkD65mYBx5"
   },
   "source": [
    "Comment:  No schema check, extractor, or reward function ever sees the full decoded sequence. They only ever see generated_text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_hw6md1LSm3"
   },
   "source": [
    "# Step 6: Defining Output Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858533676,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "O3zR1mkFtm-N"
   },
   "outputs": [],
   "source": [
    "# Regular expressions for tag validation (case-insensitive)\n",
    "RE_START = re.compile(r\"<START_WORKING_OUT>\", re.IGNORECASE)\n",
    "RE_END   = re.compile(r\"</END_WORKING_OUT>\", re.IGNORECASE)\n",
    "RE_SOL   = re.compile(r\"<SOLUTION>\", re.IGNORECASE)\n",
    "RE_SOL_END = re.compile(r\"</SOLUTION>\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858534054,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "5oxttpUDtm0G"
   },
   "outputs": [],
   "source": [
    "def validate_schema(text: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Checks whether the model output follows the exact required schema.\n",
    "    Returns (is_valid, reason).\n",
    "    \"\"\"\n",
    "    if not RE_START.search(text):\n",
    "        return False, \"Missing <START_WORKING_OUT>\"\n",
    "    if not RE_END.search(text):\n",
    "        return False, \"Missing </END_WORKING_OUT>\"\n",
    "    if not RE_SOL.search(text):\n",
    "        return False, \"Missing <SOLUTION>\"\n",
    "    if not RE_SOL_END.search(text):\n",
    "        return False, \"Missing </SOLUTION>\"\n",
    "\n",
    "    # Optional: check order consistency\n",
    "    start_idx = RE_START.search(text).start()\n",
    "    sol_idx   = RE_SOL.search(text).start()\n",
    "    if sol_idx < start_idx:\n",
    "        return False, \"Tag order incorrect (<SOLUTION> before reasoning block).\"\n",
    "\n",
    "    return True, \"Schema valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1766858534271,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "YA_UNFQgLrF0",
    "outputId": "ee3bf1fa-9426-4e44-ca7b-9ea30ab0f661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Check: True | Schema valid\n"
     ]
    }
   ],
   "source": [
    "# Run a sanity test using the previous decoded output\n",
    "is_valid, reason = validate_schema(decoded)\n",
    "print(\"Schema Check:\", is_valid, \"|\", reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EN-nTksMUff"
   },
   "source": [
    "# Step 7: Solution Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858535825,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Iv-5aUVNL3aC"
   },
   "outputs": [],
   "source": [
    "# Regex to extract the code block between <SOLUTION> ... </SOLUTION>\n",
    "RE_SOLUTION = re.compile(r\"<SOLUTION>\\s*(.*?)\\s*</SOLUTION>\", re.IGNORECASE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858536225,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "nlcNKSrIMjBy"
   },
   "outputs": [],
   "source": [
    "def extract_solution(text: str) -> tuple[str | None, str]:\n",
    "    \"\"\"\n",
    "    Extracts the Python code inside <SOLUTION> tags.\n",
    "    Returns (code, status) where:\n",
    "        code   -> the extracted string or None if failed\n",
    "        status -> textual reason (for debugging)\n",
    "    \"\"\"\n",
    "    match = RE_SOLUTION.search(text)\n",
    "    if not match:\n",
    "        return None, \"No <SOLUTION> block found.\"\n",
    "\n",
    "    code = match.group(1).strip()\n",
    "    if not code:\n",
    "        return None, \"Empty <SOLUTION> block.\"\n",
    "\n",
    "    # Syntax check via Python's AST parser\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        return None, f\"Syntax error in code: {e}\"\n",
    "\n",
    "    return code, \"Valid Python code extracted.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1766858536419,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "hgaK_ceNV4Le"
   },
   "outputs": [],
   "source": [
    "# Calculate where the prompt ends\n",
    "input_len = inputs[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858536604,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "8hwepazlV-Ho"
   },
   "outputs": [],
   "source": [
    "# Decode ONLY the new tokens (The Assistant's reply)\n",
    "generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1766858536842,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "CKKZsiDRMnKy",
    "outputId": "f2dc3103-a186-413f-c958-7a6f117b7671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Valid Python code extracted.\n"
     ]
    }
   ],
   "source": [
    "# Now run the check on ONLY the generated text\n",
    "code, status = extract_solution(generated_text) # Use the new variable\n",
    "print(\"Status:\", status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1766858536990,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "G8DcMGzTMtQ0",
    "outputId": "81602031-de5f-44e1-c830-966d4c9b92fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extracted Python Code ---\n",
      "\n",
      "def factorial(n):\n",
      "    if n <= 0:\n",
      "        return 1\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    return result\n"
     ]
    }
   ],
   "source": [
    "# Show snippet of the extracted code\n",
    "if code:\n",
    "    print(\"\\n--- Extracted Python Code ---\\n\")\n",
    "    print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1urFQMgaulu"
   },
   "source": [
    "# Step 8: Verifier Integration (EvalPlus MBPP+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1766858539040,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "dYJ4b4wiN-PR",
    "outputId": "614624e2-1e11-452e-ba94-33d2fa5d0028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from https://github.com/evalplus/mbppplus_release/releases/download/v0.2.0/MbppPlus.jsonl.gz\n",
      "Loaded MBPP+ tasks: 378\n"
     ]
    }
   ],
   "source": [
    "# Load MBPP+ tasks as a dict: {task_id: problem_dict}\n",
    "MBPP_TASKS = get_mbpp_plus()\n",
    "\n",
    "print(f\"Loaded MBPP+ tasks: {len(MBPP_TASKS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1766858539055,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "A3fpg80PbTlc",
    "outputId": "5f3669a2-7a80-41f1-dc3f-ba01f170f57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Task ID: Mbpp/2\n",
      "Keys: ['task_id', 'prompt', 'entry_point', 'canonical_solution', 'base_input', 'atol', 'plus_input', 'contract', 'assertion']\n",
      "\n",
      "Prompt (first 400 chars):\n",
      " \"\"\"\n",
      "Write a function to find the shared elements from the given two lists.\n",
      "assert set(similar_elements((3, 4, 5, 6),(5, 7, 4, 10))) == set((4, 5))\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick peek at one task to confirm fields & shape\n",
    "sample_task_id = next(iter(MBPP_TASKS.keys()))\n",
    "sample_task = MBPP_TASKS[sample_task_id]\n",
    "\n",
    "print(\"\\nSample Task ID:\", sample_task_id)\n",
    "print(\"Keys:\", list(sample_task.keys()))\n",
    "print(\"\\nPrompt (first 400 chars):\\n\", sample_task[\"prompt\"][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858539061,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "RZsHYa9wbdUH"
   },
   "outputs": [],
   "source": [
    "# Different EvalPlus versions may store tests under slightly different keys,\n",
    "# so we normalize via a helper (used later in reward function).\n",
    "def get_tests_from_task(task: dict) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts MBPP test assertions from a task.\n",
    "    Supports both list-based and string-based formats.\n",
    "    \"\"\"\n",
    "    # Case 1: already a list of assertions\n",
    "    for k in (\"test_list\", \"tests\", \"plus_tests\", \"base_tests\"):\n",
    "        if k in task and task[k]:\n",
    "            return list(task[k])\n",
    "\n",
    "    # Case 2: single multiline assertion string (MBPP+ common case)\n",
    "    if \"assertion\" in task and task[\"assertion\"]:\n",
    "        lines = task[\"assertion\"].strip().splitlines()\n",
    "        return [line for line in lines if line.strip()]\n",
    "\n",
    "    raise KeyError(f\"No tests found in task keys: {list(task.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6wcAKaBfVwn"
   },
   "source": [
    "# Step 9: Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858539137,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "ugV0PZjq6WS0"
   },
   "outputs": [],
   "source": [
    "def _exec_code_and_tests_worker(code: str, tests: list[str], queue: mp.Queue) -> None:\n",
    "    \"\"\"\n",
    "    Runs model code + tests in a shared environment.\n",
    "    Fixes import errors and reports specific test failures.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the \"Main Desk\" (Environment)\n",
    "        env = {\"__builtins__\": __builtins__}\n",
    "\n",
    "        # Run the User's Code into 'env'\n",
    "        # We pass 'env' twice so it acts as both Globals and Locals\n",
    "        exec(code, env, env)\n",
    "\n",
    "        # Run the Test Cases\n",
    "        for t in tests:\n",
    "            try:\n",
    "                # Run the test using that same desk\n",
    "                exec(t, env, env)\n",
    "            except AssertionError:\n",
    "                # If a test fails, tell us WHICH one\n",
    "                queue.put((False, f\"Failed assertion: {t}\"))\n",
    "                return\n",
    "\n",
    "        # If we finish the loop, all tests passed\n",
    "        queue.put((True, None))\n",
    "\n",
    "    except Exception:\n",
    "        # Catch any other crashes (syntax errors, etc.)\n",
    "        queue.put((False, traceback.format_exc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766858539790,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "7qvlGFIjfpRV"
   },
   "outputs": [],
   "source": [
    "def run_mbpp_tests(code: str, task: dict, timeout_s: float = 2.0) -> tuple[bool, str | None]:\n",
    "    \"\"\"\n",
    "    Executes MBPP tests for a given task in a subprocess with timeout.\n",
    "    Returns (passed, error_str).\n",
    "    \"\"\"\n",
    "    tests = get_tests_from_task(task)\n",
    "\n",
    "    ctx = mp.get_context(\"fork\")  # Colab/Linux: fork is fastest & simplest\n",
    "    q = ctx.Queue()\n",
    "    p = ctx.Process(target=_exec_code_and_tests_worker, args=(code, tests, q))\n",
    "    p.start()\n",
    "    p.join(timeout_s)\n",
    "\n",
    "    if p.is_alive():\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "        return False, f\"Timeout after {timeout_s:.1f}s\"\n",
    "\n",
    "    if q.empty():\n",
    "        return False, \"No result returned from worker.\"\n",
    "\n",
    "    passed, err = q.get()\n",
    "    return passed, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlE3bAZ3LjwO"
   },
   "source": [
    "# Step 10: Defining Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858540847,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "q5SEd65CLneK"
   },
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Rewards the model for strictly following the XML schema.\n",
    "    Args:\n",
    "        completions: List of generated strings from the model.\n",
    "    Returns:\n",
    "        List of rewards (0.1 for valid schema, 0.0 for invalid).\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        # Uses your existing validator from Step 6\n",
    "        is_valid, _ = validate_schema(completion)\n",
    "        rewards.append(0.1 if is_valid else 0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858541050,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "bfVxKTg9L6cd"
   },
   "outputs": [],
   "source": [
    "def reasoning_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Rewards the model for generating a detailed reasoning block.\n",
    "    Uses a \"soft length\" penalty to encourage thinking without spamming.\n",
    "    Args:\n",
    "        completions: List of generated strings from the model.\n",
    "    Returns:\n",
    "        List of rewards (0.0 to 0.2, scaled by length of reasoning).\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        # Regex to find the reasoning block specifically\n",
    "        match = re.search(r\"<START_WORKING_OUT>(.*?)</END_WORKING_OUT>\", completion, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            reasoning_content = match.group(1).strip()\n",
    "            # Soft Length Reward: Cap at 0.2 for ~500 characters\n",
    "            # This incentivizes \"thinking\" without encouraging infinite spam\n",
    "            length = len(reasoning_content)\n",
    "            score = min(0.2, (length / 1000.0) * 0.2)\n",
    "            rewards.append(score)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766858541282,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "hLNU6pML0gqN"
   },
   "outputs": [],
   "source": [
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Rewards the model for writing code that passes the actual unit tests.\n",
    "    Args:\n",
    "        prompts: The prompts fed to the model.\n",
    "        completions: The model's generated answers.\n",
    "        answer: The ground-truth data (Expected to be the MBPP task dict).\n",
    "    Returns:\n",
    "        List of rewards (1.0 for passing tests, 0.0 for failing).\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for prompt, completion, task_data in zip(prompts, completions, answer):\n",
    "        code, status = extract_solution(completion)\n",
    "        if not code:\n",
    "            rewards.append(0.0)\n",
    "            # Debug: Log extraction failures\n",
    "            # print(f\"[Debug] Extract Failed: {status}\")\n",
    "            continue\n",
    "\n",
    "        passed, err = run_mbpp_tests(code, task_data)\n",
    "\n",
    "        if passed:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "            # CRITICAL: Print the error for the user to see!\n",
    "            # We only print the first few chars to avoid spamming the logs\n",
    "            print(f\"\\n[FAIL] Task: {task_data.get('task_id', 'Unknown')}\")\n",
    "            print(f\"Error: {err}\")\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI6Eut_JXUZW"
   },
   "source": [
    "# Step 11: Dataset Formatting and Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858542121,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "jJF74XFesHNc"
   },
   "outputs": [],
   "source": [
    "# Clean the Data\n",
    "# The raw MBPP+ dataset has inconsistent schemas (some fields are lists, some are None).\n",
    "# We fix this by extracting ONLY what we need: the test cases.\n",
    "dict_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1766858542363,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Fx2h97Kfp9sf"
   },
   "outputs": [],
   "source": [
    "for task_id, task_data in MBPP_TASKS.items():\n",
    "    # Extract the test cases using our helper from Step 8\n",
    "    # This handles the \"messy\" parsing right now, so the Dataset is clean.\n",
    "    try:\n",
    "        tests = get_tests_from_task(task_data)\n",
    "    except KeyError:\n",
    "        # If a task is broken/empty, skip it to prevent crashes\n",
    "        print(f\"Skipping task {task_id}: No tests found.\")\n",
    "        continue\n",
    "\n",
    "    # Create a CLEAN 'answer' dictionary\n",
    "    # This guarantees every row has the exact same structure.\n",
    "    # This prevents the \"ArrowInvalid\" error.\n",
    "    clean_answer = {\n",
    "        \"task_id\": str(task_id),\n",
    "        \"test_list\": tests  # Always a List of Strings\n",
    "    }\n",
    "\n",
    "    # Append to our list\n",
    "    dict_data.append({\n",
    "        \"prompt\": task_data[\"prompt\"],\n",
    "        \"answer\": clean_answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858542933,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "O3VW7urjp9qi"
   },
   "outputs": [],
   "source": [
    "# Creating a Hugging Face compatible dataset\n",
    "dataset = Dataset.from_list(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1766858543121,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "IZsJGtmHp9oL",
    "outputId": "1863324c-b4a2-4c84-a923-1fe5782e1632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Features: {'prompt': Value('string'), 'answer': {'task_id': Value('string'), 'test_list': List(Value('string'))}}\n",
      "Sample Row Answer Keys: dict_keys(['task_id', 'test_list'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Features:\", dataset.features)\n",
    "print(\"Sample Row Answer Keys:\", dataset[0][\"answer\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858543321,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "pv2zGhEauI7Y"
   },
   "outputs": [],
   "source": [
    "# Pick the 2nd task again for consistency\n",
    "task = dataset[2][\"answer\"] # We grab it from our NEW dataset column\n",
    "prompt = dataset[2][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1766858543495,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "THY3eewTxW9T",
    "outputId": "b996d49c-1bde-4ca4-d6d2-67f46e76c38a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\"\"\"\\nWrite a function to find the n largest integers from a given list of numbers, returned in descending order.\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65]\\n\"\"\"\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766858544250,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "J5qx7l6duI5A"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build the prompt structure\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1766858544450,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zp95CUQouI25"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ")\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 12612,
     "status": "ok",
     "timestamp": 1766858557279,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zB24zGc3uI0m"
   },
   "outputs": [],
   "source": [
    "# Generate\n",
    "FastLanguageModel.for_inference(model)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1766858557281,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "yEa1OUdJuIxm"
   },
   "outputs": [],
   "source": [
    "# Slice to get only the generated text\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1766858557285,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "1-hJ_d1-p9j3",
    "outputId": "ee2b7c2d-12ce-429c-d8a6-48050f05360d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START_WORKING_OUT>\n",
      "Problem: Find n largest integers from a list, return in descending order.\n",
      "Approach: Use heapq.nlargest which returns n largest elements in order from largest to smallest.\n",
      "Parameters: List of numbers, integer n.\n",
      "Return: List of n largest numbers in descending order.\n",
      "</END_WORKING_OUT>\n",
      "<SOLUTION>\n",
      "import heapq\n",
      "\n",
      "def heap_queue_largest(nums, n):\n",
      "    \"\"\"\n",
      "    Return the n largest numbers from nums in descending order.\n",
      "    \n",
      "    Args:\n",
      "        nums: List of numbers (integers or floats)\n",
      "        n: Number of largest elements to return\n",
      "        \n",
      "    Returns:\n",
      "        List of n largest numbers in descending order\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return []\n",
      "    if n >= len(nums):\n",
      "        nums_sorted = sorted(nums, reverse=True)\n",
      "        return nums_sorted[:n]\n",
      "    return heapq.nlargest(n, nums)\n",
      "</SOLUTION>\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1766858558909,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "mGvSjCOcun8N"
   },
   "outputs": [],
   "source": [
    "# CRITICAL PART: Testing the Reward Functions\n",
    "# The Reward Functions expect LISTS (Batches), so we wrap our single item in a list.\n",
    "# This simulates a batch size of 1.\n",
    "batch_prompts = [prompt]\n",
    "batch_completions = [generated_text]\n",
    "batch_answers = [task] # This is the \"answer\" column data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1766858559112,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "QKcwBYWcun5J",
    "outputId": "1a19b189-69c7-46c1-9293-518f8ac09646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format Reward (Expect 0.1): 0.1\n"
     ]
    }
   ],
   "source": [
    "# 1. Test Format Reward\n",
    "r_format = format_reward_func(completions=batch_completions)\n",
    "print(f\"Format Reward (Expect 0.1): {r_format[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766858559306,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "h8Mo1AnZun3L",
    "outputId": "afa52926-4a76-4fa3-9389-dc635fdcc287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning Reward (Expect 0.0-0.2): 0.0530\n"
     ]
    }
   ],
   "source": [
    "# 2. Test Reasoning Reward\n",
    "r_reason = reasoning_reward_func(completions=batch_completions)\n",
    "print(f\"Reasoning Reward (Expect 0.0-0.2): {r_reason[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1766858559594,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "07ZCMGfIun08",
    "outputId": "36532e7d-f39b-40c0-e648-f037a9645b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness Reward (Expect 1.0 or 0.0): 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Test Correctness Reward (The complex one)\n",
    "# Note: We pass 'answer' explicitly, just like the Trainer will.\n",
    "r_correct = correctness_reward_func(\n",
    "    prompts=batch_prompts,\n",
    "    completions=batch_completions,\n",
    "    answer=batch_answers\n",
    ")\n",
    "print(f\"Correctness Reward (Expect 1.0 or 0.0): {r_correct[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1766858559694,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "fADybjDeunyh",
    "outputId": "7a4d9b36-6f44-4d03-b45d-036b9cf79f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SUCCESS: All reward functions accepted the inputs and returned scores.\n",
      " The plumbing is connected correctly.\n"
     ]
    }
   ],
   "source": [
    "if r_format[0] > 0 and (r_correct[0] == 0.0 or r_correct[0] == 1.0):\n",
    "    print(\" SUCCESS: All reward functions accepted the inputs and returned scores.\")\n",
    "    print(\" The plumbing is connected correctly.\")\n",
    "else:\n",
    "    print(\" FAIL: Something returned an unexpected format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24UBloc4drcX"
   },
   "source": [
    "# Step 12: Apply Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766858561822,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "RO2L3cPMdG0H"
   },
   "outputs": [],
   "source": [
    "def apply_chat_template(row):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": row[\"prompt\"]}\n",
    "    ]\n",
    "\n",
    "    # \"tokenize=False\" gives us the raw text string (e.g. \"<|system|>...<|user|>...\")\n",
    "    # This is exactly what the GRPOTrainer expects in the 'prompt' column.\n",
    "    row[\"prompt\"] = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBJ_nPqWdzC-"
   },
   "outputs": [],
   "source": [
    "# Apply it to the whole dataset\n",
    "original_prompt = dataset[0][\"prompt\"]\n",
    "dataset = dataset.map(apply_chat_template)\n",
    "\n",
    "print(\"\\n--- BEFORE ---\")\n",
    "print(original_prompt)\n",
    "print(\"\\n--- AFTER (What the Model Sees) ---\")\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX_1Z2XBgGJk"
   },
   "source": [
    "# Step 13: Setting up GRPO Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766858564965,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "HArFazRbd3w5"
   },
   "outputs": [],
   "source": [
    "# We give the model ample room so it never gets cut off\n",
    "max_prompt_length = 512\n",
    "max_completion_length = 2048  # doubled from T4 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766858565143,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "s3bSpC1ngg_K"
   },
   "outputs": [],
   "source": [
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 0.95,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    temperature = 0.9, # High enough to get diverse answers for GRPO\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1766858565939,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "IfEtXGoIgrEV",
    "outputId": "3856222a-db7e-454d-d0dd-df649b8cdaf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` * `gradient_accumulation_steps` * `world_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 4 to the `num_generations` of 8\n"
     ]
    }
   ],
   "source": [
    "# 3. The Trainer Config\n",
    "training_args = GRPOConfig(\n",
    "    # Integration\n",
    "    vllm_sampling_params = vllm_sampling_params, # We use vLLM for speed\n",
    "    output_dir = \"outputs\",\n",
    "    report_to = \"wandb\",\n",
    "    run_name = \"mbpp-grpo-a100-run1\",\n",
    "\n",
    "    # Optimization\n",
    "    learning_rate = 5e-6,        # Safe, stable LR\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "\n",
    "    # A100 POWER SETTINGS\n",
    "    per_device_train_batch_size = 4, # A100 can handle real batches\n",
    "    gradient_accumulation_steps = 1, # No need to accumulate if batch is 4\n",
    "    num_generations = 8,             # G=8: Much better stability than G=4\n",
    "\n",
    "    # Lengths\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "\n",
    "    # Duration\n",
    "    #num_train_epochs = 1,            # 1 Epoch is safest for RL on small data\n",
    "    max_steps = 5,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps = 1,\n",
    "    save_steps = 50,                 # Save more frequently\n",
    "    use_vllm = True,                 # Explicitly enable vLLM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz18F7NsrHPI"
   },
   "source": [
    "# Step 14: Initialize and Run GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766858567385,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "bDJjRr2srGn1"
   },
   "outputs": [],
   "source": [
    "# Select the Reward Functions we defined in Step 10\n",
    "# These are the \"Judges\" that will score the model's outputs.\n",
    "reward_functions = [\n",
    "    format_reward_func,       # Did it use <START_WORKING_OUT> and <SOLUTION>? (0.1)\n",
    "    reasoning_reward_func,    # Did it write ~500 chars of thought? (0.2)\n",
    "    correctness_reward_func   # Did the code actually pass the tests? (1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1766858567734,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "cpXKD9MYhJaI"
   },
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = reward_functions,\n",
    "    args = training_args,         # The A100 Config we just built\n",
    "    train_dataset = dataset,      # The dataset with the Chat Template applied\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 85216,
     "status": "ok",
     "timestamp": 1766858652998,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "TRNxA8IKraoK",
    "outputId": "50543cc4-44bb-44d4-c8d9-4d482c1e1054"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 378 | Num Epochs = 1 | Total steps = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/drive/MyDrive/grpo-verified-reasoner/wandb/run-20251227_180252-vbpxhgha</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/vbpxhgha' target=\"_blank\">mbpp-grpo-a100-run1</a></strong> to <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/vbpxhgha' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/vbpxhgha</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Detected [huggingface_hub.inference, openai] in use.\n",
      "wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "\n",
      "[FAIL] Task: Mbpp/603\n",
      "Error: Timeout after 2.0s\n",
      "\n",
      "[FAIL] Task: Mbpp/603\n",
      "Error: Timeout after 2.0s\n",
      "\n",
      "[FAIL] Task: Mbpp/603\n",
      "Error: Failed assertion: assert get_ludic(10) == [1, 2, 3, 5, 7]\n",
      "\n",
      "[FAIL] Task: Mbpp/603\n",
      "Error: Failed assertion: assert get_ludic(10) == [1, 2, 3, 5, 7]\n",
      "\n",
      "[FAIL] Task: Mbpp/603\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-2806261792.py\", line 18, in _exec_code_and_tests_worker\n",
      "    exec(t, env, env)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 17, in get_ludic\n",
      "ValueError: invalid literal for int() with base 10: ''\n",
      "\n",
      "\n",
      "[FAIL] Task: Mbpp/603\n",
      "Error: Failed assertion: assert get_ludic(25) == [1, 2, 3, 5, 7, 11, 13, 17, 23, 25]\n",
      "\n",
      "[FAIL] Task: Mbpp/603\n",
      "Error: Failed assertion: assert get_ludic(10) == [1, 2, 3, 5, 7]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / format_reward_func / mean</th>\n",
       "      <th>rewards / format_reward_func / std</th>\n",
       "      <th>rewards / reasoning_reward_func / mean</th>\n",
       "      <th>rewards / reasoning_reward_func / std</th>\n",
       "      <th>rewards / correctness_reward_func / mean</th>\n",
       "      <th>rewards / correctness_reward_func / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.210575</td>\n",
       "      <td>0.061160</td>\n",
       "      <td>296.250000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>296.250000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>2.011650</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110575</td>\n",
       "      <td>0.061160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.129825</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>3.989282</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029825</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>1.134350</td>\n",
       "      <td>0.010042</td>\n",
       "      <td>77.625000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.625000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>21.002621</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034350</td>\n",
       "      <td>0.010042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.901125</td>\n",
       "      <td>0.464266</td>\n",
       "      <td>109.875000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109.875000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>1.275098</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051125</td>\n",
       "      <td>0.014626</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.462910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.149850</td>\n",
       "      <td>0.022861</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>1.438902</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049850</td>\n",
       "      <td>0.022861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FAIL] Task: Mbpp/409\n",
      "Error: Failed assertion: assert min_product_tuple([(2, 7), (2, 6), (1, 8), (4, 9)] )==8\n",
      "\n",
      "[FAIL] Task: Mbpp/409\n",
      "Error: Failed assertion: assert min_product_tuple([(2, 7), (2, 6), (1, 8), (4, 9)] )==8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>profiling/Time taken: UnslothGRPOTrainer._calculate_rewards</td><td>â–ˆâ–â–â–â–</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer._prepare_inputs</td><td>â–ˆâ–â–â–â–‚</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.correctness_reward_func</td><td>â–ˆâ–â–â–â–</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.format_reward_func</td><td>â–ˆâ–â–ƒâ–ƒâ–‚</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.reasoning_reward_func</td><td>â–ˆâ–â–ˆâ–ƒâ–†</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.vLLM.generate</td><td>â–ˆâ–â–â–â–ƒ</td></tr><tr><td>train/completion_length</td><td>â–ˆâ–â–â–‚â–‚</td></tr><tr><td>train/completions/clipped_ratio</td><td>â–â–â–â–â–</td></tr><tr><td>train/completions/max_length</td><td>â–ˆâ–â–â–‚â–ƒ</td></tr><tr><td>train/completions/max_terminated_length</td><td>â–ˆâ–â–â–‚â–ƒ</td></tr><tr><td>+20</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>profiling/Time taken: UnslothGRPOTrainer._calculate_rewards</td><td>0.85325</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer._prepare_inputs</td><td>5.84074</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.correctness_reward_func</td><td>0.85062</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.format_reward_func</td><td>0.00018</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.reasoning_reward_func</td><td>0.00026</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.vLLM.generate</td><td>4.79006</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/completion_length</td><td>108.5</td></tr><tr><td>train/completions/clipped_ratio</td><td>0</td></tr><tr><td>train/completions/max_length</td><td>198</td></tr><tr><td>+25</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mbpp-grpo-a100-run1</strong> at: <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/vbpxhgha' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/vbpxhgha</a><br> View project at: <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251227_180252-vbpxhgha/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=0.005943450331687927, metrics={'train_runtime': 79.6173, 'train_samples_per_second': 0.502, 'train_steps_per_second': 0.063, 'total_flos': 0.0, 'train_loss': 0.005943450331687927})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rXx16M4-i8j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPFT9V292zRjo8a0fXb1VMU",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
