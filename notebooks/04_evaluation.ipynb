{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMpuHFIPCnx2WTNpeWRXmu4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Step 1: Mounting Google Drive and Importing Libraries\n"],"metadata":{"id":"qMEo3HOH-slR"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrrwKNmg-lNR","executionInfo":{"status":"ok","timestamp":1767061638767,"user_tz":360,"elapsed":32987,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"eef3b6e5-e518-4ab5-e0b7-1d69ad4e8a7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/grpo-verified-reasoner\n","data\t\t\t      LICENSE\t outputs    unsloth_compiled_cache\n","grpo_trainer_lora_model       models\t README.md  _unsloth_sentencepiece_temp\n","huggingface_tokenizers_cache  notebooks  src\t    wandb\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd /content/drive/MyDrive/grpo-verified-reasoner\n","!ls"]},{"cell_type":"code","source":["# Install UV (Faster pip)\n","!pip install --upgrade -qqq uv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0q0FVqc67uRU","executionInfo":{"status":"ok","timestamp":1767061647527,"user_tz":360,"elapsed":7144,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"bf060e08-ec3c-4823-d2f3-d2601a753873"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import os\n","import subprocess"],"metadata":{"id":"ZKwjGl3V7zI1","executionInfo":{"status":"ok","timestamp":1767061686334,"user_tz":360,"elapsed":5,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:False\""],"metadata":{"id":"imojZdcn7ztn","executionInfo":{"status":"ok","timestamp":1767061693213,"user_tz":360,"elapsed":1,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\""],"metadata":{"id":"5Y5vd4i8712_","executionInfo":{"status":"ok","timestamp":1767061693514,"user_tz":360,"elapsed":1,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install -q unsloth vllm human-eval tqdm\n","else:\n","    # Version matching for Colab GPUs\n","    try:\n","        import numpy, PIL\n","        get_numpy = f\"numpy=={numpy.__version__}\"\n","        get_pil   = f\"pillow=={PIL.__version__}\"\n","    except Exception:\n","        get_numpy, get_pil = \"numpy\", \"pillow\"\n","\n","    try:\n","        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n","    except Exception:\n","        is_t4 = False\n","\n","    # A100/H100: vllm 0.10.2, T4: vllm 0.9.2 + pinned triton\n","    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n","\n","    !uv pip install -qqq --upgrade \\\n","        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers tqdm human-eval\n","    !uv pip install -qqq {get_triton}"],"metadata":{"id":"vAQLD8-a9blY","executionInfo":{"status":"ok","timestamp":1767061746150,"user_tz":360,"elapsed":52001,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import json\n","import re\n","import ast\n","import torch\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","\n","from unsloth import FastLanguageModel\n","from vllm import SamplingParams\n","\n","from human_eval.data import read_problems, write_jsonl\n","from human_eval.evaluation import evaluate_functional_correctness"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4hqdCI59tMU","executionInfo":{"status":"ok","timestamp":1767061820414,"user_tz":360,"elapsed":74265,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"22b44184-85eb-4766-9266-9543bd4528a0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py:2525: SyntaxWarning: invalid escape sequence '\\.'\n","  * regex for parameter names, must start with `re:`, e.g. `re:language\\.layers\\..+\\.q_proj.weight`.\n"]},{"output_type":"stream","name":"stdout","text":["INFO 12-30 02:29:47 [__init__.py:244] Automatically detected platform cuda.\n","ERROR 12-30 02:29:50 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n","ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"]}]},{"cell_type":"code","source":["SEED = 3407\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","!nvidia-smi -L"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_b5DzuU-S6z","executionInfo":{"status":"ok","timestamp":1767061820478,"user_tz":360,"elapsed":62,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"f36b70fb-34fb-4065-9890-849ecc3c298b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-8c9690ae-c44c-5dcf-cf3b-904693311f78)\n"]}]},{"cell_type":"markdown","source":["# Step 2: Verifying GPU and Environment"],"metadata":{"id":"OWuA9pSe-0M8"}},{"cell_type":"code","source":["print(\"Torch version:\", torch.__version__)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KmlB9Uie-xLj","executionInfo":{"status":"ok","timestamp":1767061826104,"user_tz":360,"elapsed":14,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"dfd57c50-1266-4665-dbf8-0b2ef1d74770"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 2.7.0+cu126\n","CUDA available: True\n","GPU: Tesla T4\n"]}]},{"cell_type":"markdown","source":["# Step 3: Setting Up the Main Variables"],"metadata":{"id":"zA7FVu96-anB"}},{"cell_type":"code","source":["# HumanEval evaluation settings\n","N_SAMPLES_PER_PROBLEM = 1          # pass@1 by default; raise to 5 or 10 if you later want pass@k\n","MAX_NEW_TOKENS_NON_COT = 256       # Non-CoT completions are usually short (function body)\n","MAX_NEW_TOKENS_COT = 768           # CoT can be longer due to tags + full function\n","\n","TEMP_NON_COT = 0.0                # low sampling noise; stable for benchmarking\n","TEMP_COT = 0.0                     # encourages exploration under schema (optional)\n","\n","TOP_P = 0.95\n","MIN_P = 0.10\n","\n","# Stop guards (prevent rambling without clipping typical solutions)\n","STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]"],"metadata":{"id":"xjIWuUg0-aC4","executionInfo":{"status":"ok","timestamp":1767061853857,"user_tz":360,"elapsed":11,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Output paths (keep all artifacts under one folder)\n","EVAL_DIR = \"data/evaluation\"\n","os.makedirs(EVAL_DIR, exist_ok=True)"],"metadata":{"id":"SN2W2UUc-XgA","executionInfo":{"status":"ok","timestamp":1767061854077,"user_tz":360,"elapsed":3,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Model paths / identifiers\n","BASE_MODEL_PATH = \"unsloth/Qwen3-4B-Base\"\n","SFT_MODEL_PATH  = \"models/qwen3-4b-sft\"\n","GRPO_MODEL_PATH = \"models/qwen3-4b-grpo-final\""],"metadata":{"id":"fZp2YS7SABnS","executionInfo":{"status":"ok","timestamp":1767061854856,"user_tz":360,"elapsed":3,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# CoT system prompt (must match training schema)\n","COT_SYSTEM_PROMPT_HUMANEVAL = \"\"\"You are a code-generation engine.\n","\n","You must output your response in the following exact format:\n","\n","<START_WORKING_OUT>\n","Concise reasoning steps required to solve the problem.\n","</END_WORKING_OUT>\n","<SOLUTION>\n","Write ONLY the function body that continues from the given prompt.\n","Do NOT repeat the function signature (the 'def ...:' line).\n","Do NOT include tests, examples, explanations, or any text outside these tags.\n","</SOLUTION>\n","\n","Do not output anything outside these tags.\n","\"\"\""],"metadata":{"id":"_ahx401qAOOR","executionInfo":{"status":"ok","timestamp":1767061925364,"user_tz":360,"elapsed":3,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Loading HumanEval Problems"],"metadata":{"id":"tFeCRur7Aa7k"}},{"cell_type":"code","source":["problems = read_problems()  # dict: {task_id: {\"prompt\":..., \"test\":..., \"entry_point\":...}}\n","task_ids = list(problems.keys())\n","\n","print(f\"Loaded HumanEval problems: {len(task_ids)}\")\n","print(\"Example task_id:\", task_ids[0])\n","print(\"\\n--- Prompt Preview ---\")\n","print(problems[task_ids[0]][\"prompt\"][:500])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5BicgVzbAVcf","executionInfo":{"status":"ok","timestamp":1767062048052,"user_tz":360,"elapsed":7,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"ccc6d167-de78-4053-b184-b9d1ee219e66"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded HumanEval problems: 164\n","Example task_id: HumanEval/0\n","\n","--- Prompt Preview ---\n","from typing import List\n","\n","\n","def has_close_elements(numbers: List[float], threshold: float) -> bool:\n","    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n","    given threshold.\n","    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n","    False\n","    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n","    True\n","    \"\"\"\n","\n"]}]},{"cell_type":"markdown","source":["# Step 5: Prompt Builders"],"metadata":{"id":"B0hbt-vjEf-y"}},{"cell_type":"code","source":["def build_non_cot_prompt(problem: dict) -> str:\n","    \"\"\"\n","    Non-CoT: HumanEval-style continuation.\n","    We provide ONLY the HumanEval prompt.\n","    The harness will prepend this prompt again during execution.\n","    \"\"\"\n","    return problem[\"prompt\"]"],"metadata":{"id":"t6ZmWRrRAjGh","executionInfo":{"status":"ok","timestamp":1767062157623,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def build_cot_prompt(problem: dict, tokenizer) -> str:\n","    \"\"\"\n","    CoT: Uses the same chat template distribution as SFT/GRPO training.\n","    Returns a fully formatted ChatML prompt string.\n","    \"\"\"\n","    messages = [\n","        {\"role\": \"system\", \"content\": COT_SYSTEM_PROMPT_HUMANEVAL},\n","        {\"role\": \"user\", \"content\": problem[\"prompt\"]},\n","    ]\n","    return tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True,\n","    )"],"metadata":{"id":"na1taHI7Emmv","executionInfo":{"status":"ok","timestamp":1767062172295,"user_tz":360,"elapsed":5,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Step 6: Output Post-processing Logic"],"metadata":{"id":"W_FSvWfHE24y"}},{"cell_type":"code","source":["# Stop strings (for safety against rambling)\n","# These are conservative: they stop the model from starting a NEW definition / class / main block.\n","STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]\n","\n","SOLUTION_RE = re.compile(r\"<SOLUTION>(.*?)</SOLUTION>\", re.DOTALL | re.IGNORECASE)"],"metadata":{"id":"mHbWyt3kH0PO","executionInfo":{"status":"ok","timestamp":1767062356132,"user_tz":360,"elapsed":41,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def extract_cot_solution(completion_only_text: str) -> str:\n","    \"\"\"\n","    Extract code from <SOLUTION>...</SOLUTION>.\n","    Returns \"\" on failure (schema violation â†’ harness will fail, which is correct behavior).\n","    \"\"\"\n","    m = SOLUTION_RE.search(completion_only_text)\n","    if not m:\n","        return \"\"\n","    return m.group(1).strip()"],"metadata":{"id":"VN1M8SizH0Ne","executionInfo":{"status":"ok","timestamp":1767062408328,"user_tz":360,"elapsed":3,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def truncate_on_stop_strings(text: str, stop_strings: list[str]) -> str:\n","    \"\"\"\n","    Stops the completion if it begins a new unrelated block (def/class/main).\n","    This reduces harness crashes from rambling continuations.\n","    \"\"\"\n","    cut = len(text)\n","    for s in stop_strings:\n","        idx = text.find(s)\n","        if idx != -1:\n","            cut = min(cut, idx)\n","    return text[:cut].rstrip()"],"metadata":{"id":"LS7QqhbiH0LJ","executionInfo":{"status":"ok","timestamp":1767062443123,"user_tz":360,"elapsed":3,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Step 7: Defining Evaluation Loop"],"metadata":{"id":"NO7zEeCjK8le"}},{"cell_type":"code","source":["def evaluate_model(\n","    model_path: str,\n","    tokenizer,\n","    problems: dict,\n","    task_ids: list[str],\n","    *,\n","    use_cot: bool,\n","    output_jsonl: str,\n","    max_new_tokens: int,\n","    temperature: float,\n","    top_p: float = 0.95,\n","    min_p: float = 0.10,\n","    load_in_4bit: bool = False,\n","    gpu_memory_utilization: float = 0.9,\n","):\n","    \"\"\"\n","    Generates 1 completion per HumanEval problem and writes JSONL for the official harness.\n","\n","    JSONL schema: {\"task_id\": ..., \"completion\": ...}\n","    The human-eval harness will run: problem[\"prompt\"] + completion + tests\n","\n","    Therefore:\n","      - Non-CoT completion MUST be just the continuation (no prompt duplication).\n","      - CoT completion MUST be extracted code from <SOLUTION> and MUST NOT repeat signature.\n","    \"\"\"\n","\n","    print(f\"\\n Loading: {model_path} | Mode: {'CoT' if use_cot else 'Non-CoT'}\")\n","\n","    model, _ = FastLanguageModel.from_pretrained(\n","        model_name=model_path,\n","        max_seq_length=3072,\n","        load_in_4bit=load_in_4bit,\n","        fast_inference=True,\n","        gpu_memory_utilization=gpu_memory_utilization,\n","    )\n","    FastLanguageModel.for_inference(model)\n","\n","    samples = []\n","\n","    for task_id in tqdm(task_ids, desc=f\"Generating ({'CoT' if use_cot else 'Non-CoT'})\"):\n","        problem = problems[task_id]\n","\n","        # 1) Build prompt\n","        if use_cot:\n","            prompt_text = build_cot_prompt(problem, tokenizer)\n","        else:\n","            prompt_text = build_non_cot_prompt(problem)\n","\n","        # 2) Tokenize (IMPORTANT: always produce input_ids we can slice against)\n","        inputs = tokenizer(\n","            prompt_text,\n","            return_tensors=\"pt\",\n","        ).to(\"cuda\")\n","\n","        input_len = inputs[\"input_ids\"].shape[1]\n","\n","        # 3) Generate\n","        with torch.no_grad():\n","            out = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                temperature=temperature,\n","                do_sample=False,          # deterministic benchmarking\n","                top_p=top_p,\n","                min_p=min_p,\n","                eos_token_id=tokenizer.eos_token_id,\n","                use_cache=True,\n","            )\n","\n","        # 4) Decode ONLY newly generated tokens (completion-only)\n","        completion_only = tokenizer.decode(\n","            out[0][input_len:],\n","            skip_special_tokens=False,   # keep tags for CoT extraction\n","        )\n","\n","        # 5) Post-process into harness-ready completion\n","        if use_cot:\n","            completion = extract_cot_solution(completion_only)\n","            # Optional stop-guard on extracted code (prevents second defs)\n","            completion = truncate_on_stop_strings(completion, STOP_STRINGS)\n","        else:\n","            completion = completion_only\n","            completion = truncate_on_stop_strings(completion, STOP_STRINGS)\n","\n","        samples.append({\"task_id\": task_id, \"completion\": completion})\n","\n","    write_jsonl(output_jsonl, samples)\n","    print(f\" Saved {len(samples)} samples to: {output_jsonl}\")\n","\n","    # Cleanup to avoid VRAM fragmentation when you load the next model\n","    del model\n","    torch.cuda.empty_cache()"],"metadata":{"id":"cRwm9H5uH0I-","executionInfo":{"status":"ok","timestamp":1767063359001,"user_tz":360,"elapsed":6,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":20,"outputs":[]}]}