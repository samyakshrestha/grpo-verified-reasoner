{"cells":[{"cell_type":"markdown","metadata":{"id":"EwCYejay3dmd"},"source":["# Notebook: HumanEval Evaluation\n","\n","This notebook performs the **final evaluation phase** of the GRPO verifiable-reward coding project using the **HumanEval benchmark**.\n","\n","---\n","\n","## Objective\n","\n","Evaluate and compare the functional correctness of multiple models trained under different regimes:\n","\n","- **Base model** (no fine-tuning)\n","- **SFT model** (supervised fine-tuning warm-up)\n","- **GRPO model** (verifiable-reward reinforcement learning)\n","\n","Each model is evaluated in both:\n","- **Non-CoT mode** (direct code completion)\n","- **CoT mode** (reasoning + solution, schema-constrained)\n","\n","The goal is to measure **pass@1 performance** under a **strict execution harness**, ensuring all outputs are runnable, well-formed Python.\n","\n","---\n","\n","## Evaluation Protocol\n","\n","1. **Prompt Construction**\n","   - Non-CoT: model completes the function body directly.\n","   - CoT: model is instructed to emit reasoning followed by a `<SOLUTION>` block.\n","\n","2. **Batch Generation**\n","   - All HumanEval tasks are generated using **vLLM batch inference** for speed and determinism.\n","   - Stop tokens are applied to prevent over-generation.\n","\n","3. **Output Sanitization**\n","   A custom `cleaner` function enforces:\n","   - Strict `<SOLUTION>` extraction (CoT only)\n","   - Markdown fence removal (```python â€¦ ```)\n","   - Redundant `def` removal\n","   - Docstring stripping\n","   - Robust indentation normalization compatible with the HumanEval harness\n","\n","4. **Execution-Based Scoring**\n","   - Each completion is executed against the official HumanEval tests.\n","   - Results are aggregated into a single results table.\n","\n","---\n","\n","## Metrics\n","\n","- **pass@1**: fraction of problems solved correctly on the first attempt\n","- Results are reported per model and per generation mode (CoT / non-CoT)\n","\n","Duplicate rows from batched generation are explicitly deduplicated before scoring.\n","\n","---\n","\n","## Output Artifacts\n","\n","- `*_non_cot.jsonl` â€” raw non-CoT completions\n","- `*_cot.jsonl` â€” raw CoT completions\n","- `df_results` â€” consolidated evaluation table with pass@1 scores\n","\n","These artifacts form the **final empirical evidence** for assessing GRPO effectiveness.\n","\n","---\n","\n","## Result Summary\n","\n","- Generation stability issues (empty outputs, truncation) were resolved by:\n","  - Removing aggressive repetition penalties\n","  - Correcting stop-token logic\n","  - Fixing markdown and tag-handling bugs in the cleaner\n","- Final runs achieved **near-complete coverage** (â‰¤1 empty completion out of 164)\n","- GRPO shows measurable gains over Base and SFT under controlled conditions"]},{"cell_type":"markdown","metadata":{"id":"qMEo3HOH-slR"},"source":["# Step 1: Mounting Google Drive and Importing Libraries\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17216,"status":"ok","timestamp":1767410604652,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"yrrwKNmg-lNR","outputId":"5be0afaa-7f0f-48c8-d56e-03741efc62b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/grpo-verified-reasoner\n","data\t\t\t      LICENSE\t outputs    unsloth_compiled_cache\n","grpo_trainer_lora_model       models\t README.md  _unsloth_sentencepiece_temp\n","huggingface_tokenizers_cache  notebooks  src\t    wandb\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd /content/drive/MyDrive/grpo-verified-reasoner\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0q0FVqc67uRU"},"outputs":[],"source":["# Install UV (Faster pip)\n","!pip install --upgrade -qqq uv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKwjGl3V7zI1"},"outputs":[],"source":["import os\n","import subprocess"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imojZdcn7ztn"},"outputs":[],"source":["os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:False\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Y5vd4i8712_"},"outputs":[],"source":["# os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAQLD8-a9blY"},"outputs":[],"source":["if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install -q unsloth vllm human-eval tqdm\n","else:\n","    # Version matching for Colab GPUs\n","    try:\n","        import numpy, PIL\n","        get_numpy = f\"numpy=={numpy.__version__}\"\n","        get_pil   = f\"pillow=={PIL.__version__}\"\n","    except Exception:\n","        get_numpy, get_pil = \"numpy\", \"pillow\"\n","\n","    try:\n","        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n","    except Exception:\n","        is_t4 = False\n","\n","    # A100/H100: vllm 0.10.2, T4: vllm 0.9.2 + pinned triton\n","    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n","\n","    !uv pip install -qqq --upgrade \\\n","        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers tqdm human-eval\n","    !uv pip install -qqq {get_triton}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36096,"status":"ok","timestamp":1767410730264,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"f4hqdCI59tMU","outputId":"8a10c49b-8987-471e-bed0-a1927251ee3c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-1905740597.py:12: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n","\n","Please restructure your imports with 'import unsloth' at the top of your file.\n","  from unsloth import FastLanguageModel\n"]},{"name":"stdout","output_type":"stream","text":["ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","INFO 01-03 03:25:08 [__init__.py:216] Automatically detected platform cuda.\n","ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"]}],"source":["import gc\n","import json\n","import re\n","import ast\n","import torch\n","import random\n","import textwrap\n","import numpy as np\n","from tqdm import tqdm\n","import pandas as pd\n","\n","from unsloth import FastLanguageModel\n","from vllm import SamplingParams\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","from human_eval.data import read_problems, write_jsonl\n","from human_eval.evaluation import evaluate_functional_correctness"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1767410671627,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"3_b5DzuU-S6z","outputId":"1ef705db-8d81-4d45-a61b-cd65d164c219"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-2bf919d7-e2b7-f8a2-e353-e748b78185c1)\n"]}],"source":["SEED = 3407\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","!nvidia-smi -L"]},{"cell_type":"markdown","metadata":{"id":"OWuA9pSe-0M8"},"source":["# Step 2: Verifying GPU and Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1767410671638,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"KmlB9Uie-xLj","outputId":"8bd3cdb7-1a49-4843-c67b-016e80cc081e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Torch version: 2.8.0+cu128\n","CUDA available: True\n","GPU: NVIDIA A100-SXM4-80GB\n"]}],"source":["print(\"Torch version:\", torch.__version__)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"]},{"cell_type":"markdown","metadata":{"id":"zA7FVu96-anB"},"source":["# Step 3: Setting Up the Main Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjIWuUg0-aC4"},"outputs":[],"source":["# HumanEval evaluation settings\n","N_SAMPLES_PER_PROBLEM = 1          # pass@1 by default; raise to 5 or 10 if you later want pass@k\n","MAX_NEW_TOKENS_NON_COT = 512       # Non-CoT completions are usually short (function body)\n","MAX_NEW_TOKENS_COT = 2048           # CoT can be longer due to tags + full function\n","\n","TEMP_NON_COT = 0.0                # low sampling noise; stable for benchmarking\n","TEMP_COT = 0.0                     # encourages exploration under schema (optional)\n","\n","TOP_P = 0.95\n","MIN_P = 0.10\n","\n","# Stop guards (prevent rambling without clipping typical solutions)\n","STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SN2W2UUc-XgA"},"outputs":[],"source":["# Output paths (keep all artifacts under one folder)\n","EVAL_DIR = \"data/evaluation\"\n","os.makedirs(EVAL_DIR, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZp2YS7SABnS"},"outputs":[],"source":["# Model paths / identifiers\n","BASE_MODEL_PATH = \"unsloth/Qwen3-4B-Base\"\n","SFT_MODEL_PATH  = \"models/qwen3-4b-sft\"\n","GRPO_MODEL_PATH = \"models/qwen3-4b-grpo-final\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ezJrKFY8lGR"},"outputs":[],"source":["COT_SYSTEM_PROMPT_HUMANEVAL = \"\"\"You are a code-generation engine.\n","You must output your response in the following exact format:\n","<START_WORKING_OUT>\n","Concise reasoning steps required to solve the problem.\n","</END_WORKING_OUT>\n","<SOLUTION>\n","Valid Python code only.\n","</SOLUTION>\n","Do not output anything outside these tags.\"\"\""]},{"cell_type":"markdown","metadata":{"id":"tFeCRur7Aa7k"},"source":["# Step 4: Loading HumanEval Problems"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1767406140896,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"5BicgVzbAVcf","outputId":"4e5246f8-7fe6-479f-cbf8-e1064e8934f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded HumanEval problems: 164\n","Example task_id: HumanEval/0\n","\n","--- Prompt Preview ---\n","from typing import List\n","\n","\n","def has_close_elements(numbers: List[float], threshold: float) -> bool:\n","    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n","    given threshold.\n","    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n","    False\n","    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n","    True\n","    \"\"\"\n","\n"]}],"source":["problems = read_problems()  # dict: {task_id: {\"prompt\":..., \"test\":..., \"entry_point\":...}}\n","task_ids = list(problems.keys())\n","\n","print(f\"Loaded HumanEval problems: {len(task_ids)}\")\n","print(\"Example task_id:\", task_ids[0])\n","print(\"\\n--- Prompt Preview ---\")\n","print(problems[task_ids[0]][\"prompt\"][:500])"]},{"cell_type":"markdown","metadata":{"id":"B0hbt-vjEf-y"},"source":["# Step 5: Prompt Builders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6ZmWRrRAjGh"},"outputs":[],"source":["def build_non_cot_prompt(problem: dict) -> str:\n","    \"\"\"\n","    Non-CoT: HumanEval-style continuation.\n","    We provide ONLY the HumanEval prompt.\n","    The harness will prepend this prompt again during execution.\n","    \"\"\"\n","    return problem[\"prompt\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"na1taHI7Emmv"},"outputs":[],"source":["def build_cot_prompt(problem: dict, tokenizer) -> str:\n","    \"\"\"\n","    CoT: Uses the same chat template distribution as SFT/GRPO training.\n","    Returns a fully formatted ChatML prompt string.\n","    \"\"\"\n","    messages = [\n","        {\"role\": \"system\", \"content\": COT_SYSTEM_PROMPT_HUMANEVAL},\n","        {\"role\": \"user\", \"content\": problem[\"prompt\"]},\n","    ]\n","    return tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"W_FSvWfHE24y"},"source":["# Step 6: Output Post-processing Logic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHbWyt3kH0PO"},"outputs":[],"source":["# Stop strings (for safety against rambling)\n","# These are conservative: they stop the model from starting a NEW definition / class / main block.\n","STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]\n","\n","SOLUTION_RE = re.compile(r\"<SOLUTION>(.*?)</SOLUTION>\", re.DOTALL | re.IGNORECASE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VN1M8SizH0Ne"},"outputs":[],"source":["\n","def extract_cot_solution(completion_only_text: str) -> str:\n","    \"\"\"\n","    Extract code from <SOLUTION>...</SOLUTION>.\n","    Returns \"\" on failure (schema violation â†’ harness will fail, which is correct behavior).\n","    \"\"\"\n","    m = SOLUTION_RE.search(completion_only_text)\n","    if not m:\n","        return \"\"\n","    return m.group(1).strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LS7QqhbiH0LJ"},"outputs":[],"source":["def truncate_on_stop_strings(text: str, stop_strings: list[str]) -> str:\n","    \"\"\"\n","    Stops the completion if it begins a new unrelated block (def/class/main).\n","    This reduces harness crashes from rambling continuations.\n","    \"\"\"\n","    cut = len(text)\n","    for s in stop_strings:\n","        idx = text.find(s)\n","        if idx != -1:\n","            cut = min(cut, idx)\n","    return text[:cut].rstrip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFsflZFYrXSM"},"outputs":[],"source":["def cleaner(code: str, entry_point: str, is_cot: bool) -> str:\n","    # A. EXTRACT CoT\n","    if is_cot:\n","        if \"<SOLUTION>\" in code:\n","            code = code.split(\"<SOLUTION>\")[-1]\n","            if \"</SOLUTION>\" in code:\n","                code = code.split(\"</SOLUTION>\")[0]\n","        else:\n","            return \"\"\n","\n","    # B. REMOVE MARKDOWN\n","    code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n","\n","    # C. FILTER LINES\n","    lines = code.split('\\n')\n","    filtered_lines = []\n","\n","    def_pattern = re.compile(rf\"^\\s*def\\s+{re.escape(entry_point)}(\\s*\\(|\\s*:)\")\n","\n","    for line in lines:\n","        # Remove redundant definition\n","        if def_pattern.match(line):\n","            continue\n","        # Remove \"from typing\" (Always redundant in HumanEval, creates noise)\n","        if line.strip().startswith(\"from typing\"):\n","            continue\n","        filtered_lines.append(line)\n","\n","    code = \"\\n\".join(filtered_lines)\n","\n","    # D. REMOVE DOCSTRINGS\n","    code = re.sub(r'(\\s*(\"\"\"|\\'\\'\\')[\\s\\S]*?\\2)', '', code, count=1)\n","    code = code.lstrip('\\n\\r')\n","\n","    # E. SMART RELATIVE NORMALIZATION (The Fix)\n","    if code.strip():\n","        lines = code.split('\\n')\n","\n","        # 1. Find the \"Reference Line\" to measure indentation.\n","        # We skip lines starting with 'import' so we measure the ACTUAL code body.\n","        reference_line = None\n","        for l in lines:\n","            stripped = l.strip()\n","            if stripped and not stripped.startswith((\"import \", \"from \")):\n","                reference_line = l\n","                break\n","\n","        # Fallback: If no code found (only imports?), use the first line.\n","        if reference_line is None:\n","            reference_line = next((l for l in lines if l.strip()), None)\n","\n","        if reference_line:\n","            # Measure indentation of the BODY\n","            body_indent = len(reference_line) - len(reference_line.lstrip())\n","\n","            normalized_lines = []\n","            for line in lines:\n","                # If the line is empty, just strip it\n","                if not line.strip():\n","                    normalized_lines.append(\"\")\n","                    continue\n","\n","                # Calculate current indent\n","                current_indent = len(line) - len(line.lstrip())\n","\n","                # Subtract the body_indent.\n","                # If an import is at 0 and body is at 4, this becomes -4.\n","                # We max(0, ...) to ensure we don't crash, effectively pulling the import in.\n","                new_indent = max(0, current_indent - body_indent)\n","\n","                # Reconstruct the line\n","                normalized_lines.append(\" \" * new_indent + line.lstrip())\n","\n","            code = \"\\n\".join(normalized_lines)\n","\n","        # Finally, indent everything by 4 for the harness\n","        code = textwrap.indent(code, '    ')\n","\n","    return code"]},{"cell_type":"markdown","metadata":{"id":"NO7zEeCjK8le"},"source":["# Step 7: Defining Evaluation Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGtRrQolGSLk"},"outputs":[],"source":["def evaluate_model(\n","    model_path: str,\n","    problems: dict,\n","    task_ids: list[str],\n","    *,\n","    use_cot: bool,\n","    output_jsonl: str,\n","    max_new_tokens: int,\n","    temperature: float,\n","    top_p: float = 0.95,\n","    min_p: float = 0.10,\n","    load_in_4bit: bool = True,\n","    gpu_memory_utilization: float = 0.7,\n","):\n","    print(f\"\\n Loading: {model_path} | Mode: {'CoT' if use_cot else 'Non-CoT'}\")\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name=model_path,\n","        max_seq_length=4096,\n","        load_in_4bit=load_in_4bit,\n","        fast_inference=True,\n","        gpu_memory_utilization=gpu_memory_utilization,\n","    )\n","    FastLanguageModel.for_inference(model)\n","\n","    prompts = []\n","    print(f\" Preparing {len(task_ids)} prompts...\")\n","    for task_id in task_ids:\n","        problem = problems[task_id]\n","        if use_cot:\n","            prompt_text = build_cot_prompt(problem, tokenizer)\n","        else:\n","            prompt_text = build_non_cot_prompt(problem)\n","        prompts.append(prompt_text)\n","\n","    # Auto-Switching Stop Logic\n","    if use_cot:\n","        stop_tokens = [\"</SOLUTION>\"]\n","    else:\n","        stop_tokens = [\"\\nclass\", \"\\nif __name__\", \"\\nprint\", \"\\ndef \"]\n","\n","    sampling_params = SamplingParams(\n","        temperature=temperature,\n","        top_p=top_p,\n","        min_p=min_p,\n","        max_tokens=max_new_tokens,\n","        stop=stop_tokens,\n","        # repetition_penalty REMOVED per orders\n","    )\n","\n","    print(f\" Running vLLM Batch Generation...\")\n","    outputs = model.fast_generate(prompts, sampling_params=sampling_params)\n","\n","    samples = []\n","    for i, task_id in enumerate(task_ids):\n","        problem = problems[task_id]\n","        completion_only = outputs[i].outputs[0].text\n","\n","        completion = cleaner(completion_only, problem[\"entry_point\"], use_cot)\n","\n","        samples.append({\n","            \"task_id\": task_id,\n","            \"prompt\": problem[\"prompt\"],\n","            \"completion\": completion\n","        })\n","\n","    write_jsonl(output_jsonl, samples)\n","    print(f\" Saved {len(samples)} samples to: {output_jsonl}\")\n","\n","    del model, tokenizer\n","    gc.collect()\n","    torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"ZsDfO98zwS48"},"source":["# Step 8: Generating JSONL Files for Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czY25ERRwcWH"},"outputs":[],"source":["GEN_DIR = Path(EVAL_DIR) / \"generations\"\n","GEN_DIR.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9e07d981d95b4b6fac218bf1a5cd4310","c97bc9dff1e54ca682f684f2ecda5f91","9517697050174a11a667421cf3b82dc8","4c8666538c57445dbc451d8eb56355cb","23703f7d48a249dc9c52a329e90111e4","7caa2ebfd52e4cd9975fae9c894db6b2","ddb5f0a4713e415eb1eb0fa7117cacd8","c37d9c2146af49f2a8871369274c7117","65d55abc399c46cda630fe6ce221dd70","56491b9d325e4efa934f324f8f7ac069","c59249c1219f4d8aa78f5640105f3d51","86453b4351bd458d94985eb2e2369478","e37c5f6d52164cf698863c6024f553ac","cb96ab4fbef44ca58be90f961029698a","fdb0c52bc2ef40d484b2efbbf10f60c0","dc5e8d1c4930442cb058d226df998809","ed1b82d393c84942a17e5f6a05109f2b","a425bef413e9416788939c8a7017a44b","1bb44fd0835c407885c4c5fe8e4d72da","fd005e42b5a24f379c99fb2c8d525a0f","7f325c8a677e4ac9a0fda1ee5ce7dbab","128d005d68bb48c986a1844810e62455","cfc1a7a603b548eeba145344d6c6380c","31dd11d1d36b4ec481c9835c7c9bd4f7","96bfd49bb45f44d080bc259c9a9021d0","6cd33c9a903a448c84ce655286c75240","5d92da5b6bb9425698d1f01c1e8393f3","e2c6b77ca36a43d2b5cb5bcab1e07443","756f24906c634b04b369818594729397","88fea147f555478195e17483b49808fb","558bea4d97b84928941f5760ba849b9f","a1c5409099154a8bb10cc70a15d875a2","10879fd7064a42d28ae3b330ce166e75","573e32f79cfc4a44ac6e9da4a133ec51","0f9691a3300545dab1057fbcd57f7dee","d791cd625ef44a79b68a7bd4ecefdc17","13f6bef9ac914ec881bbed09ca41654e","9aa5130192da4193907133cd94e88739","ad81c3d4627f4f779fa707604a324a29","9d5d1f2424d545e4a5c5dea1950889c8","6c4c54209d7344daad6f66fed3f805a9","39a4e4ecadc846138ebc364a8d46c577","4ced5aa9982342ecaf19419929d8cf9c","291e1af1a80c41c5a9bb5da37776cb6d"]},"executionInfo":{"elapsed":116992,"status":"ok","timestamp":1767353974514,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"s2O5o6iww1uQ","outputId":"6ae52a9c-a51a-4365-de86-3930c9a27d6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Loading: unsloth/Qwen3-4B-Base | Mode: Non-CoT\n","INFO 01-02 11:37:40 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 11:37:40 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 19.78%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 64.\n","Unsloth: vLLM's KV Cache can use up to 12.83 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 11:37:49 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.19782509162794168, 'max_num_batched_tokens': 6144, 'max_num_seqs': 64, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 11:37:51 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n","INFO 01-02 11:37:51 [__init__.py:1815] Using max model len 4096\n","INFO 01-02 11:37:52 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=6144.\n","WARNING 01-02 11:37:52 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n","INFO 01-02 11:37:55 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":128,\"local_cache_dir\":null}\n","INFO 01-02 11:37:56 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 11:37:57 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 11:37:57 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 11:37:58 [weight_utils.py:348] Using model weights format ['*.safetensors']\n","INFO 01-02 11:37:59 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e07d981d95b4b6fac218bf1a5cd4310","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86453b4351bd458d94985eb2e2369478","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:38:02 [gpu_model_runner.py:2392] Model loading took 3.3523 GiB and 2.875338 seconds\n","INFO 01-02 11:38:17 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/0550210604/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 11:38:17 [backends.py:550] Dynamo bytecode transform time: 14.14 s\n","INFO 01-02 11:38:23 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.087 s\n","INFO 01-02 11:38:26 [monitor.py:34] torch.compile takes 14.14 s in total\n","INFO 01-02 11:38:28 [gpu_worker.py:298] Available KV cache memory: 11.71 GiB\n","INFO 01-02 11:38:29 [kv_cache_utils.py:864] GPU KV cache size: 85,296 tokens\n","INFO 01-02 11:38:29 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 20.82x\n","INFO 01-02 11:38:29 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n","INFO 01-02 11:38:29 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"name":"stderr","output_type":"stream","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:06<00:00,  3.07it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:03<00:00,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:38:39 [gpu_model_runner.py:3118] Graph capturing finished in 10 secs, took 0.27 GiB\n","INFO 01-02 11:38:39 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 10 secs.\n","INFO 01-02 11:38:39 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 10 secs.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:38:41 [gpu_worker.py:391] Free memory on device (22.42/79.32 GiB) on startup. Desired GPU memory utilization is (0.19782509162794168, 15.69 GiB). Actual usage is 3.35 GiB for weight, 0.62 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.27 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=12133338214` to fit into requested memory, or `--kv-cache-memory=19354002944` to fully utilize gpu memory. Current kv cache memory in use is 12577934438 bytes.\n","INFO 01-02 11:38:42 [core.py:218] init engine (profile, create kv cache, warmup model) took 39.97 seconds\n","INFO 01-02 11:38:44 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 11:38:44 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm2', 'q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm1', 'ffn_norm', 'k_norm', 'layer_norm1', 'norm', 'input_layernorm', 'post_attention_layernorm', 'norm2', 'attention_norm', 'post_layernorm']\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm2', 'cross_attn_input_layernorm', 'q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm1', 'ffn_norm', 'k_norm', 'layer_norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'post_attention_layernorm', 'norm2', 'attention_norm', 'post_layernorm']\n"," Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfc1a7a603b548eeba145344d6c6380c","version_major":2,"version_minor":0},"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"573e32f79cfc4a44ac6e9da4a133ec51","version_major":2,"version_minor":0},"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" Saved 164 samples to: data/evaluation/generations/base_non_cot.jsonl\n"]}],"source":["# BASE MODEL (Non-CoT)\n","evaluate_model(\n","    model_path=BASE_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=False,\n","    output_jsonl=str(GEN_DIR / \"base_non_cot.jsonl\"),\n","    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n","    temperature=TEMP_NON_COT,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["52dae0c139574d6b97726e49fbc4f499","76d06788dd484852bb1af7d7a00eedce","906c8feadcec4feda4c2ef5c71c58ad1","fad724606e4c42ab8ec76dbac81759bb","6a0f7b13060848929f3ab0f7c97f5018","77d699cc0a55478c85801098e92c0ac7","25ba8d7ce7c84e79b500e2fe8469d20b","a5271e0fa88b4bcf94b59c32f4fd4ad9","bd45b3f39f26457294feb175c29f4f84","2287da862c6047f58b0f6c2fd5406a72","81bcf1025a8f4d928072d3112baa0d5f","260af0bd31c543f78ac7890b26ec34cb","217f205297894f469c75ed0299e2811c","092f90f9b74f4cb6aa193d609c34f882","046614e4ca634ca299bf87263fcff97d","eb6fb60334a243379a8560fd0e155bbe","e8efce440d30465b8c26c1e4e26ff761","8959a054c6fd448e81f873d33531b8d4","88f7a819ea6149bbb3552eff51a81f37","df08567d5f99413b95aa9ee45569c191","a844930fb787424c8f6a778d5df54a0f","7ec9d9f2630e4cac97a6512b2faa2230","79a475f98248445db0139a3301493fbe","021e3768bdf449c1b4e0f2e1c0d07aa0","70cde0f9c934447e80f8124b961cd81d","47ae4883970c4d339b2f21a74ccdaf8c","793b9bf9231d4d529d28926e85c64302","bdda8c9bc3634a6894ca37e6eb8ff0c8","707cb13f5ff14b9c961f61a419c3660d","16e7c63350414f12b70ab8ffe3cc1664","ffef79f8dde4489ba108af5422b4f837","c64a75b5a77c41228c0cc00225faba42","d41ddc60d37e4436b2e7414e0c5d5a65","0a047b552d5b4c179453c4afda7ba89a","8315ce042ce945549fd198d7b4cb3d78","c883463a5b2b41b8b293437b0fb87edb","74c83541d6474bd391da1d41d46ed6dd","c5c031ff8c86479caf49f954548b1a08","a55db7e85e5841eeac6ba943b9b5a118","c254b2673a624e7aa74391b4998f3eb0","79285b34a9ad4ec48d35d6b56ce69cc6","7205d163371b4d5b966050dbe3fb9bec","f758cc49cff543afa37a4be57eb86d60","7478ec008e664cec9dcc38fd1ad6a2e2"]},"executionInfo":{"elapsed":125409,"status":"ok","timestamp":1767353725454,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"yeEE_WbAxYOX","outputId":"54556c95-5169-459d-dcd7-b7551e5e463f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Loading: models/qwen3-4b-sft | Mode: Non-CoT\n","INFO 01-02 11:33:21 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 11:33:21 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 69.6%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n","Unsloth: vLLM's KV Cache can use up to 52.34 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 11:33:31 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.6960016128672332, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 11:33:50 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"]},{"name":"stderr","output_type":"stream","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:33:50 [__init__.py:1815] Using max model len 4096\n","WARNING 01-02 11:33:50 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n","INFO 01-02 11:33:53 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n","WARNING 01-02 11:33:53 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n","INFO 01-02 11:33:56 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n","INFO 01-02 11:33:57 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n","WARNING 01-02 11:33:57 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n","INFO 01-02 11:33:57 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 11:33:57 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 11:33:58 [cuda.py:362] Using Flash Attention backend on V1 engine.\n","INFO 01-02 11:33:58 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 11:33:59 [weight_utils.py:348] Using model weights format ['*.safetensors']\n","INFO 01-02 11:34:00 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52dae0c139574d6b97726e49fbc4f499","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"260af0bd31c543f78ac7890b26ec34cb","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:34:01 [punica_selector.py:19] Using PunicaWrapperGPU.\n","INFO 01-02 11:34:03 [gpu_model_runner.py:2392] Model loading took 3.3602 GiB and 3.964858 seconds\n","INFO 01-02 11:34:19 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ca83b4ba7b/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 11:34:19 [backends.py:550] Dynamo bytecode transform time: 15.01 s\n","INFO 01-02 11:34:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.177 s\n","INFO 01-02 11:34:29 [monitor.py:34] torch.compile takes 15.01 s in total\n","INFO 01-02 11:34:31 [gpu_worker.py:298] Available KV cache memory: 51.01 GiB\n","INFO 01-02 11:34:32 [kv_cache_utils.py:864] GPU KV cache size: 371,408 tokens\n","INFO 01-02 11:34:32 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 90.68x\n","INFO 01-02 11:34:32 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"name":"stderr","output_type":"stream","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:11<00:00,  3.09it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:06<00:00,  3.01it/s]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:34:50 [gpu_model_runner.py:3118] Graph capturing finished in 18 secs, took 1.21 GiB\n","INFO 01-02 11:34:50 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 18 secs.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:34:51 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.6960016128672332, 55.21 GiB). Actual usage is 3.36 GiB for weight, 0.82 GiB for peak activation, 0.02 GiB for non-torch memory, and 1.21 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=53313622835` to fit into requested memory, or `--kv-cache-memory=78638161920` to fully utilize gpu memory. Current kv cache memory in use is 54766949171 bytes.\n","INFO 01-02 11:34:52 [core.py:218] init engine (profile, create kv cache, warmup model) took 48.99 seconds\n","INFO 01-02 11:34:54 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 11:34:54 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm2', 'q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm1', 'ffn_norm', 'k_norm', 'layer_norm1', 'norm', 'input_layernorm', 'post_attention_layernorm', 'norm2', 'attention_norm', 'post_layernorm']\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm2', 'cross_attn_input_layernorm', 'q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm1', 'ffn_norm', 'k_norm', 'layer_norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'post_attention_layernorm', 'norm2', 'attention_norm', 'post_layernorm']\n"]},{"name":"stderr","output_type":"stream","text":["Unsloth 2025.12.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"]},{"name":"stdout","output_type":"stream","text":[" Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79a475f98248445db0139a3301493fbe","version_major":2,"version_minor":0},"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a047b552d5b4c179453c4afda7ba89a","version_major":2,"version_minor":0},"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" Saved 164 samples to: data/evaluation/generations/sft_non_cot.jsonl\n"]}],"source":["# SFT MODEL (Non-CoT)\n","evaluate_model(\n","    model_path=SFT_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=False,\n","    output_jsonl=str(GEN_DIR / \"sft_non_cot.jsonl\"),\n","    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n","    temperature=TEMP_NON_COT,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7de28f0af56c4637a5b0cdb56d41c24a","1d9e6327d2c94c3e9de690690d1d6dc1","bc36fec764324a3e8ad803bc0c6ab6a8","f4a2bc05061d41f4a0b364869195bf18","24cf7cf9a96841a7a45e6ace272bb7f9","58a0cbda19be4dadbb5d70b7ae610591","9e101b7855654afaa261a8e50f45e43c","3673be0514cd4717bda5e780e3ea1869","401127bea538400a9e9abd3cd529119d","cb74370ac3994457854b43b8c386e160","995ed386e17e4d90a6f9298ed9fc8ca3","e15a72c48a2645fb90ba60a9478ab96a","31f979ec37a64274960741b6926103d4","6e02b33090924ae786c53455e275854c","880dd4210db74f748d60539ddcf41b16","8faebbfbda944821afe9cf9e53117336","97de53a5ca994d99886f35ed36315985","7717dcdf798e430e84dafc68a4c27801","09ae04adcc774d70ba03e0d20f0b74f0","94983102be2d45e98c0ba17fbee21416","bf75a9c603544760911027e214ffb604","b84ead8524e243f59ad6bd05a8794f79","b38723147a504817a4db00afa4568c48","d78dfe1df6f6466b82dfda067d163019","e8e7aa6b20f0498db47ec6dde1e28828","1ea3a70931b344ba8aac50cc764b9078","8e67fb40859248d5b6578c80a5d95fb9","9a3d4e2138f64e0ba949d908484a932a","8fd88a2a7f7340ec93d3f6a241f5c577","52d418e654e74fd7b80e7e26d1c8c7a7","c012574146c345dcaa2342fe1b02dcb5","e2049941b8104e9eaf5986887fd0b222","2e92afebe0ec445cbd968b1cbc0eae8c","c28de286b97749479e130f0ffa8ae920","62b05eae8327439e81ed605f77178b8e","d4950467d4cd4ba7abfeba3e458e5800","82dee44ec54b4995b2cbd4e0f0884837","19dafd90627144e48934c47daa080c5e","95949ab50ea644a0bd60787b55c08eb7","44856bac63de4e62b16a4f04c1e5deb9","002556d328684773b0029a9a816807bd","b64d826707a040ca8e606cc619ce1d4c","717a62bb0b62431299b2e969d9bbbca4","00b46237fc6e4d8eb5fb58335134c7f5"]},"executionInfo":{"elapsed":196739,"status":"ok","timestamp":1767352723316,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"LwL8oA_SMkdv","outputId":"2b81fa11-4237-41ce-a40d-4a68486d8c76"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Loading: models/qwen3-4b-sft | Mode: CoT\n","INFO 01-02 11:15:28 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 11:15:28 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 6.49%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 16.\n","Unsloth: vLLM's KV Cache can use up to 2.28 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 11:15:38 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.06489528290199138, 'max_num_batched_tokens': 2048, 'max_num_seqs': 16, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 11:15:40 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n","INFO 01-02 11:15:40 [__init__.py:1815] Using max model len 4096\n","INFO 01-02 11:15:41 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","WARNING 01-02 11:15:41 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n","INFO 01-02 11:15:44 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":32,\"local_cache_dir\":null}\n","INFO 01-02 11:15:46 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 11:15:47 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 11:15:47 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 11:15:47 [weight_utils.py:348] Using model weights format ['*.safetensors']\n","INFO 01-02 11:15:48 [weight_utils.py:369] Time spent downloading weights for unsloth/qwen3-4b-base-unsloth-bnb-4bit: 0.878051 seconds\n","INFO 01-02 11:15:49 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7de28f0af56c4637a5b0cdb56d41c24a","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e15a72c48a2645fb90ba60a9478ab96a","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:15:53 [gpu_model_runner.py:2392] Model loading took 3.3747 GiB and 3.275754 seconds\n","INFO 01-02 11:16:08 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/392892021c/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 11:16:08 [backends.py:550] Dynamo bytecode transform time: 14.29 s\n"]},{"name":"stderr","output_type":"stream","text":["Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 500.45it/s, triton_poi_fused_view_6]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:16:10 [backends.py:194] Cache the graph for dynamic shape for later use\n"]},{"name":"stderr","output_type":"stream","text":["\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 536.58it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 555.90it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 532.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 532.19it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 516.41it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 509.57it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 536.51it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 535.46it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 535.42it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 534.71it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 511.74it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 519.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 534.29it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 484.97it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 494.36it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 509.05it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 531.44it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 482.41it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 523.78it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 479.50it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 519.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 517.11it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 544.71it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 524.11it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 515.59it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 530.83it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 518.65it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 536.99it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 523.89it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 524.21it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 509.98it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 518.18it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 520.71it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 521.06it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 499.24it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 466.56it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:16:19 [backends.py:215] Compiling a graph for dynamic shape takes 9.29 s\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:16:22 [monitor.py:34] torch.compile takes 23.58 s in total\n","INFO 01-02 11:16:25 [gpu_worker.py:298] Available KV cache memory: 1.53 GiB\n","INFO 01-02 11:16:27 [kv_cache_utils.py:864] GPU KV cache size: 11,136 tokens\n","INFO 01-02 11:16:27 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 2.72x\n","INFO 01-02 11:16:27 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n","INFO 01-02 11:16:27 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n","INFO 01-02 11:16:27 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"name":"stderr","output_type":"stream","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.95it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.85it/s]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:16:31 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.05 GiB\n","INFO 01-02 11:16:31 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n","INFO 01-02 11:16:31 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n","INFO 01-02 11:16:31 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 11:16:34 [gpu_worker.py:391] Free memory on device (7.35/79.32 GiB) on startup. Desired GPU memory utilization is (0.06489528290199138, 5.15 GiB). Actual usage is 3.37 GiB for weight, 0.24 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=1429225369` to fit into requested memory, or `--kv-cache-memory=3797919232` to fully utilize gpu memory. Current kv cache memory in use is 1643134873 bytes.\n","INFO 01-02 11:16:34 [core.py:218] init engine (profile, create kv cache, warmup model) took 41.58 seconds\n","INFO 01-02 11:16:37 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 11:16:37 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'cross_attn_input_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"," Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b38723147a504817a4db00afa4568c48","version_major":2,"version_minor":0},"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c28de286b97749479e130f0ffa8ae920","version_major":2,"version_minor":0},"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" Saved 164 samples to: data/evaluation/generations/sft_cot.jsonl\n"]}],"source":["# SFT MODEL (CoT)\n","evaluate_model(\n","    model_path=SFT_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=True,\n","    output_jsonl=str(GEN_DIR / \"sft_cot.jsonl\"),\n","    max_new_tokens=MAX_NEW_TOKENS_COT,\n","    temperature=TEMP_COT,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["01636da2ef524f05bd829019181ad415","67e58f0969b4499eba66fda09a242e1a","eae19252908244f4b68dc8f154acd083","05ae6026e8f346cda8a5a0a7d00e17fa","0bb0fbdda5db4e43b608c8375acde029","0b2b0f2f12584bb9bb1d2c67598889f7","54f338234f564c2ba54c99e847d7d693","e122dcb142514ce98c92bebb1598c7c7","b601e6a29afb4cee8a5007721d830151","9ea6415cddfc492da92768c335938cbf","8a95701cc0db4d6e9caa0420900d12db","399df7cd8a69427399578dbc3713acbf","b515915e02a84bde8915d354a3326e15","12bfca7c2d934018b46e508c750bff7d","662b00ad837646259828b7678beb5003","c2c15b3d54a443f2bf9cc5b9ca7b271e","92c75be1e78045079d54dcbe6e158fdb","372366e7b54a46f8a71d4cb61e4b7785","35e2a566afea4b3497d1fa2b5813db97","6aee1c7ac1cf4694892915a00f402d12","37e80677b07f4835bce151f80f8a573c","f749c55db6364c5d85302b1997937a3c","978e367ad00148cbbb4485225df7715d","42bd5c84229b4fbdab91471e87fcdb03","0693a1c390574722864ea0b2b171c052","3e0347ff60ec41deb9eb73e8c15e5d7a","faf0dc4eafc24f35b9f22d0d6133980f","cfad52bc2ebe46069f74fe208d6f165d","253ba4d2c7d447228b04602ac7c7c723","85666a43f063430f83dc5a3593254030","5e4087d7ccae4dca8d82f7836a922f4a","f608275508a4427aad84b873ab022d4d","eae556a8e0a84035843db7a56a95be5b","0676ecabe6164cf6aa4844ca5030d9e8","774b577f72094f8f81fa3703229936b7","5785a50bed224f7fa653bd8d993b7be7","3d396ac3fd684c9c8d35986bf186d5a3","2d6d2d2997c84e4897ee1a0da0c6b8d6","f52a92a6e435447796b75c3f03ba95ad","2ad6e74d83fb42ff91a48b6eb35438dc","73a14414d22c4873b9a693fdaeeda3d2","25f492a9e83946a5a040b5b9b82e2530","f6e1995166fc45b6ab6b6350da695e74","d97053da18934fdc9081aa47132cd3ca"]},"executionInfo":{"elapsed":168736,"status":"ok","timestamp":1767350981604,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"8KMQzj0nVx6C","outputId":"3ff6fb12-5750-44f0-8278-bc1c25298a07"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Loading: models/qwen3-4b-grpo-final | Mode: Non-CoT\n","INFO 01-02 10:46:53 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 10:46:53 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 20.41%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 64.\n","Unsloth: vLLM's KV Cache can use up to 13.33 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 10:47:03 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.20411650503366935, 'max_num_batched_tokens': 6144, 'max_num_seqs': 64, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 10:47:05 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n","INFO 01-02 10:47:05 [__init__.py:1815] Using max model len 4096\n","INFO 01-02 10:47:06 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=6144.\n","WARNING 01-02 10:47:06 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n","INFO 01-02 10:47:09 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":128,\"local_cache_dir\":null}\n","INFO 01-02 10:47:10 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 10:47:11 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 10:47:11 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 10:47:12 [weight_utils.py:348] Using model weights format ['*.safetensors']\n","INFO 01-02 10:47:13 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01636da2ef524f05bd829019181ad415","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"399df7cd8a69427399578dbc3713acbf","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:47:16 [gpu_model_runner.py:2392] Model loading took 3.3988 GiB and 2.904477 seconds\n","INFO 01-02 10:47:31 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/0550210604/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 10:47:31 [backends.py:550] Dynamo bytecode transform time: 14.14 s\n"]},{"name":"stderr","output_type":"stream","text":["Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 412.70it/s, triton_poi_fused_view_6]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:47:35 [backends.py:194] Cache the graph for dynamic shape for later use\n"]},{"name":"stderr","output_type":"stream","text":["\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 402.89it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 417.91it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 414.07it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 400.58it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 442.91it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 432.42it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 619.74it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 601.42it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 618.25it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 627.06it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 657.19it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 635.03it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 630.86it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 612.09it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 606.70it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 596.61it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 588.25it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 634.72it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 623.19it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 637.70it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 633.17it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 625.32it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 607.94it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 649.29it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 651.71it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 658.94it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 642.74it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 558.31it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 566.97it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 640.22it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 592.09it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 623.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 597.86it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 619.56it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 597.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 384.81it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:48:34 [backends.py:215] Compiling a graph for dynamic shape takes 62.09 s\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:48:41 [monitor.py:34] torch.compile takes 76.23 s in total\n","INFO 01-02 10:48:43 [gpu_worker.py:298] Available KV cache memory: 12.17 GiB\n","INFO 01-02 10:48:45 [kv_cache_utils.py:864] GPU KV cache size: 88,576 tokens\n","INFO 01-02 10:48:45 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 21.62x\n","INFO 01-02 10:48:45 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n","INFO 01-02 10:48:45 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"name":"stderr","output_type":"stream","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:06<00:00,  3.03it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:03<00:00,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:48:55 [gpu_model_runner.py:3118] Graph capturing finished in 10 secs, took 0.27 GiB\n","INFO 01-02 10:48:55 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 10 secs.\n","INFO 01-02 10:48:55 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 10 secs.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:48:57 [gpu_worker.py:391] Free memory on device (23.13/79.32 GiB) on startup. Desired GPU memory utilization is (0.20411650503366935, 16.19 GiB). Actual usage is 3.4 GiB for weight, 0.62 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.27 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=12618304614` to fit into requested memory, or `--kv-cache-memory=20068607488` to fully utilize gpu memory. Current kv cache memory in use is 13062900838 bytes.\n","INFO 01-02 10:48:57 [core.py:218] init engine (profile, create kv cache, warmup model) took 101.60 seconds\n","INFO 01-02 10:49:00 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 10:49:00 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'cross_attn_input_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"," Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"978e367ad00148cbbb4485225df7715d","version_major":2,"version_minor":0},"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0676ecabe6164cf6aa4844ca5030d9e8","version_major":2,"version_minor":0},"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" Saved 164 samples to: data/evaluation/generations/grpo_non_cot.jsonl\n"]}],"source":["# GRPO MODEL (Non-CoT)\n","evaluate_model(\n","    model_path=GRPO_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=False,\n","    output_jsonl=str(GEN_DIR / \"grpo_non_cot.jsonl\"),\n","    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n","    temperature=TEMP_NON_COT,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6a52934b8aeb45759a61e500c278e665","a190852104c14a6b87d719ebb8d6d912","7f98c8589b024b5bbe6d5c929b3b7981","1ec9bd92a4484cceae354ee4e1e5d890","f1adb3732fda47ec87c5511f8e23a659","e465c758682b457a80dca75b9530ef2c","a59bd86f96234614a60d74b60bfe5649","9a25a1efebd64039a785bf52b9176e20","84e834cc22694b31bc5604d67ac3bd6a","5ebaca34db3a4af1ac14e497f36f2d40","e16056f7f5304a4599c1fb65cbcd3ba2","9b808453ba704088a51e44ad8178c382","cee52e56dfa041be92d5947dde06f368","dfcdb300a18d4a8ca2e7ace50af48b20","f8a082d4c99f4b2e9d8543e5804f582f","bd5d748cb59540979509700c5ae30ef1","4247a676c68c4789bbbae0b5d961cf71","a7ff023db6cf459cb80a367b415d0a2c","c7a5a2454cf2412bb025c6c0792486a1","90691bb2fcf24790ac631d4e7c25d0f8","2480c3a740fd458ab468ad5668c04b11","9bdd4e57669b4cbfaa46e0bab8dc876d","a5c3561275e6474dab13cc8f021c8e01","0cef8a3cf8da4169bdb6a05676286027","9bb210d9db9f4fb787c57ec583a0cdd3","da9eabda62c842b6a7b21ab3661fb9d5","feb7cfa915224a47a8fae486ddb19578","7d8e7f625a99436f8387755ac12b784b","993e54c9466647bc8ad441860cb7623a","117c534d1b8742fe93d98856bfa6abd5","3862ea0c4555471685d890f321b1079a","55d809342c2c4ef48b5fea053589e62a","110b364877ff49d98ed6e3ce9045fc44","1cc41dd180934482a99716883c23867e","2081cec9668146a4aacaec480a3ac5b5","0375f74fe69c4b59bc128504e2755c09","e4fe97664f6e48e1bc8085ba551e62c0","1e3528036327443d953d8e70d7ae08eb","9020ef6705094a1eb845855835337f3e","db71615d176740cf94f4910a3d913527","f65139456d4846d39a07ea70d69beb26","38819b3450754189a2f5e92412e14a41","0d74d1891d464d2292c0d767b604c1f4","751595b0364241e38b47a53c7e231ff5","6febfdb59c0541618e5c749c7bd2cd50","c9b174d079e94e49b6bfbffe33f56534","92f7620bd45149e698c21a82127c816c","f4c7661dcd2441a1b7e9fbc018b667e6","65623eb5d7fb466bbbf429235302502d","d246786626444978b385c55076c3a7d2","40abe243d4ca49558cf9b064090f0b40","a70348ff14134a9b9a3ec97207b437a1","bd195ac815f7440a82d521e21b995ca6","65cac6e4708c4056b1db0ce49fbdc2b5","6dba11abceee4fb48a97a4cb218e3857","483ed27dfc6c4369bb7871eccee293fb","3b0f99baf5f349438efc7bb84926e4f7","41653c2073f24328b8ca93939aaea5fd","1de2c3c29e9e4c55a4137aadcfc1f8d6","707f0ab41ca94e02b7404e07aa084640","49c53d2edc8346db97c63010fefe60da","2b6ebe945a9e4234a3588f375bfdb260","e3d088f42df44726b9bff0dd74f4f9c8","6cd1ad25ccc04a99a1bcebfbb91ffa6a","07e4304b9ea441ecbbd30c0571444390","2820755d5c0f4606887533770945491a","c657d343415b4b3a9225353461da0127","3299928a9e8d416fbb2fdcf5c71e2750","ecbc030dafa54caaa45a0c703ac0f839","97ab3d4bb5ca495e90ba7ed0153b6031","53088384236e4e8abb81dca3e806ad8f","726975d1daa64681846aa895e1668462","52102dc7f3de418db0eeb2003e2c7839","f5075b7e58944ff28c9e65fbe067ee60","ba9d36b4c3424fb1aa0bd65e1a063d89","18693ab9f5a14286838e64708c340342","1ee5407a426d479d87332b5b3736d7ad","60d978e69eb845c982be256dbfbcd1b4","c46817662793409b832d061ce92dd303","ec96dd4bc024433fbb93663f6502f5d6","d3c5a796136e493bb80c93b9b76de073","4518a0c1d8d64ac088c8cb82029a2b77","93fa4c4bcc3042049bb10a24664809b9","d74de1a39b0e4eeb982b4f1b7e8677cb","88b72e6b233446d4b50346a5ce28a9b6","0e9021ee9b0244c19d854c7ea2a4f074","8f3f3517ce3447a58805e6d9fdc3db80","10e68b002d834300979c6767e66d6d02","86323caba22c46e782f5832eecebd5de","d7fe978f265f4a2d9e2b12d402ec8cc2","03437ec77b3a432fbdc777474add756a","02cc0476b6ae421eb81ba325c6996371","6f15c78aa1aa42f0b2cf87f22e922b02","c1be894066ff427c9aef7bb583377879","da5f2302c4a54c4e8d30d3e4a75a6e38","b1245728220a4c87a4119b9024855370","87d7e934e3f943b38de617d1a8ac90d2","50d7b70d9ef34c2abe455cdd3458b0f9","8700892b68af447185684a97ef82ce54","52408356313d491fbb4683ea50d8fbbe","c0c2afd307cc46308cbaffc2fe6c383e","7e8de4401f1c4207becd8f7940f62f48","e36740ed19724c8b8aa3a7140c66419b","5172a48ab2824b698228675d83440009","b58610314bde4114a4a67e6fcd3f9070","54671f66e10f48e7bace465567a9f060","31b041d77a8340cfa6663ec04f1479f4","3359e171713a44b8ae0dd8e4e0bd36ab","3335174f4d3b4e678634ea50ddef4546","7eea11cd89f14c57aef438fdd5afa5d3","bac00b7731784f69851d018971052134","991a12fd57cf439f9f6e885cbf43549a","24fea4efafc543a4861b49fe5ec4dac8","acf72109026e4e08834702a740eb5db9","d2aca9da4bf34abdb0b67f0a42c449cb","ada4ae8d125e429389fcefb06a4e1747","61c4ce9466e0466eaf3348cf8a8d52a1","037ed8a909f747d195390c8e3e389399","cc7084dd3f0e444699e5a7cd85ac4397","c5bf30d2a4484da38780ac37e21c2a5f","61bde22bcfcd4075baa764f66bbe6232","c2ac4aa106d34d508f3e1d486cd77177","cf7aa0942dbe4655affdbbf8a61ae6a2","f476bdf66a2742b8a484eba4b477e0f3","f239be2beaff4fea9c71d44e9dd30f5f","fd743b489bab4da8a1b806892f45d279","9430edff75944a3db491f7b93f779ae5","80a22037f8da4c41b7758e9c7c80e87a","c21dc12cdcfe401585c273664157d0c7","0748ee2cf20e45cb926b283f31a6b935","a1b28075152e4d70bed129ed40bfb359","f1058bf3b3e24b0b83e3c1320fd1346c"]},"executionInfo":{"elapsed":249480,"status":"ok","timestamp":1767349014253,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"YNRyAc4MAAcP","outputId":"411d5d18-af11-47ac-d7b1-5c062a86c621"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Loading: models/qwen3-4b-grpo-final | Mode: CoT\n","INFO 01-02 10:12:47 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 10:12:47 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 69.6%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n","Unsloth: vLLM's KV Cache can use up to 52.34 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 10:12:57 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.6960016128672332, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 10:13:17 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"]},{"name":"stderr","output_type":"stream","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:13:17 [__init__.py:1815] Using max model len 4096\n","WARNING 01-02 10:13:17 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n","INFO 01-02 10:13:20 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n","WARNING 01-02 10:13:20 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a52934b8aeb45759a61e500c278e665","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b808453ba704088a51e44ad8178c382","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5c3561275e6474dab13cc8f021c8e01","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1cc41dd180934482a99716883c23867e","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6febfdb59c0541618e5c749c7bd2cd50","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"483ed27dfc6c4369bb7871eccee293fb","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c657d343415b4b3a9225353461da0127","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:13:27 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n","INFO 01-02 10:13:28 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n","WARNING 01-02 10:13:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n","INFO 01-02 10:13:28 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 10:13:28 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 10:13:29 [cuda.py:362] Using Flash Attention backend on V1 engine.\n","INFO 01-02 10:13:29 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 10:13:30 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60d978e69eb845c982be256dbfbcd1b4","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/3.32G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:13:39 [weight_utils.py:369] Time spent downloading weights for unsloth/qwen3-4b-base-unsloth-bnb-4bit: 8.815815 seconds\n","INFO 01-02 10:13:39 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86323caba22c46e782f5832eecebd5de","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52408356313d491fbb4683ea50d8fbbe","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:13:41 [punica_selector.py:19] Using PunicaWrapperGPU.\n","INFO 01-02 10:13:42 [gpu_model_runner.py:2392] Model loading took 3.3825 GiB and 12.377858 seconds\n","INFO 01-02 10:13:58 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ca83b4ba7b/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 10:13:58 [backends.py:550] Dynamo bytecode transform time: 15.21 s\n"]},{"name":"stderr","output_type":"stream","text":["Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 10.44it/s, triton_poi_fused_view_6]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:14:05 [backends.py:194] Cache the graph for dynamic shape for later use\n"]},{"name":"stderr","output_type":"stream","text":["\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 19.16it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 347.40it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 326.81it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 355.44it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 334.50it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 327.11it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 463.73it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 472.54it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 454.92it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 469.06it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 483.57it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 461.87it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 462.39it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 467.36it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 461.77it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 466.29it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 475.26it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 461.62it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 476.88it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 501.06it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 494.11it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 493.94it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 492.50it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 501.82it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 453.55it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 471.54it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 498.16it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 479.40it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 456.33it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 480.59it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 513.26it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 437.21it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 517.29it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 487.53it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 441.83it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 24.52it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:15:03 [backends.py:215] Compiling a graph for dynamic shape takes 62.63 s\n","INFO 01-02 10:15:26 [monitor.py:34] torch.compile takes 77.84 s in total\n","INFO 01-02 10:15:28 [gpu_worker.py:298] Available KV cache memory: 50.98 GiB\n","INFO 01-02 10:15:29 [kv_cache_utils.py:864] GPU KV cache size: 371,216 tokens\n","INFO 01-02 10:15:29 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 90.63x\n","INFO 01-02 10:15:29 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"name":"stderr","output_type":"stream","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:20<00:00,  1.71it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:06<00:00,  3.02it/s]"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:15:56 [gpu_model_runner.py:3118] Graph capturing finished in 27 secs, took 1.21 GiB\n","INFO 01-02 10:15:56 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 27 secs.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["INFO 01-02 10:15:58 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.6960016128672332, 55.21 GiB). Actual usage is 3.38 GiB for weight, 0.82 GiB for peak activation, 0.02 GiB for non-torch memory, and 1.21 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=53277184819` to fit into requested memory, or `--kv-cache-memory=78601723904` to fully utilize gpu memory. Current kv cache memory in use is 54738899763 bytes.\n","INFO 01-02 10:15:58 [core.py:218] init engine (profile, create kv cache, warmup model) took 135.94 seconds\n","INFO 01-02 10:16:01 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 10:16:01 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'cross_attn_input_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"]},{"name":"stderr","output_type":"stream","text":["Unsloth 2025.12.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"]},{"name":"stdout","output_type":"stream","text":[" Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bac00b7731784f69851d018971052134","version_major":2,"version_minor":0},"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2ac4aa106d34d508f3e1d486cd77177","version_major":2,"version_minor":0},"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" Saved 164 samples to: data/evaluation/generations/grpo_cot.jsonl\n"]}],"source":["# GRPO MODEL (CoT)\n","evaluate_model(\n","    model_path=GRPO_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=True,\n","    output_jsonl=str(GEN_DIR / \"grpo_cot.jsonl\"),\n","    max_new_tokens=2048, # Increased for safety, vLLM handles the speed\n","    temperature=TEMP_COT,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110,"status":"ok","timestamp":1767349396483,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"-glzNHeLCvx8","outputId":"8f665da8-8e29-4768-df4e-a0bfb5e95a27"},"outputs":[{"name":"stdout","output_type":"stream","text":[" INSPECTING: data/evaluation/generations/grpo_cot.jsonl\n","\n"," TASK: HumanEval/0\n","--------------------------------------------------------------------------------\n","    for i in range(len(numbers)):\n","        for j in range(i+1, len(numbers)):\n","            if abs(numbers[i] - numbers[j]) < threshold:\n","                return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/1\n","--------------------------------------------------------------------------------\n","    paren_string = paren_string.replace(\" \", \"\")\n","    groups = []\n","    counter = 0\n","    current_group = \"\"\n","    for char in paren_string:\n","        if char == \"(\":\n","            counter += 1\n","            current_group += char\n","        elif char == \")\":\n","            counter -= 1\n","            current_group += char\n","            if counter == 0:\n","                groups.append(current_group)\n","                current_group = \"\"\n","    return groups\n","\n","================================================================================\n","\n"," TASK: HumanEval/2\n","--------------------------------------------------------------------------------\n","    integer_part = int(number)\n","    decimal_part = number - integer_part\n","    return decimal_part\n","\n","================================================================================\n","\n"," TASK: HumanEval/3\n","--------------------------------------------------------------------------------\n","    balance = 0\n","    for op in operations:\n","        balance += op\n","        if balance < 0:\n","            return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/4\n","--------------------------------------------------------------------------------\n","    x_mean = sum(numbers) / len(numbers)\n","    return sum(abs(x - x_mean) for x in numbers) / len(numbers)\n","\n","================================================================================\n","\n"," TASK: HumanEval/5\n","--------------------------------------------------------------------------------\n","    result = []\n","    for i, num in enumerate(numbers):\n","        result.append(num)\n","        if i < len(numbers) - 1:\n","            result.append(delimeter)\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/6\n","--------------------------------------------------------------------------------\n","    result = []\n","    groups = paren_string.split(' ')\n","    for group in groups:\n","        max_nesting_level = 0\n","        current_nesting_level = 0\n","        for char in group:\n","            if char == '(':\n","                current_nesting_level += 1\n","            elif char == ')':\n","                current_nesting_level -= 1\n","            if current_nesting_level > max_nesting_level:\n","                max_nesting_level = current_nesting_level\n","        result.append(max_nesting_level)\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/7\n","--------------------------------------------------------------------------------\n","    filtered_strings = [s for s in strings if substring in s]\n","    return filtered_strings\n","\n","================================================================================\n","\n"," TASK: HumanEval/8\n","--------------------------------------------------------------------------------\n","    sum_result = 0\n","    product_result = 1\n","    for num in numbers:\n","        sum_result += num\n","        product_result *= num\n","    return (sum_result, product_result)\n","\n","================================================================================\n","\n"," TASK: HumanEval/9\n","--------------------------------------------------------------------------------\n","    max_list = []\n","    current_max = float('-inf')\n","    for num in numbers:\n","        if num > current_max:\n","            current_max = num\n","        max_list.append(current_max)\n","    return max_list\n","\n","================================================================================\n","\n"," TASK: HumanEval/10\n","--------------------------------------------------------------------------------\n","    if not string:\n","        return string\n","    for i in range(len(string), -1, -1):\n","        if is_palindrome(string[:i]):\n","            return string + string[:i][::-1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/11\n","--------------------------------------------------------------------------------\n","    return ''.join([format(ord(a[i]) ^ ord(b[i]), '08b')[-1] for i in range(len(a))])\n","\n","================================================================================\n","\n"," TASK: HumanEval/12\n","--------------------------------------------------------------------------------\n","    if not strings:\n","        return None\n","    max_length = 0\n","    longest_string = None\n","    for string in strings:\n","        if len(string) > max_length:\n","            max_length = len(string)\n","            longest_string = string\n","    return longest_string\n","\n","================================================================================\n","\n"," TASK: HumanEval/13\n","--------------------------------------------------------------------------------\n","    while b:\n","        a, b = b, a % b\n","    return a\n","\n","================================================================================\n","\n"," TASK: HumanEval/14\n","--------------------------------------------------------------------------------\n","    prefixes = []\n","    for i in range(1, len(string) + 1):\n","        prefixes.append(string[:i])\n","    return prefixes\n","\n","================================================================================\n","\n"," TASK: HumanEval/15\n","--------------------------------------------------------------------------------\n","    result = \"\"\n","    for i in range(n + 1):\n","        result += str(i) + \" \"\n","    return result.rstrip()\n","\n","================================================================================\n","\n"," TASK: HumanEval/16\n","--------------------------------------------------------------------------------\n","    return len(set(string.lower()))\n","\n","================================================================================\n","\n"," TASK: HumanEval/17\n","--------------------------------------------------------------------------------\n","    note_to_beats = {\n","        'o': 4,\n","        'o|': 2,\n","        '.|': 1\n","    }\n","\n","    beats = []\n","    i = 0\n","    while i < len(music_string):\n","        if music_string[i] == 'o':\n","            beats.append(note_to_beats['o'])\n","            i += 1\n","        elif music_string[i:i+2] == 'o|':\n","            beats.append(note_to_beats['o|'])\n","            i += 2\n","        elif music_string[i:i+2] == '.|':\n","            beats.append(note_to_beats['.|'])\n","            i += 2\n","        else:\n","            i += 1\n","\n","    return beats\n","\n","================================================================================\n","\n"," TASK: HumanEval/18\n","--------------------------------------------------------------------------------\n","    count = 0\n","    for i in range(len(string) - len(substring) + 1):\n","        if string[i:i+len(substring)] == substring:\n","            count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/19\n","--------------------------------------------------------------------------------\n","    word_to_num = {\n","        'zero': 0,\n","        'one': 1,\n","        'two': 2,\n","        'three': 3,\n","        'four': 4,\n","        'five': 5,\n","        'six': 6,\n","        'seven': 7,\n","        'eight': 8,\n","        'nine': 9\n","    }\n","    num_to_word = {v: k for k, v in word_to_num.items()}\n","    words = numbers.split()\n","    nums = [word_to_num[word] for word in words]\n","    nums.sort()\n","    sorted_words = [num_to_word[num] for num in nums]\n","    return ' '.join(sorted_words)\n","\n","================================================================================\n","\n"," TASK: HumanEval/20\n","--------------------------------------------------------------------------------\n","    numbers.sort()\n","    min_diff = float('inf')\n","    closest_pair = (numbers[0], numbers[0])\n","    for i in range(1, len(numbers)):\n","        diff = numbers[i] - numbers[i-1]\n","        if diff < min_diff:\n","            min_diff = diff\n","            closest_pair = (numbers[i-1], numbers[i])\n","    return closest_pair\n","\n","================================================================================\n","\n"," TASK: HumanEval/21\n","--------------------------------------------------------------------------------\n","    min_val = min(numbers)\n","    max_val = max(numbers)\n","    range_val = max_val - min_val\n","    return [(x - min_val) / range_val for x in numbers]\n","\n","================================================================================\n","\n"," TASK: HumanEval/22\n","--------------------------------------------------------------------------------\n","    integers = []\n","    for value in values:\n","        if isinstance(value, int):\n","            integers.append(value)\n","    return integers\n","\n","================================================================================\n","\n"," TASK: HumanEval/23\n","--------------------------------------------------------------------------------\n","    return len(string)\n","\n","================================================================================\n","\n"," TASK: HumanEval/24\n","--------------------------------------------------------------------------------\n","    for i in range(n-1, 0, -1):\n","        if n % i == 0:\n","            return i\n","\n","================================================================================\n","\n"," TASK: HumanEval/25\n","--------------------------------------------------------------------------------\n","    factors = []\n","    divisor = 2\n","    while n > 1:\n","        while n % divisor == 0:\n","            factors.append(divisor)\n","            n = n // divisor\n","        divisor += 1\n","    return factors\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/26\n","--------------------------------------------------------------------------------\n","    result = []\n","    seen = set()\n","    for num in numbers:\n","        if num not in seen:\n","            result.append(num)\n","            seen.add(num)\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/27\n","--------------------------------------------------------------------------------\n","    result = \"\"\n","    for char in string:\n","        if char.islower():\n","            result += char.upper()\n","        elif char.isupper():\n","            result += char.lower()\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/28\n","--------------------------------------------------------------------------------\n","    return ''.join(strings)\n","\n","================================================================================\n","\n"," TASK: HumanEval/29\n","--------------------------------------------------------------------------------\n","    filtered_strings = [s for s in strings if s.startswith(prefix)]\n","    return filtered_strings\n","\n","================================================================================\n","\n"," TASK: HumanEval/30\n","--------------------------------------------------------------------------------\n","    positive_numbers = []\n","    for num in l:\n","        if num > 0:\n","            positive_numbers.append(num)\n","    return positive_numbers\n","\n","================================================================================\n","\n"," TASK: HumanEval/31\n","--------------------------------------------------------------------------------\n","    if n < 2:\n","        return False\n","    for i in range(2, int(n**0.5) + 1):\n","        if n % i == 0:\n","            return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/32\n","--------------------------------------------------------------------------------\n","    import math\n","\n","\n","    def poly(xs: list, x: float):\n","        return sum([coeff * math.pow(x, i) for i, coeff in enumerate(xs)])\n","\n","\n","        \"\"\" xs are coefficients of a polynomial.\n","        find_zero find x such that poly(x) = 0.\n","        find_zero returns only only zero point, even if there are many.\n","        Moreover, find_zero only takes list xs having even number of coefficients\n","        and largest non zero coefficient as it guarantees\n","        a solution.\n","        >>> round(find_zero([1, 2]), 2) # f(x) = 1 + 2x\n","        -0.5\n","        >>> round(find_zero([-6, 11, -6, 1]), 2) # (x - 1) * (x - 2) * (x - 3) = -6 + 11x - 6x^2 + x^3\n","        1.0\n","        \"\"\"\n","        if len(xs) % 2 == 0 and max(xs) > 0:\n","            for x in range(-100, 100):\n","                if round(poly(xs, x), 2) == 0:\n","                    return round(x, 2)\n","        return None\n","\n","================================================================================\n","\n"," TASK: HumanEval/33\n","--------------------------------------------------------------------------------\n","    # Extract elements at indices divisible by three\n","    third_elements = [l[i] for i in range(len(l)) if (i + 1) % 3 == 0]\n","\n","    # Sort the extracted elements\n","    third_elements_sorted = sorted(third_elements)\n","\n","    # Replace the elements at indices divisible by three with the sorted elements\n","    for i in range(len(third_elements_sorted)):\n","        l[i * 3 - 1] = third_elements_sorted[i]\n","\n","    return l\n","\n","================================================================================\n","\n"," TASK: HumanEval/34\n","--------------------------------------------------------------------------------\n","    return sorted(list(set(l)))\n","\n","================================================================================\n","\n"," TASK: HumanEval/35\n","--------------------------------------------------------------------------------\n","    if not l:\n","        return None\n","    max_val = l[0]\n","    for num in l[1:]:\n","        if num > max_val:\n","            max_val = num\n","    return max_val\n","\n","================================================================================\n","\n"," TASK: HumanEval/36\n","--------------------------------------------------------------------------------\n","    count = 0\n","    for i in range(1, n):\n","        if i % 11 == 0 or i % 13 == 0:\n","            count += str(i).count('7')\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/37\n","--------------------------------------------------------------------------------\n","    even_indices = [l[i] for i in range(0, len(l), 2)]\n","    even_indices.sort()\n","    l[::2] = even_indices\n","    return l\n","\n","================================================================================\n","\n"," TASK: HumanEval/38\n","--------------------------------------------------------------------------------\n","    def encode_cyclic(s: str):\n","        groups = [s[(3 * i):min((3 * i + 3), len(s))] for i in range((len(s) + 2) // 3)]\n","        groups = [(group[1:] + group[0]) if len(group) == 3 else group for group in groups]\n","        return \"\".join(groups)\n","\n","================================================================================\n","\n"," TASK: HumanEval/39\n","--------------------------------------------------------------------------------\n","    def is_prime(num: int) -> bool:\n","        if num < 2:\n","            return False\n","        for i in range(2, int(num ** 0.5) + 1):\n","            if num % i == 0:\n","                return False\n","        return True\n","\n","    fib_numbers = []\n","    a, b = 0, 1\n","    while len(fib_numbers) < n:\n","        a, b = b, a + b\n","        if is_prime(b):\n","            fib_numbers.append(b)\n","\n","    return fib_numbers[n - 1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/40\n","--------------------------------------------------------------------------------\n","    l.sort()\n","    n = len(l)\n","    for i in range(n - 2):\n","        if i > 0 and l[i] == l[i - 1]:\n","            continue\n","        left, right = i + 1, n - 1\n","        while left < right:\n","            total = l[i] + l[left] + l[right]\n","            if total == 0:\n","                return True\n","            elif total < 0:\n","                left += 1\n","            else:\n","                right -= 1\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/41\n","--------------------------------------------------------------------------------\n","    return n * n\n","\n","================================================================================\n","\n"," TASK: HumanEval/42\n","--------------------------------------------------------------------------------\n","    return [x + 1 for x in l]\n","\n","================================================================================\n","\n"," TASK: HumanEval/43\n","--------------------------------------------------------------------------------\n","    for i in range(len(l)):\n","        for j in range(i+1, len(l)):\n","            if l[i] + l[j] == 0:\n","                return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/44\n","--------------------------------------------------------------------------------\n","    result = \"\"\n","    while x > 0:\n","        result += str(x % base)\n","        x //= base\n","    return result[::-1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/45\n","--------------------------------------------------------------------------------\n","    return (a * h) / 2\n","\n","================================================================================\n","\n"," TASK: HumanEval/46\n","--------------------------------------------------------------------------------\n","    if n == 0 or n == 1:\n","        return 0\n","    elif n == 2:\n","        return 2\n","    elif n == 3:\n","        return 0\n","    else:\n","        a, b, c, d = 0, 0, 2, 0\n","        for _ in range(4, n + 1):\n","            a, b, c, d = b, c, d, a + b + c + d\n","        return d\n","\n","================================================================================\n","\n"," TASK: HumanEval/47\n","--------------------------------------------------------------------------------\n","    l.sort()\n","    n = len(l)\n","    if n % 2 == 1:\n","        return l[n // 2]\n","    else:\n","        return (l[n // 2 - 1] + l[n // 2]) / 2\n","\n","================================================================================\n","\n"," TASK: HumanEval/48\n","--------------------------------------------------------------------------------\n","    return text == text[::-1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/49\n","--------------------------------------------------------------------------------\n","    if n == 0:\n","        return 1\n","    if p == 1:\n","        return 0\n","    result = 1\n","    base = 2\n","    while n > 0:\n","        if n % 2 == 1:\n","            result = (result * base) % p\n","        base = (base * base) % p\n","        n = n // 2\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/50\n","--------------------------------------------------------------------------------\n","    def encode_shift(s: str):\n","        return \"\".join([chr(((ord(ch) + 5 - ord(\"a\")) % 26) + ord(\"a\")) for ch in s])\n","\n","================================================================================\n","\n"," TASK: HumanEval/51\n","--------------------------------------------------------------------------------\n","    return ''.join([char for char in text if char.lower() not in 'aeiou'])\n","\n","================================================================================\n","\n"," TASK: HumanEval/52\n","--------------------------------------------------------------------------------\n","    all_below = True\n","    for num in l:\n","        if num >= t:\n","            all_below = False\n","            break\n","    return all_below\n","\n","================================================================================\n","\n"," TASK: HumanEval/53\n","--------------------------------------------------------------------------------\n","    return x + y\n","\n","================================================================================\n","\n"," TASK: HumanEval/54\n","--------------------------------------------------------------------------------\n","    from collections import Counter\n","    return Counter(s0) == Counter(s1)\n","\n","================================================================================\n","\n"," TASK: HumanEval/55\n","--------------------------------------------------------------------------------\n","    a, b = 0, 1\n","    for _ in range(n):\n","        a, b = b, a + b\n","    return a\n","\n","================================================================================\n","\n"," TASK: HumanEval/56\n","--------------------------------------------------------------------------------\n","    balance = 0\n","    for char in brackets:\n","        if char == \"<\":\n","            balance += 1\n","        elif char == \">\":\n","            balance -= 1\n","        if balance < 0:\n","            return False\n","    return balance == 0\n","\n","================================================================================\n","\n"," TASK: HumanEval/57\n","--------------------------------------------------------------------------------\n","    if not l or len(l) == 1:\n","        return True\n","    increasing = True\n","    decreasing = True\n","    for i in range(len(l) - 1):\n","        if l[i] > l[i + 1]:\n","            increasing = False\n","        if l[i] < l[i + 1]:\n","            decreasing = False\n","    return increasing or decreasing\n","\n","================================================================================\n","\n"," TASK: HumanEval/58\n","--------------------------------------------------------------------------------\n","    return sorted(list(set(l1) & set(l2)))\n","\n","================================================================================\n","\n"," TASK: HumanEval/59\n","--------------------------------------------------------------------------------\n","    i = 2\n","    while i * i <= n:\n","        if n % i:\n","            i += 1\n","        else:\n","            n //= i\n","    return n\n","\n","================================================================================\n","\n"," TASK: HumanEval/60\n","--------------------------------------------------------------------------------\n","    return n * (n + 1) // 2\n","\n","================================================================================\n","\n"," TASK: HumanEval/61\n","--------------------------------------------------------------------------------\n","    balance = 0\n","    for char in brackets:\n","        if char == '(':\n","            balance += 1\n","        elif char == ')':\n","            balance -= 1\n","        if balance < 0:\n","            return False\n","    return balance == 0\n","\n","================================================================================\n","\n"," TASK: HumanEval/62\n","--------------------------------------------------------------------------------\n","    return [i * x for i, x in enumerate(xs)][1:]\n","\n","================================================================================\n","\n"," TASK: HumanEval/63\n","--------------------------------------------------------------------------------\n","    if n == 0 or n == 1:\n","        return 0\n","    elif n == 2:\n","        return 1\n","    else:\n","        dp = [0] * (n + 1)\n","        dp[0] = 0\n","        dp[1] = 0\n","        dp[2] = 1\n","        for i in range(3, n + 1):\n","            dp[i] = dp[i - 1] + dp[i - 2] + dp[i - 3]\n","        return dp[n]\n","\n","================================================================================\n","\n"," TASK: HumanEval/64\n","--------------------------------------------------------------------------------\n","    vowels = {'a', 'e', 'i', 'o', 'u'}\n","    count = sum(1 for char in s.lower() if char in vowels)\n","    if s.endswith('y'):\n","        count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/65\n","--------------------------------------------------------------------------------\n","    x_str = str(x)\n","    num_digits = len(x_str)\n","    if shift >= num_digits:\n","        return x_str[::-1]\n","    else:\n","        return x_str[-shift:] + x_str[:-shift]\n","\n","================================================================================\n","\n"," TASK: HumanEval/66\n","--------------------------------------------------------------------------------\n","    total = 0\n","    for char in s:\n","        if char.isupper():\n","            total += ord(char)\n","    return total\n","\n","================================================================================\n","\n"," TASK: HumanEval/67\n","--------------------------------------------------------------------------------\n","    # Extract the number of apples and oranges from the string\n","    num_apples = int(s.split(\"apples\")[0].split(\" \")[-1])\n","    num_oranges = int(s.split(\"oranges\")[0].split(\" \")[-1])\n","\n","    # Calculate the number of mangoes\n","    num_mangoes = n - num_apples - num_oranges\n","\n","    return num_mangoes\n","\n","================================================================================\n","\n"," TASK: HumanEval/68\n","--------------------------------------------------------------------------------\n","    smallest_even = float('inf')\n","    smallest_index = -1\n","    for i, num in enumerate(arr):\n","        if num % 2 == 0 and num < smallest_even:\n","            smallest_even = num\n","            smallest_index = i\n","    return [smallest_even, smallest_index] if smallest_even != float('inf') else []\n","\n","================================================================================\n","\n"," TASK: HumanEval/69\n","--------------------------------------------------------------------------------\n","    freq = {}\n","    for num in lst:\n","        if num in freq:\n","            freq[num] += 1\n","        else:\n","            freq[num] = 1\n","    for num in lst:\n","        if freq[num] >= num:\n","            return num\n","    return -1\n","\n","================================================================================\n","\n"," TASK: HumanEval/70\n","--------------------------------------------------------------------------------\n","    sorted_lst = []\n","    while lst:\n","        min_val = min(lst)\n","        sorted_lst.append(min_val)\n","        lst.remove(min_val)\n","        if lst:\n","            max_val = max(lst)\n","            sorted_lst.append(max_val)\n","            lst.remove(max_val)\n","    return sorted_lst\n","\n","================================================================================\n","\n"," TASK: HumanEval/71\n","--------------------------------------------------------------------------------\n","    if a + b > c and a + c > b and b + c > a:\n","        s = (a + b + c) / 2\n","        area = (s * (s - a) * (s - b) * (s - c)) ** 0.5\n","        return round(area, 2)\n","    else:\n","        return -1\n","\n","================================================================================\n","\n"," TASK: HumanEval/72\n","--------------------------------------------------------------------------------\n","    # Check if the list is balanced (palindromic)\n","    is_balanced = q == q[::-1]\n","\n","    # Calculate the sum of the elements\n","    total_weight = sum(q)\n","\n","    # Check if the sum is less than or equal to w\n","    can_fly = total_weight <= w\n","\n","    # Return True if both conditions are met\n","    return is_balanced and can_fly\n","\n","================================================================================\n","\n"," TASK: HumanEval/73\n","--------------------------------------------------------------------------------\n","    changes = 0\n","    for i in range(len(arr) // 2):\n","        if arr[i] != arr[-(i + 1)]:\n","            changes += 1\n","    return changes\n","\n","================================================================================\n","\n"," TASK: HumanEval/74\n","--------------------------------------------------------------------------------\n","    total1 = sum(len(s) for s in lst1)\n","    total2 = sum(len(s) for s in lst2)\n","    if total1 <= total2:\n","        return lst1\n","    else:\n","        return lst2\n","\n","================================================================================\n","\n"," TASK: HumanEval/75\n","--------------------------------------------------------------------------------\n","    if a >= 100:\n","        return False\n","\n","    def is_prime(n):\n","        if n < 2:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","    primes = [i for i in range(2, a + 1) if is_prime(i)]\n","\n","    for i in range(len(primes)):\n","        for j in range(i, len(primes)):\n","            for k in range(j, len(primes)):\n","                if primes[i] * primes[j] * primes[k] == a:\n","                    return True\n","    return False\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/76\n","--------------------------------------------------------------------------------\n","    while x > 1:\n","        if x % n != 0:\n","            return False\n","        x //= n\n","    return x == 1\n","\n","================================================================================\n","\n"," TASK: HumanEval/77\n","--------------------------------------------------------------------------------\n","    cube_root = round(a ** (1/3))\n","    return cube_root ** 3 == a\n","\n","================================================================================\n","\n"," TASK: HumanEval/78\n","--------------------------------------------------------------------------------\n","    prime_digits = {'2', '3', '5', '7', 'B', 'D'}\n","    count = 0\n","    for digit in num:\n","        if digit in prime_digits:\n","            count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/79\n","--------------------------------------------------------------------------------\n","    return f\"db{bin(decimal)[2:]}db\"\n","\n","================================================================================\n","\n"," TASK: HumanEval/80\n","--------------------------------------------------------------------------------\n","    if len(s) < 3:\n","        return False\n","    for i in range(len(s) - 2):\n","        if s[i] == s[i+1] or s[i] == s[i+2] or s[i+1] == s[i+2]:\n","            return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/81\n","--------------------------------------------------------------------------------\n","    letter_grades = []\n","    for gpa in grades:\n","        if gpa == 4.0:\n","            letter_grades.append('A+')\n","        elif gpa > 3.7:\n","            letter_grades.append('A')\n","        elif gpa > 3.3:\n","            letter_grades.append('A-')\n","        elif gpa > 3.0:\n","            letter_grades.append('B+')\n","        elif gpa > 2.7:\n","            letter_grades.append('B')\n","        elif gpa > 2.3:\n","            letter_grades.append('B-')\n","        elif gpa > 2.0:\n","            letter_grades.append('C+')\n","        elif gpa > 1.7:\n","            letter_grades.append('C')\n","        elif gpa > 1.3:\n","            letter_grades.append('C-')\n","        elif gpa > 1.0:\n","            letter_grades.append('D+')\n","        elif gpa > 0.7:\n","            letter_grades.append('D')\n","        elif gpa > 0.0:\n","            letter_grades.append('D-')\n","        else:\n","            letter_grades.append('E')\n","    return letter_grades\n","\n","================================================================================\n","\n"," TASK: HumanEval/82\n","--------------------------------------------------------------------------------\n","    length = len(string)\n","    if length < 2:\n","        return False\n","    for i in range(2, int(length ** 0.5) + 1):\n","        if length % i == 0:\n","            return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/83\n","--------------------------------------------------------------------------------\n","    if n == 1:\n","        return 1\n","    return 10**(n-1) + 9*10**(n-2) - 10**(n-2)\n","\n","================================================================================\n","\n"," TASK: HumanEval/84\n","--------------------------------------------------------------------------------\n","    return bin(sum(int(digit) for digit in str(N)))[2:]\n","\n","================================================================================\n","\n"," TASK: HumanEval/85\n","--------------------------------------------------------------------------------\n","    sum = 0\n","    for i in range(len(lst)):\n","        if i % 2 == 1 and lst[i] % 2 == 0:\n","            sum += lst[i]\n","    return sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/86\n","--------------------------------------------------------------------------------\n","    words = s.split(' ')\n","    sorted_words = [''.join(sorted(word)) for word in words]\n","    return ' '.join(sorted_words)\n","\n","================================================================================\n","\n"," TASK: HumanEval/87\n","--------------------------------------------------------------------------------\n","    coordinates = []\n","    for i, row in enumerate(lst):\n","        for j, value in enumerate(row):\n","            if value == x:\n","                coordinates.append((i, j))\n","    coordinates.sort(key=lambda x: (x[0], -x[1]))\n","    return coordinates\n","\n","================================================================================\n","\n"," TASK: HumanEval/88\n","--------------------------------------------------------------------------------\n","    if not array or len(array) == 1:\n","        return array\n","    first_last_sum = array[0] + array[-1]\n","    if first_last_sum % 2 == 1:\n","        return sorted(array)\n","    else:\n","        return sorted(array, reverse=True)\n","\n","================================================================================\n","\n"," TASK: HumanEval/89\n","--------------------------------------------------------------------------------\n","    result = []\n","    for char in s:\n","        if char.isalpha():\n","            shift = 2\n","            if char.islower():\n","                base = ord('a')\n","            else:\n","                base = ord('A')\n","            shifted_char = chr((ord(char) - base + shift) % 26 + base)\n","            result.append(shifted_char)\n","        else:\n","            result.append(char)\n","    return ''.join(result)\n","\n","================================================================================\n","\n"," TASK: HumanEval/90\n","--------------------------------------------------------------------------------\n","    if not lst:\n","        return None\n","    lst.sort()\n","    if len(lst) < 2:\n","        return None\n","    if len(lst) == 2:\n","        return lst[1]\n","    return lst[1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/91\n","--------------------------------------------------------------------------------\n","    sentences = S.split('. ') + S.split('? ') + S.split('! ')\n","    boredom_count = 0\n","    for sentence in sentences:\n","        if sentence.startswith('I'):\n","            boredom_count += 1\n","    return boredom_count\n","\n","================================================================================\n","\n"," TASK: HumanEval/92\n","--------------------------------------------------------------------------------\n","    if isinstance(x, int) and isinstance(y, int) and isinstance(z, int):\n","        if x == y + z or y == x + z or z == x + y:\n","            return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/93\n","--------------------------------------------------------------------------------\n","    vowels = 'aeiou'\n","    encoded_message = ''\n","    for char in message:\n","        if char.isalpha():\n","            if char.lower() in vowels:\n","                char = chr((ord(char.lower()) - 97 + 2) % 26 + 97).upper() if char.islower() else chr((ord(char.lower()) - 97 + 2) % 26 + 97)\n","            else:\n","                char = char.swapcase()\n","        encoded_message += char\n","    return encoded_message\n","\n","================================================================================\n","\n"," TASK: HumanEval/94\n","--------------------------------------------------------------------------------\n","    def is_prime(n):\n","        if n < 2:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","    def sum_of_digits(n):\n","        return sum(int(digit) for digit in str(n))\n","\n","        largest_prime = max(filter(is_prime, lst))\n","        return sum_of_digits(largest_prime)\n","\n","================================================================================\n","\n"," TASK: HumanEval/95\n","--------------------------------------------------------------------------------\n","    if not dict:\n","        return False\n","    all_lower = True\n","    all_upper = True\n","    for key in dict.keys():\n","        if not isinstance(key, str):\n","            all_lower = False\n","            all_upper = False\n","            break\n","        if key.islower():\n","            all_lower = True\n","        else:\n","            all_upper = True\n","    return all_lower or all_upper\n","\n","================================================================================\n","\n"," TASK: HumanEval/96\n","--------------------------------------------------------------------------------\n","    if n <= 1:\n","        return []\n","\n","    is_prime = [True] * n\n","    is_prime[0] = is_prime[1] = False\n","\n","    for p in range(2, int(n**0.5) + 1):\n","        if is_prime[p]:\n","            for i in range(p*p, n, p):\n","                is_prime[i] = False\n","\n","    return [p for p in range(n) if is_prime[p]]\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/97\n","--------------------------------------------------------------------------------\n","    return (a % 10) * (b % 10)\n","\n","================================================================================\n","\n"," TASK: HumanEval/98\n","--------------------------------------------------------------------------------\n","    count = 0\n","    for i in range(0, len(s), 2):\n","        if s[i] in 'AEIOU':\n","            count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/99\n","--------------------------------------------------------------------------------\n","    num = float(value)\n","    if num - int(num) == 0.5:\n","        return int(num) + 1 if num > 0 else int(num) - 1\n","    else:\n","        return round(num)\n","\n","================================================================================\n","\n"," TASK: HumanEval/100\n","--------------------------------------------------------------------------------\n","    pile = [n]\n","    for i in range(1, n):\n","        if n % 2 == 0:\n","            next_stones = n + 2\n","        else:\n","            next_stones = n + 2\n","        pile.append(next_stones)\n","        n = next_stones\n","    return pile\n","\n","================================================================================\n","\n"," TASK: HumanEval/101\n","--------------------------------------------------------------------------------\n","    return s.split(', ')\n","\n","================================================================================\n","\n"," TASK: HumanEval/102\n","--------------------------------------------------------------------------------\n","    if x % 2 == 0 and x in range(x, y + 1):\n","        return x\n","    elif x % 2 == 1 and x + 1 in range(x, y + 1):\n","        return x + 1\n","    elif x % 2 == 1 and x + 2 in range(x, y + 1):\n","        return x + 2\n","    elif x % 2 == 1 and x + 3 in range(x, y + 1):\n","        return x + 3\n","    else:\n","        return -1\n","\n","================================================================================\n","\n"," TASK: HumanEval/103\n","--------------------------------------------------------------------------------\n","    if n > m:\n","        return -1\n","    avg = (n + m) / 2\n","    rounded_avg = round(avg)\n","    return bin(rounded_avg)[2:]\n","\n","================================================================================\n","\n"," TASK: HumanEval/104\n","--------------------------------------------------------------------------------\n","    result = []\n","    for num in x:\n","        if all(int(digit) % 2 != 0 for digit in str(num)):\n","            result.append(num)\n","    return sorted(result)\n","\n","================================================================================\n","\n"," TASK: HumanEval/105\n","--------------------------------------------------------------------------------\n","    # Filter the array to include only integers between 1 and 9 inclusive\n","    filtered_arr = [x for x in arr if 1 <= x <= 9]\n","\n","    # Sort the filtered array\n","    sorted_arr = sorted(filtered_arr)\n","\n","    # Reverse the sorted array\n","    reversed_arr = sorted_arr[::-1]\n","\n","    # Map each digit to its corresponding name\n","    digit_names = {\n","        1: \"One\",\n","        2: \"Two\",\n","        3: \"Three\",\n","        4: \"Four\",\n","        5: \"Five\",\n","        6: \"Six\",\n","        7: \"Seven\",\n","        8: \"Eight\",\n","        9: \"Nine\"\n","    }\n","\n","    # Return the resulting array\n","    return [digit_names[x] for x in reversed_arr]\n","\n","================================================================================\n","\n"," TASK: HumanEval/106\n","--------------------------------------------------------------------------------\n","    def factorial(i):\n","        if i == 0 or i == 1:\n","            return 1\n","        return i * factorial(i - 1)\n","\n","    def sum_to_i(i):\n","        return i * (i + 1) // 2\n","\n","    result = []\n","    for i in range(1, n + 1):\n","        if i % 2 == 0:\n","            result.append(factorial(i))\n","        else:\n","            result.append(sum_to_i(i))\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/107\n","--------------------------------------------------------------------------------\n","    def is_palindrome(x):\n","        return str(x) == str(x)[::-1]\n","\n","    even_count = 0\n","    odd_count = 0\n","\n","    for i in range(1, n + 1):\n","        if is_palindrome(i):\n","            if i % 2 == 0:\n","                even_count += 1\n","            else:\n","                odd_count += 1\n","\n","    return (even_count, odd_count)\n","\n","================================================================================\n","\n"," TASK: HumanEval/108\n","--------------------------------------------------------------------------------\n","    def sum_of_digits(n):\n","        return sum(int(digit) for digit in str(abs(n)))\n","\n","    count = 0\n","    for num in arr:\n","        if sum_of_digits(num) > 0:\n","            count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/109\n","--------------------------------------------------------------------------------\n","    if not arr:\n","        return True\n","    sorted_arr = sorted(arr)\n","    n = len(arr)\n","    for i in range(n):\n","        if arr == sorted_arr[i:] + sorted_arr[:i]:\n","            return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/110\n","--------------------------------------------------------------------------------\n","    odd_count_lst1 = sum(1 for x in lst1 if x % 2 != 0)\n","    even_count_lst2 = sum(1 for x in lst2 if x % 2 == 0)\n","\n","    if odd_count_lst1 <= even_count_lst2:\n","        return \"YES\"\n","    else:\n","        return \"NO\"\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/111\n","--------------------------------------------------------------------------------\n","    # Split the input string into individual characters\n","    chars = test.split()\n","\n","    # Count the occurrences of each character\n","    char_counts = {char: chars.count(char) for char in set(chars)}\n","\n","    # Find the maximum count\n","    max_count = max(char_counts.values())\n","\n","    # Filter characters that have the maximum count\n","    result = {char: count for char, count in char_counts.items() if count == max_count}\n","\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/112\n","--------------------------------------------------------------------------------\n","    result = ''.join([char for char in s if char not in c])\n","    return (result, result == result[::-1])\n","\n","================================================================================\n","\n"," TASK: HumanEval/113\n","--------------------------------------------------------------------------------\n","    result = []\n","    for s in lst:\n","        odd_count = sum(1 for char in s if int(char) % 2 != 0)\n","        result.append(f\"the number of odd elements {odd_count}n the str{odd_count}ng {odd_count} of the {odd_count}nput.\")\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/114\n","--------------------------------------------------------------------------------\n","    current_sum = min_sum = nums[0]\n","    for num in nums[1:]:\n","        current_sum = min(num, current_sum + num)\n","        min_sum = min(min_sum, current_sum)\n","    return min_sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/115\n","--------------------------------------------------------------------------------\n","    total_water = sum(sum(row) for row in grid)\n","    if total_water <= 0:\n","        return 0\n","    return math.ceil(total_water / capacity)\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/116\n","--------------------------------------------------------------------------------\n","    sorted_arr = []\n","    for num in arr:\n","        count = bin(num).count('1')\n","        sorted_arr.append((num, count))\n","    sorted_arr.sort(key=lambda x: (x[1], x[0]))\n","    return [x[0] for x in sorted_arr]\n","\n","================================================================================\n","\n"," TASK: HumanEval/117\n","--------------------------------------------------------------------------------\n","    vowels = \"aeiou\"\n","    result = []\n","    for word in s.split():\n","        consonant_count = sum(1 for char in word if char.isalpha() and char.lower() not in vowels)\n","        if consonant_count == n:\n","            result.append(word)\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/118\n","--------------------------------------------------------------------------------\n","    vowels = \"aeiouAEIOU\"\n","    for i in range(len(word) - 2, 0, -1):\n","        if word[i] in vowels and word[i-1] not in vowels and word[i-2] not in vowels:\n","            return word[i]\n","    return \"\"\n","\n","================================================================================\n","\n"," TASK: HumanEval/119\n","--------------------------------------------------------------------------------\n","    if lst[0].count('(') > lst[1].count('('):\n","        return 'No'\n","    if lst[1].count(')') > lst[0].count(')'):\n","        return 'No'\n","    return 'Yes'\n","\n","================================================================================\n","\n"," TASK: HumanEval/120\n","--------------------------------------------------------------------------------\n","    import heapq\n","\n","    return sorted(heapq.nlargest(k, arr))\n","\n","================================================================================\n","\n"," TASK: HumanEval/121\n","--------------------------------------------------------------------------------\n","    sum_odd = 0\n","    for index, value in enumerate(lst):\n","        if index % 2 == 0 and value % 2 != 0:\n","            sum_odd += value\n","    return sum_odd\n","\n","================================================================================\n","\n"," TASK: HumanEval/122\n","--------------------------------------------------------------------------------\n","    sum = 0\n","    for i in range(k):\n","        if arr[i] < 100:\n","            sum += arr[i]\n","    return sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/123\n","--------------------------------------------------------------------------------\n","    collatz_sequence = [n]\n","    while n != 1:\n","        if n % 2 == 0:\n","            n = n // 2\n","        else:\n","            n = 3 * n + 1\n","        collatz_sequence.append(n)\n","    odd_numbers = [num for num in collatz_sequence if num % 2 != 0]\n","    return sorted(odd_numbers)\n","\n","================================================================================\n","\n"," TASK: HumanEval/124\n","--------------------------------------------------------------------------------\n","    if not date:\n","        return False\n","    if len(date) != 10 or date[2] != '-' or date[5] != '-':\n","        return False\n","    month = int(date[:2])\n","    day = int(date[3:5])\n","    year = int(date[6:])\n","    if month < 1 or month > 12:\n","        return False\n","    if month in [1, 3, 5, 7, 8, 10, 12]:\n","        if day < 1 or day > 31:\n","            return False\n","    elif month in [4, 6, 9, 11]:\n","        if day < 1 or day > 30:\n","            return False\n","    elif month == 2:\n","        if day < 1 or day > 29:\n","            return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/125\n","--------------------------------------------------------------------------------\n","    if ' ' in txt:\n","        return txt.split(' ')\n","    elif ',' in txt:\n","        return txt.split(',')\n","    else:\n","        return sum(1 for char in txt if char.islower() and (ord(char) - ord('a')) % 2 == 1)\n","\n","================================================================================\n","\n"," TASK: HumanEval/126\n","--------------------------------------------------------------------------------\n","    is_sorted = True\n","    for i in range(1, len(lst)):\n","        if lst[i] < lst[i-1]:\n","            is_sorted = False\n","            break\n","    if lst.count(lst[0]) > 1:\n","        is_sorted = False\n","    return is_sorted\n","\n","================================================================================\n","\n"," TASK: HumanEval/127\n","--------------------------------------------------------------------------------\n","    def is_prime(n):\n","        if n < 2:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","        start1, end1 = interval1\n","        start2, end2 = interval2\n","        start = max(start1, start2)\n","        end = min(end1, end2)\n","        if start > end:\n","            return \"NO\"\n","        length = end - start + 1\n","        return \"YES\" if is_prime(length) else \"NO\"\n","\n","================================================================================\n","\n"," TASK: HumanEval/128\n","--------------------------------------------------------------------------------\n","    if not arr:\n","        return None\n","    sign_product = 1\n","    magnitude_sum = 0\n","    for num in arr:\n","        magnitude_sum += abs(num)\n","        sign_product *= 1 if num > 0 else -1 if num < 0 else 0\n","    return sign_product * magnitude_sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/129\n","--------------------------------------------------------------------------------\n","    n = len(grid)\n","    dp = [[[float('inf')] * 4 for _ in range(n)] for _ in range(n)]\n","    for i in range(n):\n","        for j in range(n):\n","            dp[i][j][0] = grid[i][j]\n","    for _ in range(k - 1):\n","        new_dp = [[[float('inf')] * 4 for _ in range(n)] for _ in range(n)]\n","        for i in range(n):\n","            for j in range(n):\n","                for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n","                    x, y = i + dx, j + dy\n","                    if 0 <= x < n and 0 <= y < n:\n","                        for d in range(4):\n","                            new_dp[x][y][d] = min(new_dp[x][y][d], dp[i][j][d] + grid[x][y])\n","    min_val = float('inf')\n","    min_path = []\n","    for i in range(n):\n","        for j in range(n):\n","            for d in range(4):\n","                if dp[i][j][d] < min_val:\n","                    min_val = dp[i][j][d]\n","                    min_path = [grid[i][j]]\n","                    x, y = i, j\n","                    for _ in range(k - 1):\n","                        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n","                            nx, ny = x + dx, y + dy\n","                            if 0 <= nx < n and 0 <= ny < n and dp[nx][ny][d] == dp[x][y][d] + grid[nx][ny]:\n","                                x, y = nx, ny\n","                                min_path.append(grid[x][y])\n","    return min_path\n","\n","================================================================================\n","\n"," TASK: HumanEval/130\n","--------------------------------------------------------------------------------\n","    tribonacci = [1, 3, 2]\n","    for i in range(3, n + 1):\n","        if i % 2 == 0:\n","            tribonacci.append(1 + i / 2)\n","        else:\n","            tribonacci.append(tribonacci[i - 1] + tribonacci[i - 2] + tribonacci[i + 1])\n","    return tribonacci\n","\n","================================================================================\n","\n"," TASK: HumanEval/131\n","--------------------------------------------------------------------------------\n","    product = 1\n","    for digit in str(n):\n","        if int(digit) % 2 != 0:\n","            product *= int(digit)\n","    return product if product != 1 else 0\n","\n","================================================================================\n","\n"," TASK: HumanEval/132\n","--------------------------------------------------------------------------------\n","    depth = 0\n","    for char in string:\n","        if char == '[':\n","            depth += 1\n","        elif char == ']':\n","            depth -= 1\n","        if depth > 1:\n","            return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/133\n","--------------------------------------------------------------------------------\n","    total = 0\n","    for num in lst:\n","        total += round(num) ** 2\n","    return total\n","\n","================================================================================\n","\n"," TASK: HumanEval/134\n","--------------------------------------------------------------------------------\n","    if not txt:\n","        return False\n","    last_char = txt[-1]\n","    if last_char.isalpha() and (len(txt) == 1 or not txt[-2].isalpha()):\n","        return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/135\n","--------------------------------------------------------------------------------\n","    for i in range(1, len(arr)):\n","        if arr[i] >= arr[i - 1]:\n","            return i\n","    return -1\n","\n","================================================================================\n","\n"," TASK: HumanEval/136\n","--------------------------------------------------------------------------------\n","    largest_negative = None\n","    smallest_positive = None\n","    for num in lst:\n","        if num < 0:\n","            if largest_negative is None or num > largest_negative:\n","                largest_negative = num\n","        elif num > 0:\n","            if smallest_positive is None or num < smallest_positive:\n","                smallest_positive = num\n","    return (largest_negative, smallest_positive)\n","\n","================================================================================\n","\n"," TASK: HumanEval/137\n","--------------------------------------------------------------------------------\n","    try:\n","        a_float = float(a)\n","        b_float = float(b)\n","        if a_float > b_float:\n","            return a\n","        elif b_float > a_float:\n","            return b\n","        else:\n","            return None\n","    except ValueError:\n","        a_str = str(a)\n","        b_str = str(b)\n","        if a_str > b_str:\n","            return a\n","        elif b_str > a_str:\n","            return b\n","        else:\n","            return None\n","\n","================================================================================\n","\n"," TASK: HumanEval/138\n","--------------------------------------------------------------------------------\n","    if n % 2 != 0:\n","        return False\n","    if n < 8:\n","        return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/139\n","--------------------------------------------------------------------------------\n","    def factorial(x):\n","        if x == 0 or x == 1:\n","            return 1\n","        else:\n","            return x * factorial(x - 1)\n","\n","        result = 1\n","        for i in range(1, n + 1):\n","            result *= factorial(i)\n","        return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/140\n","--------------------------------------------------------------------------------\n","    result = \"\"\n","    previous_char_was_space = False\n","    for char in text:\n","        if char == \" \":\n","            if previous_char_was_space:\n","                result += \"_\"\n","            else:\n","                result += \" \"\n","            previous_char_was_space = True\n","        else:\n","            result += char\n","            previous_char_was_space = False\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/141\n","--------------------------------------------------------------------------------\n","    # Check if the file name contains exactly one dot '.'\n","    if file_name.count('.') != 1:\n","        return 'No'\n","\n","    # Split the file name into prefix and suffix\n","    prefix, suffix = file_name.split('.')\n","\n","    # Check if the prefix starts with a latin alphapet letter\n","    if not prefix[0].isalpha():\n","        return 'No'\n","\n","    # Check if the suffix is one of ['txt', 'exe', 'dll']\n","    if suffix not in ['txt', 'exe', 'dll']:\n","        return 'No'\n","\n","    # Count the number of digits in the prefix\n","    digit_count = sum(c.isdigit() for c in prefix)\n","\n","    # Check if there are more than three digits in the prefix\n","    if digit_count > 3:\n","        return 'No'\n","\n","    # Return 'Yes' if all conditions are met\n","    return 'Yes'\n","\n","================================================================================\n","\n"," TASK: HumanEval/142\n","--------------------------------------------------------------------------------\n","    total = 0\n","    for i, num in enumerate(lst):\n","        if i % 3 == 0:\n","            total += num ** 2\n","        elif i % 4 == 0:\n","            total += num ** 3\n","        else:\n","            total += num\n","    return total\n","\n","================================================================================\n","\n"," TASK: HumanEval/143\n","--------------------------------------------------------------------------------\n","    def is_prime(n):\n","        if n < 2:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","        words = sentence.split()\n","        prime_length_words = [word for word in words if is_prime(len(word))]\n","        return ' '.join(prime_length_words)\n","\n","================================================================================\n","\n"," TASK: HumanEval/144\n","--------------------------------------------------------------------------------\n","    x_num, x_den = map(int, x.split('/'))\n","    n_num, n_den = map(int, n.split('/'))\n","    return (x_num * n_num) % (x_den * n_den) == 0\n","\n","================================================================================\n","\n"," TASK: HumanEval/145\n","--------------------------------------------------------------------------------\n","    def sum_of_digits(n):\n","        return sum(int(digit) for digit in str(abs(n)))\n","\n","    return sorted(nums, key=lambda x: (sum_of_digits(x), nums.index(x)))\n","\n","================================================================================\n","\n"," TASK: HumanEval/146\n","--------------------------------------------------------------------------------\n","    count = 0\n","    for num in nums:\n","        if num > 10:\n","            first_digit = int(str(num)[0])\n","            last_digit = int(str(num)[-1])\n","            if first_digit % 2 != 0 and last_digit % 2 != 0:\n","                count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/147\n","--------------------------------------------------------------------------------\n","    a = [i * i - i + 1 for i in range(1, n + 1)]\n","    count = 0\n","    for i in range(n):\n","        for j in range(i + 1, n):\n","            for k in range(j + 1, n):\n","                if (a[i] + a[j] + a[k]) % 3 == 0:\n","                    count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/148\n","--------------------------------------------------------------------------------\n","    planets = [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"]\n","    if planet1 not in planets or planet2 not in planets:\n","        return ()\n","    index1 = planets.index(planet1)\n","    index2 = planets.index(planet2)\n","    if index1 > index2:\n","        index1, index2 = index2, index1\n","    return tuple(planets[index1 + 1:index2])\n","\n","================================================================================\n","\n"," TASK: HumanEval/149\n","--------------------------------------------------------------------------------\n","    # Filter out strings with odd lengths\n","    even_length_strings = [s for s in lst if len(s) % 2 == 0]\n","\n","    # Sort the list first by length, then alphabetically\n","    even_length_strings.sort(key=lambda x: (len(x), x))\n","\n","    return even_length_strings\n","\n","================================================================================\n","\n"," TASK: HumanEval/150\n","--------------------------------------------------------------------------------\n","    def is_prime(n):\n","        if n <= 1:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","        return x if is_prime(n) else y\n","\n","================================================================================\n","\n"," TASK: HumanEval/151\n","--------------------------------------------------------------------------------\n","    sum = 0\n","    for num in lst:\n","        if isinstance(num, int) and num >= 0:\n","            if num % 2 != 0:\n","                sum += num ** 2\n","    return sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/152\n","--------------------------------------------------------------------------------\n","    differences = []\n","    for score, g in zip(game, guess):\n","        differences.append(abs(score - g))\n","    return differences\n","\n","================================================================================\n","\n"," TASK: HumanEval/153\n","--------------------------------------------------------------------------------\n","    max_strength = float('-inf')\n","    strongest_extension = ''\n","    for extension in extensions:\n","        cap = sum(1 for char in extension if char.isupper())\n","        sm = sum(1 for char in extension if char.islower())\n","        strength = cap - sm\n","        if strength > max_strength:\n","            max_strength = strength\n","            strongest_extension = extension\n","    return f\"{class_name}.{strongest_extension}\"\n","\n","================================================================================\n","\n"," TASK: HumanEval/154\n","--------------------------------------------------------------------------------\n","    # Concatenate the first word with itself\n","    concatenated_a = a + a\n","    # Check if the second word is a substring of the concatenated string\n","    return b in concatenated_a\n","\n","================================================================================\n","\n"," TASK: HumanEval/155\n","--------------------------------------------------------------------------------\n","    num_str = str(abs(num))\n","    even_count = 0\n","    odd_count = 0\n","    for char in num_str:\n","        if char.isdigit():\n","            digit = int(char)\n","            if digit % 2 == 0:\n","                even_count += 1\n","            else:\n","                odd_count += 1\n","    return (even_count, odd_count)\n","\n","================================================================================\n","\n"," TASK: HumanEval/156\n","--------------------------------------------------------------------------------\n","    roman_numerals = {\n","        1000: 'm',\n","        900: 'cm',\n","        500: 'd',\n","        400: 'cd',\n","        100: 'c',\n","        90: 'xc',\n","        50: 'l',\n","        40: 'xl',\n","        10: 'x',\n","        9: 'ix',\n","        5: 'v',\n","        4: 'iv',\n","        1: 'i'\n","    }\n","    result = ''\n","    for value, symbol in sorted(roman_numerals.items(), reverse=True):\n","        while number >= value:\n","            result += symbol\n","            number -= value\n","    return result.lower()\n","\n","================================================================================\n","\n"," TASK: HumanEval/157\n","--------------------------------------------------------------------------------\n","    sides = [a, b, c]\n","    sides.sort()\n","    return sides[0]**2 + sides[1]**2 == sides[2]**2\n","\n","================================================================================\n","\n"," TASK: HumanEval/158\n","--------------------------------------------------------------------------------\n","    max_unique = 0\n","    max_word = \"\"\n","    for word in words:\n","        unique_chars = len(set(word))\n","        if unique_chars > max_unique:\n","            max_unique = unique_chars\n","            max_word = word\n","        elif unique_chars == max_unique:\n","            if word < max_word:\n","                max_word = word\n","    return max_word\n","\n","================================================================================\n","\n"," TASK: HumanEval/159\n","--------------------------------------------------------------------------------\n","    total_eaten = number\n","    remaining_carrots = remaining\n","    if remaining_carrots < need:\n","        total_eaten += remaining_carrots\n","        remaining_carrots = 0\n","    else:\n","        total_eaten += need\n","        remaining_carrots -= need\n","    return [total_eaten, remaining_carrots]\n","\n","================================================================================\n","\n"," TASK: HumanEval/160\n","--------------------------------------------------------------------------------\n","    result = operand[0]\n","    for op, num in zip(operator, operand[1:]):\n","        if op == '+':\n","            result += num\n","        elif op == '-':\n","            result -= num\n","        elif op == '*':\n","            result *= num\n","        elif op == '//':\n","            result //= num\n","        elif op == '**':\n","            result **= num\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/161\n","--------------------------------------------------------------------------------\n","    if not any(c.isalpha() for c in s):\n","        return s[::-1]\n","    return ''.join(c.lower() if c.isupper() else c.upper() for c in s)\n","\n","================================================================================\n","\n"," TASK: HumanEval/162\n","--------------------------------------------------------------------------------\n","    import hashlib\n","\n","    if not text:\n","        return None\n","    return hashlib.md5(text.encode()).hexdigest()\n","\n","================================================================================\n","\n"," TASK: HumanEval/163\n","--------------------------------------------------------------------------------\n","    result = []\n","    for num in range(a, b + 1):\n","        if num % 2 == 0:\n","            result.append(num)\n","    return result\n","\n","================================================================================\n","\n"]}],"source":["# REPLACE THIS with the path to the file you want to inspect\n","# Example: \"generation_results/grpo_cot_2048.jsonl\"\n","FILE_PATH = str(GEN_DIR / \"grpo_cot.jsonl\")\n","\n","print(f\" INSPECTING: {FILE_PATH}\\n\")\n","\n","with open(FILE_PATH, 'r') as f:\n","    for line in f:\n","        data = json.loads(line)\n","        task_id = data.get(\"task_id\", \"Unknown Task\")\n","        completion = data.get(\"completion\", \"\")\n","\n","        print(f\" TASK: {task_id}\")\n","        print(\"-\" * 80)\n","        # Printing completion directly renders newlines correctly\n","        print(completion)\n","        print(\"=\" * 80 + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"jAkf4YxDndTV"},"source":["# Step 9: Calculating Pass@1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7CMTdIhnmzW"},"outputs":[],"source":["def run_humaneval(\n","    samples_file: str,\n","    k: int = 1,\n","    timeout: float = 3.0,\n","):\n","    \"\"\"\n","    Runs HumanEval pass@k on a JSONL file of completions.\n","    \"\"\"\n","    results = evaluate_functional_correctness(\n","        sample_file=samples_file,\n","        k=[k],\n","        timeout=timeout,\n","    )\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtrrDF79nrgb"},"outputs":[],"source":["RESULTS = []\n","\n","EVAL_RUNS = [\n","    (\"Base\",     \"Non-CoT\", GEN_DIR / \"base_non_cot.jsonl\"),\n","    (\"SFT\",      \"Non-CoT\", GEN_DIR / \"sft_non_cot.jsonl\"),\n","    (\"SFT\",      \"CoT\",     GEN_DIR / \"sft_cot.jsonl\"),\n","    (\"GRPO\",     \"Non-CoT\", GEN_DIR / \"grpo_non_cot.jsonl\"),\n","    (\"GRPO\",     \"CoT\",     GEN_DIR / \"grpo_cot.jsonl\"),\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":860},"executionInfo":{"elapsed":40442,"status":"ok","timestamp":1767409475220,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"6JZYqMdcn-Aw","outputId":"36813edd-719b-492f-d76a-e9942635a0fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Evaluating: Base | Non-CoT\n","Reading samples...\n"]},{"name":"stderr","output_type":"stream","text":["164it [00:00, 1664.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running test suites...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:08<00:00, 20.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Writing results to data/evaluation/generations/base_non_cot.jsonl_results.jsonl...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 4093.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"," Evaluating: SFT | Non-CoT\n","Reading samples...\n"]},{"name":"stderr","output_type":"stream","text":["164it [00:00, 2490.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running test suites...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:07<00:00, 20.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Writing results to data/evaluation/generations/sft_non_cot.jsonl_results.jsonl...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 11187.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"," Evaluating: SFT | CoT\n","Reading samples...\n"]},{"name":"stderr","output_type":"stream","text":["164it [00:00, 1838.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running test suites...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:07<00:00, 20.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Writing results to data/evaluation/generations/sft_cot.jsonl_results.jsonl...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 10993.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"," Evaluating: GRPO | Non-CoT\n","Reading samples...\n"]},{"name":"stderr","output_type":"stream","text":["164it [00:00, 1858.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running test suites...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:07<00:00, 20.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Writing results to data/evaluation/generations/grpo_non_cot.jsonl_results.jsonl...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 8911.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"," Evaluating: GRPO | CoT\n","Reading samples...\n"]},{"name":"stderr","output_type":"stream","text":["164it [00:00, 1643.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running test suites...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:07<00:00, 21.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Writing results to data/evaluation/generations/grpo_cot.jsonl_results.jsonl...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 11602.50it/s]\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df_results\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Base\",\n          \"SFT\",\n          \"GRPO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mode\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"CoT\",\n          \"Non-CoT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pass@1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11152037291568219,\n        \"min\": 0.5121951219512195,\n        \"max\": 0.725609756097561,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.524390243902439,\n          0.725609756097561\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df_results"},"text/html":["\n","  <div id=\"df-8ecc93bd-8f37-4244-a5a2-ec36fe9d6188\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Mode</th>\n","      <th>pass@1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Base</td>\n","      <td>Non-CoT</td>\n","      <td>0.512195</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>SFT</td>\n","      <td>Non-CoT</td>\n","      <td>0.524390</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>SFT</td>\n","      <td>CoT</td>\n","      <td>0.713415</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GRPO</td>\n","      <td>Non-CoT</td>\n","      <td>0.512195</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GRPO</td>\n","      <td>CoT</td>\n","      <td>0.725610</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ecc93bd-8f37-4244-a5a2-ec36fe9d6188')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8ecc93bd-8f37-4244-a5a2-ec36fe9d6188 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8ecc93bd-8f37-4244-a5a2-ec36fe9d6188');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-b967b19d-8866-4e26-b4e5-2df38aad90d6\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b967b19d-8866-4e26-b4e5-2df38aad90d6')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-b967b19d-8866-4e26-b4e5-2df38aad90d6 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_282f0703-e65a-429b-b329-873b129d587f\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_282f0703-e65a-429b-b329-873b129d587f button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_results');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["  Model     Mode    pass@1\n","0  Base  Non-CoT  0.512195\n","1   SFT  Non-CoT  0.524390\n","2   SFT      CoT  0.713415\n","3  GRPO  Non-CoT  0.512195\n","4  GRPO      CoT  0.725610"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["for model_name, mode, path in EVAL_RUNS:\n","    print(f\"\\n Evaluating: {model_name} | {mode}\")\n","    res = run_humaneval(str(path), k=1)\n","    RESULTS.append({\n","        \"Model\": model_name,\n","        \"Mode\": mode,\n","        \"pass@1\": res[\"pass@1\"],\n","    })\n","\n","df_results = pd.DataFrame(RESULTS)\n","df_results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"elapsed":360,"status":"ok","timestamp":1767409941051,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"j3-7lFiRxIGj","outputId":"6acc035d-3f58-491d-c252-5e4731bf8623"},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 800x400 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["df_results = df_results.copy()\n","df_results[\"Label_ml\"] = df_results[\"Model\"] + \" (\" + df_results[\"Mode\"] + \")\"\n","\n","plt.figure(figsize=(8, 4))\n","plt.bar(df_results[\"Label_ml\"], df_results[\"pass@1\"])\n","plt.ylabel(\"pass@1\")\n","plt.title(\"HumanEval pass@1 by Model and Mode\")\n","plt.xticks(rotation=30, ha=\"right\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1767409955789,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"},"user_tz":360},"id":"6d20Mujl1hsZ","outputId":"2b9b131d-7d72-4733-c567-e2af2d07b21f"},"outputs":[{"name":"stdout","output_type":"stream","text":["SFT_CoT_minus_SFT_NonCoT: 0.1890\n","GRPO_CoT_minus_SFT_CoT: 0.0122\n","GRPO_CoT_minus_Base_NonCoT: 0.2134\n"]}],"source":["def get_score(model: str, mode: str) -> float:\n","    s = df_results.loc[\n","        (df_results[\"Model\"] == model) & (df_results[\"Mode\"] == mode),\n","        \"pass@1\"\n","    ]\n","    if len(s) != 1:\n","        raise ValueError(f\"Expected exactly 1 row for ({model}, {mode}), got {len(s)}.\")\n","    return float(s.iloc[0])\n","\n","deltas = {\n","    \"SFT_CoT_minus_SFT_NonCoT\":\n","        get_score(\"SFT\", \"CoT\") - get_score(\"SFT\", \"Non-CoT\"),\n","\n","    \"GRPO_CoT_minus_SFT_CoT\":\n","        get_score(\"GRPO\", \"CoT\") - get_score(\"SFT\", \"CoT\"),\n","\n","    \"GRPO_CoT_minus_Base_NonCoT\":\n","        get_score(\"GRPO\", \"CoT\") - get_score(\"Base\", \"Non-CoT\"),\n","}\n","\n","for k, v in deltas.items():\n","    print(f\"{k}: {v:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"2aaSM2OK4ayg"},"source":["# Step 10: Fixing Metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQ_G21MH4cnb"},"outputs":[],"source":["import nbformat\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDLgosHr4e3w"},"outputs":[],"source":["from google.colab import drive, files\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLF_hG2-4gga"},"outputs":[],"source":["# List the notebook directory to confirm the file exists\n","os.listdir(\"/content/drive/MyDrive/grpo-verified-reasoner/notebooks\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpdji2u14jnS"},"outputs":[],"source":["notebook_path = \"/content/drive/MyDrive/grpo-verified-reasoner/notebooks/04_evaluation.ipynb\"\n","\n","with open(notebook_path, \"r\") as f:\n","    nb = nbformat.read(f, as_version=4)\n","\n","if \"widgets\" in nb.metadata:\n","    del nb.metadata[\"widgets\"]\n","\n","with open(notebook_path, \"w\") as f:\n","    nbformat.write(nb, f)\n","\n","print(\"Notebook fixed and saved successfully!\")"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyP6e/QqPtJcviGPVam9+e9u"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}