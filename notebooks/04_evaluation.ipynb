{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyObdNZD6wsHlXPw8QSR40n5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9e07d981d95b4b6fac218bf1a5cd4310":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c97bc9dff1e54ca682f684f2ecda5f91","IPY_MODEL_9517697050174a11a667421cf3b82dc8","IPY_MODEL_4c8666538c57445dbc451d8eb56355cb"],"layout":"IPY_MODEL_23703f7d48a249dc9c52a329e90111e4"}},"c97bc9dff1e54ca682f684f2ecda5f91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7caa2ebfd52e4cd9975fae9c894db6b2","placeholder":"​","style":"IPY_MODEL_ddb5f0a4713e415eb1eb0fa7117cacd8","value":""}},"9517697050174a11a667421cf3b82dc8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c37d9c2146af49f2a8871369274c7117","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65d55abc399c46cda630fe6ce221dd70","value":1}},"4c8666538c57445dbc451d8eb56355cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56491b9d325e4efa934f324f8f7ac069","placeholder":"​","style":"IPY_MODEL_c59249c1219f4d8aa78f5640105f3d51","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00, 16.15it/s]\n"}},"23703f7d48a249dc9c52a329e90111e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7caa2ebfd52e4cd9975fae9c894db6b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddb5f0a4713e415eb1eb0fa7117cacd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c37d9c2146af49f2a8871369274c7117":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65d55abc399c46cda630fe6ce221dd70":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"56491b9d325e4efa934f324f8f7ac069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c59249c1219f4d8aa78f5640105f3d51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86453b4351bd458d94985eb2e2369478":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e37c5f6d52164cf698863c6024f553ac","IPY_MODEL_cb96ab4fbef44ca58be90f961029698a","IPY_MODEL_fdb0c52bc2ef40d484b2efbbf10f60c0"],"layout":"IPY_MODEL_dc5e8d1c4930442cb058d226df998809"}},"e37c5f6d52164cf698863c6024f553ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed1b82d393c84942a17e5f6a05109f2b","placeholder":"​","style":"IPY_MODEL_a425bef413e9416788939c8a7017a44b","value":""}},"cb96ab4fbef44ca58be90f961029698a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bb44fd0835c407885c4c5fe8e4d72da","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd005e42b5a24f379c99fb2c8d525a0f","value":1}},"fdb0c52bc2ef40d484b2efbbf10f60c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f325c8a677e4ac9a0fda1ee5ce7dbab","placeholder":"​","style":"IPY_MODEL_128d005d68bb48c986a1844810e62455","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01&lt;00:00,  1.11s/it]\n"}},"dc5e8d1c4930442cb058d226df998809":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed1b82d393c84942a17e5f6a05109f2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a425bef413e9416788939c8a7017a44b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bb44fd0835c407885c4c5fe8e4d72da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd005e42b5a24f379c99fb2c8d525a0f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f325c8a677e4ac9a0fda1ee5ce7dbab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"128d005d68bb48c986a1844810e62455":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfc1a7a603b548eeba145344d6c6380c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31dd11d1d36b4ec481c9835c7c9bd4f7","IPY_MODEL_96bfd49bb45f44d080bc259c9a9021d0","IPY_MODEL_6cd33c9a903a448c84ce655286c75240"],"layout":"IPY_MODEL_5d92da5b6bb9425698d1f01c1e8393f3"}},"31dd11d1d36b4ec481c9835c7c9bd4f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2c6b77ca36a43d2b5cb5bcab1e07443","placeholder":"​","style":"IPY_MODEL_756f24906c634b04b369818594729397","value":"Adding requests: 100%"}},"96bfd49bb45f44d080bc259c9a9021d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88fea147f555478195e17483b49808fb","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_558bea4d97b84928941f5760ba849b9f","value":164}},"6cd33c9a903a448c84ce655286c75240":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1c5409099154a8bb10cc70a15d875a2","placeholder":"​","style":"IPY_MODEL_10879fd7064a42d28ae3b330ce166e75","value":" 164/164 [00:00&lt;00:00, 1241.76it/s]"}},"5d92da5b6bb9425698d1f01c1e8393f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2c6b77ca36a43d2b5cb5bcab1e07443":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"756f24906c634b04b369818594729397":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88fea147f555478195e17483b49808fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"558bea4d97b84928941f5760ba849b9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1c5409099154a8bb10cc70a15d875a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10879fd7064a42d28ae3b330ce166e75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"573e32f79cfc4a44ac6e9da4a133ec51":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f9691a3300545dab1057fbcd57f7dee","IPY_MODEL_d791cd625ef44a79b68a7bd4ecefdc17","IPY_MODEL_13f6bef9ac914ec881bbed09ca41654e"],"layout":"IPY_MODEL_9aa5130192da4193907133cd94e88739"}},"0f9691a3300545dab1057fbcd57f7dee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad81c3d4627f4f779fa707604a324a29","placeholder":"​","style":"IPY_MODEL_9d5d1f2424d545e4a5c5dea1950889c8","value":"Processed prompts: 100%"}},"d791cd625ef44a79b68a7bd4ecefdc17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c4c54209d7344daad6f66fed3f805a9","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39a4e4ecadc846138ebc364a8d46c577","value":164}},"13f6bef9ac914ec881bbed09ca41654e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ced5aa9982342ecaf19419929d8cf9c","placeholder":"​","style":"IPY_MODEL_291e1af1a80c41c5a9bb5da37776cb6d","value":" 164/164 [00:18&lt;00:00,  2.22it/s, est. speed input: 1179.67 toks/s, output: 749.98 toks/s]"}},"9aa5130192da4193907133cd94e88739":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"ad81c3d4627f4f779fa707604a324a29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d5d1f2424d545e4a5c5dea1950889c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c4c54209d7344daad6f66fed3f805a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39a4e4ecadc846138ebc364a8d46c577":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ced5aa9982342ecaf19419929d8cf9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"291e1af1a80c41c5a9bb5da37776cb6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52dae0c139574d6b97726e49fbc4f499":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76d06788dd484852bb1af7d7a00eedce","IPY_MODEL_906c8feadcec4feda4c2ef5c71c58ad1","IPY_MODEL_fad724606e4c42ab8ec76dbac81759bb"],"layout":"IPY_MODEL_6a0f7b13060848929f3ab0f7c97f5018"}},"76d06788dd484852bb1af7d7a00eedce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77d699cc0a55478c85801098e92c0ac7","placeholder":"​","style":"IPY_MODEL_25ba8d7ce7c84e79b500e2fe8469d20b","value":""}},"906c8feadcec4feda4c2ef5c71c58ad1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5271e0fa88b4bcf94b59c32f4fd4ad9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd45b3f39f26457294feb175c29f4f84","value":1}},"fad724606e4c42ab8ec76dbac81759bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2287da862c6047f58b0f6c2fd5406a72","placeholder":"​","style":"IPY_MODEL_81bcf1025a8f4d928072d3112baa0d5f","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00, 16.39it/s]\n"}},"6a0f7b13060848929f3ab0f7c97f5018":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77d699cc0a55478c85801098e92c0ac7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25ba8d7ce7c84e79b500e2fe8469d20b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5271e0fa88b4bcf94b59c32f4fd4ad9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd45b3f39f26457294feb175c29f4f84":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2287da862c6047f58b0f6c2fd5406a72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81bcf1025a8f4d928072d3112baa0d5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"260af0bd31c543f78ac7890b26ec34cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_217f205297894f469c75ed0299e2811c","IPY_MODEL_092f90f9b74f4cb6aa193d609c34f882","IPY_MODEL_046614e4ca634ca299bf87263fcff97d"],"layout":"IPY_MODEL_eb6fb60334a243379a8560fd0e155bbe"}},"217f205297894f469c75ed0299e2811c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8efce440d30465b8c26c1e4e26ff761","placeholder":"​","style":"IPY_MODEL_8959a054c6fd448e81f873d33531b8d4","value":""}},"092f90f9b74f4cb6aa193d609c34f882":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88f7a819ea6149bbb3552eff51a81f37","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df08567d5f99413b95aa9ee45569c191","value":1}},"046614e4ca634ca299bf87263fcff97d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a844930fb787424c8f6a778d5df54a0f","placeholder":"​","style":"IPY_MODEL_7ec9d9f2630e4cac97a6512b2faa2230","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01&lt;00:00,  1.12s/it]\n"}},"eb6fb60334a243379a8560fd0e155bbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8efce440d30465b8c26c1e4e26ff761":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8959a054c6fd448e81f873d33531b8d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88f7a819ea6149bbb3552eff51a81f37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df08567d5f99413b95aa9ee45569c191":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a844930fb787424c8f6a778d5df54a0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ec9d9f2630e4cac97a6512b2faa2230":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79a475f98248445db0139a3301493fbe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_021e3768bdf449c1b4e0f2e1c0d07aa0","IPY_MODEL_70cde0f9c934447e80f8124b961cd81d","IPY_MODEL_47ae4883970c4d339b2f21a74ccdaf8c"],"layout":"IPY_MODEL_793b9bf9231d4d529d28926e85c64302"}},"021e3768bdf449c1b4e0f2e1c0d07aa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdda8c9bc3634a6894ca37e6eb8ff0c8","placeholder":"​","style":"IPY_MODEL_707cb13f5ff14b9c961f61a419c3660d","value":"Adding requests: 100%"}},"70cde0f9c934447e80f8124b961cd81d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16e7c63350414f12b70ab8ffe3cc1664","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffef79f8dde4489ba108af5422b4f837","value":164}},"47ae4883970c4d339b2f21a74ccdaf8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c64a75b5a77c41228c0cc00225faba42","placeholder":"​","style":"IPY_MODEL_d41ddc60d37e4436b2e7414e0c5d5a65","value":" 164/164 [00:00&lt;00:00, 1213.55it/s]"}},"793b9bf9231d4d529d28926e85c64302":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdda8c9bc3634a6894ca37e6eb8ff0c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"707cb13f5ff14b9c961f61a419c3660d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16e7c63350414f12b70ab8ffe3cc1664":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffef79f8dde4489ba108af5422b4f837":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c64a75b5a77c41228c0cc00225faba42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d41ddc60d37e4436b2e7414e0c5d5a65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a047b552d5b4c179453c4afda7ba89a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8315ce042ce945549fd198d7b4cb3d78","IPY_MODEL_c883463a5b2b41b8b293437b0fb87edb","IPY_MODEL_74c83541d6474bd391da1d41d46ed6dd"],"layout":"IPY_MODEL_c5c031ff8c86479caf49f954548b1a08"}},"8315ce042ce945549fd198d7b4cb3d78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a55db7e85e5841eeac6ba943b9b5a118","placeholder":"​","style":"IPY_MODEL_c254b2673a624e7aa74391b4998f3eb0","value":"Processed prompts: 100%"}},"c883463a5b2b41b8b293437b0fb87edb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79285b34a9ad4ec48d35d6b56ce69cc6","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7205d163371b4d5b966050dbe3fb9bec","value":164}},"74c83541d6474bd391da1d41d46ed6dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f758cc49cff543afa37a4be57eb86d60","placeholder":"​","style":"IPY_MODEL_7478ec008e664cec9dcc38fd1ad6a2e2","value":" 164/164 [00:12&lt;00:00,  1.31it/s, est. speed input: 1745.80 toks/s, output: 1022.42 toks/s]"}},"c5c031ff8c86479caf49f954548b1a08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"a55db7e85e5841eeac6ba943b9b5a118":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c254b2673a624e7aa74391b4998f3eb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79285b34a9ad4ec48d35d6b56ce69cc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7205d163371b4d5b966050dbe3fb9bec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f758cc49cff543afa37a4be57eb86d60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7478ec008e664cec9dcc38fd1ad6a2e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7de28f0af56c4637a5b0cdb56d41c24a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d9e6327d2c94c3e9de690690d1d6dc1","IPY_MODEL_bc36fec764324a3e8ad803bc0c6ab6a8","IPY_MODEL_f4a2bc05061d41f4a0b364869195bf18"],"layout":"IPY_MODEL_24cf7cf9a96841a7a45e6ace272bb7f9"}},"1d9e6327d2c94c3e9de690690d1d6dc1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58a0cbda19be4dadbb5d70b7ae610591","placeholder":"​","style":"IPY_MODEL_9e101b7855654afaa261a8e50f45e43c","value":""}},"bc36fec764324a3e8ad803bc0c6ab6a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3673be0514cd4717bda5e780e3ea1869","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_401127bea538400a9e9abd3cd529119d","value":1}},"f4a2bc05061d41f4a0b364869195bf18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb74370ac3994457854b43b8c386e160","placeholder":"​","style":"IPY_MODEL_995ed386e17e4d90a6f9298ed9fc8ca3","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00, 15.46it/s]\n"}},"24cf7cf9a96841a7a45e6ace272bb7f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58a0cbda19be4dadbb5d70b7ae610591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e101b7855654afaa261a8e50f45e43c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3673be0514cd4717bda5e780e3ea1869":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"401127bea538400a9e9abd3cd529119d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb74370ac3994457854b43b8c386e160":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"995ed386e17e4d90a6f9298ed9fc8ca3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e15a72c48a2645fb90ba60a9478ab96a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31f979ec37a64274960741b6926103d4","IPY_MODEL_6e02b33090924ae786c53455e275854c","IPY_MODEL_880dd4210db74f748d60539ddcf41b16"],"layout":"IPY_MODEL_8faebbfbda944821afe9cf9e53117336"}},"31f979ec37a64274960741b6926103d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97de53a5ca994d99886f35ed36315985","placeholder":"​","style":"IPY_MODEL_7717dcdf798e430e84dafc68a4c27801","value":""}},"6e02b33090924ae786c53455e275854c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09ae04adcc774d70ba03e0d20f0b74f0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94983102be2d45e98c0ba17fbee21416","value":1}},"880dd4210db74f748d60539ddcf41b16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf75a9c603544760911027e214ffb604","placeholder":"​","style":"IPY_MODEL_b84ead8524e243f59ad6bd05a8794f79","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01&lt;00:00,  1.14s/it]\n"}},"8faebbfbda944821afe9cf9e53117336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97de53a5ca994d99886f35ed36315985":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7717dcdf798e430e84dafc68a4c27801":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09ae04adcc774d70ba03e0d20f0b74f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94983102be2d45e98c0ba17fbee21416":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf75a9c603544760911027e214ffb604":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b84ead8524e243f59ad6bd05a8794f79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b38723147a504817a4db00afa4568c48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d78dfe1df6f6466b82dfda067d163019","IPY_MODEL_e8e7aa6b20f0498db47ec6dde1e28828","IPY_MODEL_1ea3a70931b344ba8aac50cc764b9078"],"layout":"IPY_MODEL_8e67fb40859248d5b6578c80a5d95fb9"}},"d78dfe1df6f6466b82dfda067d163019":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a3d4e2138f64e0ba949d908484a932a","placeholder":"​","style":"IPY_MODEL_8fd88a2a7f7340ec93d3f6a241f5c577","value":"Adding requests: 100%"}},"e8e7aa6b20f0498db47ec6dde1e28828":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52d418e654e74fd7b80e7e26d1c8c7a7","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c012574146c345dcaa2342fe1b02dcb5","value":164}},"1ea3a70931b344ba8aac50cc764b9078":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2049941b8104e9eaf5986887fd0b222","placeholder":"​","style":"IPY_MODEL_2e92afebe0ec445cbd968b1cbc0eae8c","value":" 164/164 [00:00&lt;00:00, 887.40it/s]"}},"8e67fb40859248d5b6578c80a5d95fb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a3d4e2138f64e0ba949d908484a932a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fd88a2a7f7340ec93d3f6a241f5c577":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52d418e654e74fd7b80e7e26d1c8c7a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c012574146c345dcaa2342fe1b02dcb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2049941b8104e9eaf5986887fd0b222":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e92afebe0ec445cbd968b1cbc0eae8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c28de286b97749479e130f0ffa8ae920":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62b05eae8327439e81ed605f77178b8e","IPY_MODEL_d4950467d4cd4ba7abfeba3e458e5800","IPY_MODEL_82dee44ec54b4995b2cbd4e0f0884837"],"layout":"IPY_MODEL_19dafd90627144e48934c47daa080c5e"}},"62b05eae8327439e81ed605f77178b8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95949ab50ea644a0bd60787b55c08eb7","placeholder":"​","style":"IPY_MODEL_44856bac63de4e62b16a4f04c1e5deb9","value":"Processed prompts: 100%"}},"d4950467d4cd4ba7abfeba3e458e5800":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_002556d328684773b0029a9a816807bd","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b64d826707a040ca8e606cc619ce1d4c","value":164}},"82dee44ec54b4995b2cbd4e0f0884837":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_717a62bb0b62431299b2e969d9bbbca4","placeholder":"​","style":"IPY_MODEL_00b46237fc6e4d8eb5fb58335134c7f5","value":" 164/164 [01:10&lt;00:00,  4.18s/it, est. speed input: 483.50 toks/s, output: 462.81 toks/s]"}},"19dafd90627144e48934c47daa080c5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"95949ab50ea644a0bd60787b55c08eb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44856bac63de4e62b16a4f04c1e5deb9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"002556d328684773b0029a9a816807bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b64d826707a040ca8e606cc619ce1d4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"717a62bb0b62431299b2e969d9bbbca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00b46237fc6e4d8eb5fb58335134c7f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01636da2ef524f05bd829019181ad415":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67e58f0969b4499eba66fda09a242e1a","IPY_MODEL_eae19252908244f4b68dc8f154acd083","IPY_MODEL_05ae6026e8f346cda8a5a0a7d00e17fa"],"layout":"IPY_MODEL_0bb0fbdda5db4e43b608c8375acde029"}},"67e58f0969b4499eba66fda09a242e1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b2b0f2f12584bb9bb1d2c67598889f7","placeholder":"​","style":"IPY_MODEL_54f338234f564c2ba54c99e847d7d693","value":""}},"eae19252908244f4b68dc8f154acd083":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e122dcb142514ce98c92bebb1598c7c7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b601e6a29afb4cee8a5007721d830151","value":1}},"05ae6026e8f346cda8a5a0a7d00e17fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ea6415cddfc492da92768c335938cbf","placeholder":"​","style":"IPY_MODEL_8a95701cc0db4d6e9caa0420900d12db","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00, 16.15it/s]\n"}},"0bb0fbdda5db4e43b608c8375acde029":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b2b0f2f12584bb9bb1d2c67598889f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54f338234f564c2ba54c99e847d7d693":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e122dcb142514ce98c92bebb1598c7c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b601e6a29afb4cee8a5007721d830151":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ea6415cddfc492da92768c335938cbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a95701cc0db4d6e9caa0420900d12db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"399df7cd8a69427399578dbc3713acbf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b515915e02a84bde8915d354a3326e15","IPY_MODEL_12bfca7c2d934018b46e508c750bff7d","IPY_MODEL_662b00ad837646259828b7678beb5003"],"layout":"IPY_MODEL_c2c15b3d54a443f2bf9cc5b9ca7b271e"}},"b515915e02a84bde8915d354a3326e15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92c75be1e78045079d54dcbe6e158fdb","placeholder":"​","style":"IPY_MODEL_372366e7b54a46f8a71d4cb61e4b7785","value":""}},"12bfca7c2d934018b46e508c750bff7d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_35e2a566afea4b3497d1fa2b5813db97","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6aee1c7ac1cf4694892915a00f402d12","value":1}},"662b00ad837646259828b7678beb5003":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37e80677b07f4835bce151f80f8a573c","placeholder":"​","style":"IPY_MODEL_f749c55db6364c5d85302b1997937a3c","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01&lt;00:00,  1.12s/it]\n"}},"c2c15b3d54a443f2bf9cc5b9ca7b271e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92c75be1e78045079d54dcbe6e158fdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"372366e7b54a46f8a71d4cb61e4b7785":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35e2a566afea4b3497d1fa2b5813db97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aee1c7ac1cf4694892915a00f402d12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37e80677b07f4835bce151f80f8a573c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f749c55db6364c5d85302b1997937a3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"978e367ad00148cbbb4485225df7715d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42bd5c84229b4fbdab91471e87fcdb03","IPY_MODEL_0693a1c390574722864ea0b2b171c052","IPY_MODEL_3e0347ff60ec41deb9eb73e8c15e5d7a"],"layout":"IPY_MODEL_faf0dc4eafc24f35b9f22d0d6133980f"}},"42bd5c84229b4fbdab91471e87fcdb03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfad52bc2ebe46069f74fe208d6f165d","placeholder":"​","style":"IPY_MODEL_253ba4d2c7d447228b04602ac7c7c723","value":"Adding requests: 100%"}},"0693a1c390574722864ea0b2b171c052":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85666a43f063430f83dc5a3593254030","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5e4087d7ccae4dca8d82f7836a922f4a","value":164}},"3e0347ff60ec41deb9eb73e8c15e5d7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f608275508a4427aad84b873ab022d4d","placeholder":"​","style":"IPY_MODEL_eae556a8e0a84035843db7a56a95be5b","value":" 164/164 [00:00&lt;00:00, 1172.34it/s]"}},"faf0dc4eafc24f35b9f22d0d6133980f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfad52bc2ebe46069f74fe208d6f165d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"253ba4d2c7d447228b04602ac7c7c723":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85666a43f063430f83dc5a3593254030":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e4087d7ccae4dca8d82f7836a922f4a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f608275508a4427aad84b873ab022d4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eae556a8e0a84035843db7a56a95be5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0676ecabe6164cf6aa4844ca5030d9e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_774b577f72094f8f81fa3703229936b7","IPY_MODEL_5785a50bed224f7fa653bd8d993b7be7","IPY_MODEL_3d396ac3fd684c9c8d35986bf186d5a3"],"layout":"IPY_MODEL_2d6d2d2997c84e4897ee1a0da0c6b8d6"}},"774b577f72094f8f81fa3703229936b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f52a92a6e435447796b75c3f03ba95ad","placeholder":"​","style":"IPY_MODEL_2ad6e74d83fb42ff91a48b6eb35438dc","value":"Processed prompts: 100%"}},"5785a50bed224f7fa653bd8d993b7be7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a14414d22c4873b9a693fdaeeda3d2","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25f492a9e83946a5a040b5b9b82e2530","value":164}},"3d396ac3fd684c9c8d35986bf186d5a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6e1995166fc45b6ab6b6350da695e74","placeholder":"​","style":"IPY_MODEL_d97053da18934fdc9081aa47132cd3ca","value":" 164/164 [00:18&lt;00:00,  2.23it/s, est. speed input: 1180.22 toks/s, output: 750.33 toks/s]"}},"2d6d2d2997c84e4897ee1a0da0c6b8d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"f52a92a6e435447796b75c3f03ba95ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ad6e74d83fb42ff91a48b6eb35438dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73a14414d22c4873b9a693fdaeeda3d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25f492a9e83946a5a040b5b9b82e2530":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6e1995166fc45b6ab6b6350da695e74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d97053da18934fdc9081aa47132cd3ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a52934b8aeb45759a61e500c278e665":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a190852104c14a6b87d719ebb8d6d912","IPY_MODEL_7f98c8589b024b5bbe6d5c929b3b7981","IPY_MODEL_1ec9bd92a4484cceae354ee4e1e5d890"],"layout":"IPY_MODEL_f1adb3732fda47ec87c5511f8e23a659"}},"a190852104c14a6b87d719ebb8d6d912":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e465c758682b457a80dca75b9530ef2c","placeholder":"​","style":"IPY_MODEL_a59bd86f96234614a60d74b60bfe5649","value":"tokenizer_config.json: "}},"7f98c8589b024b5bbe6d5c929b3b7981":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a25a1efebd64039a785bf52b9176e20","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84e834cc22694b31bc5604d67ac3bd6a","value":1}},"1ec9bd92a4484cceae354ee4e1e5d890":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ebaca34db3a4af1ac14e497f36f2d40","placeholder":"​","style":"IPY_MODEL_e16056f7f5304a4599c1fb65cbcd3ba2","value":" 5.43k/? [00:00&lt;00:00, 575kB/s]"}},"f1adb3732fda47ec87c5511f8e23a659":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e465c758682b457a80dca75b9530ef2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a59bd86f96234614a60d74b60bfe5649":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a25a1efebd64039a785bf52b9176e20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"84e834cc22694b31bc5604d67ac3bd6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ebaca34db3a4af1ac14e497f36f2d40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e16056f7f5304a4599c1fb65cbcd3ba2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b808453ba704088a51e44ad8178c382":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cee52e56dfa041be92d5947dde06f368","IPY_MODEL_dfcdb300a18d4a8ca2e7ace50af48b20","IPY_MODEL_f8a082d4c99f4b2e9d8543e5804f582f"],"layout":"IPY_MODEL_bd5d748cb59540979509700c5ae30ef1"}},"cee52e56dfa041be92d5947dde06f368":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4247a676c68c4789bbbae0b5d961cf71","placeholder":"​","style":"IPY_MODEL_a7ff023db6cf459cb80a367b415d0a2c","value":"vocab.json: "}},"dfcdb300a18d4a8ca2e7ace50af48b20":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7a5a2454cf2412bb025c6c0792486a1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90691bb2fcf24790ac631d4e7c25d0f8","value":1}},"f8a082d4c99f4b2e9d8543e5804f582f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2480c3a740fd458ab468ad5668c04b11","placeholder":"​","style":"IPY_MODEL_9bdd4e57669b4cbfaa46e0bab8dc876d","value":" 2.78M/? [00:00&lt;00:00, 95.0MB/s]"}},"bd5d748cb59540979509700c5ae30ef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4247a676c68c4789bbbae0b5d961cf71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7ff023db6cf459cb80a367b415d0a2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7a5a2454cf2412bb025c6c0792486a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"90691bb2fcf24790ac631d4e7c25d0f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2480c3a740fd458ab468ad5668c04b11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bdd4e57669b4cbfaa46e0bab8dc876d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5c3561275e6474dab13cc8f021c8e01":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0cef8a3cf8da4169bdb6a05676286027","IPY_MODEL_9bb210d9db9f4fb787c57ec583a0cdd3","IPY_MODEL_da9eabda62c842b6a7b21ab3661fb9d5"],"layout":"IPY_MODEL_feb7cfa915224a47a8fae486ddb19578"}},"0cef8a3cf8da4169bdb6a05676286027":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d8e7f625a99436f8387755ac12b784b","placeholder":"​","style":"IPY_MODEL_993e54c9466647bc8ad441860cb7623a","value":"merges.txt: "}},"9bb210d9db9f4fb787c57ec583a0cdd3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_117c534d1b8742fe93d98856bfa6abd5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3862ea0c4555471685d890f321b1079a","value":1}},"da9eabda62c842b6a7b21ab3661fb9d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55d809342c2c4ef48b5fea053589e62a","placeholder":"​","style":"IPY_MODEL_110b364877ff49d98ed6e3ce9045fc44","value":" 1.67M/? [00:00&lt;00:00, 84.8MB/s]"}},"feb7cfa915224a47a8fae486ddb19578":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d8e7f625a99436f8387755ac12b784b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"993e54c9466647bc8ad441860cb7623a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"117c534d1b8742fe93d98856bfa6abd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"3862ea0c4555471685d890f321b1079a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55d809342c2c4ef48b5fea053589e62a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"110b364877ff49d98ed6e3ce9045fc44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cc41dd180934482a99716883c23867e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2081cec9668146a4aacaec480a3ac5b5","IPY_MODEL_0375f74fe69c4b59bc128504e2755c09","IPY_MODEL_e4fe97664f6e48e1bc8085ba551e62c0"],"layout":"IPY_MODEL_1e3528036327443d953d8e70d7ae08eb"}},"2081cec9668146a4aacaec480a3ac5b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9020ef6705094a1eb845855835337f3e","placeholder":"​","style":"IPY_MODEL_db71615d176740cf94f4910a3d913527","value":"tokenizer.json: 100%"}},"0375f74fe69c4b59bc128504e2755c09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f65139456d4846d39a07ea70d69beb26","max":11422654,"min":0,"orientation":"horizontal","style":"IPY_MODEL_38819b3450754189a2f5e92412e14a41","value":11422654}},"e4fe97664f6e48e1bc8085ba551e62c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d74d1891d464d2292c0d767b604c1f4","placeholder":"​","style":"IPY_MODEL_751595b0364241e38b47a53c7e231ff5","value":" 11.4M/11.4M [00:00&lt;00:00, 5.66MB/s]"}},"1e3528036327443d953d8e70d7ae08eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9020ef6705094a1eb845855835337f3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db71615d176740cf94f4910a3d913527":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f65139456d4846d39a07ea70d69beb26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38819b3450754189a2f5e92412e14a41":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0d74d1891d464d2292c0d767b604c1f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"751595b0364241e38b47a53c7e231ff5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6febfdb59c0541618e5c749c7bd2cd50":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9b174d079e94e49b6bfbffe33f56534","IPY_MODEL_92f7620bd45149e698c21a82127c816c","IPY_MODEL_f4c7661dcd2441a1b7e9fbc018b667e6"],"layout":"IPY_MODEL_65623eb5d7fb466bbbf429235302502d"}},"c9b174d079e94e49b6bfbffe33f56534":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d246786626444978b385c55076c3a7d2","placeholder":"​","style":"IPY_MODEL_40abe243d4ca49558cf9b064090f0b40","value":"added_tokens.json: 100%"}},"92f7620bd45149e698c21a82127c816c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a70348ff14134a9b9a3ec97207b437a1","max":707,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd195ac815f7440a82d521e21b995ca6","value":707}},"f4c7661dcd2441a1b7e9fbc018b667e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65cac6e4708c4056b1db0ce49fbdc2b5","placeholder":"​","style":"IPY_MODEL_6dba11abceee4fb48a97a4cb218e3857","value":" 707/707 [00:00&lt;00:00, 90.5kB/s]"}},"65623eb5d7fb466bbbf429235302502d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d246786626444978b385c55076c3a7d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40abe243d4ca49558cf9b064090f0b40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a70348ff14134a9b9a3ec97207b437a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd195ac815f7440a82d521e21b995ca6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65cac6e4708c4056b1db0ce49fbdc2b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dba11abceee4fb48a97a4cb218e3857":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"483ed27dfc6c4369bb7871eccee293fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b0f99baf5f349438efc7bb84926e4f7","IPY_MODEL_41653c2073f24328b8ca93939aaea5fd","IPY_MODEL_1de2c3c29e9e4c55a4137aadcfc1f8d6"],"layout":"IPY_MODEL_707f0ab41ca94e02b7404e07aa084640"}},"3b0f99baf5f349438efc7bb84926e4f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49c53d2edc8346db97c63010fefe60da","placeholder":"​","style":"IPY_MODEL_2b6ebe945a9e4234a3588f375bfdb260","value":"special_tokens_map.json: 100%"}},"41653c2073f24328b8ca93939aaea5fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3d088f42df44726b9bff0dd74f4f9c8","max":617,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6cd1ad25ccc04a99a1bcebfbb91ffa6a","value":617}},"1de2c3c29e9e4c55a4137aadcfc1f8d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07e4304b9ea441ecbbd30c0571444390","placeholder":"​","style":"IPY_MODEL_2820755d5c0f4606887533770945491a","value":" 617/617 [00:00&lt;00:00, 85.5kB/s]"}},"707f0ab41ca94e02b7404e07aa084640":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49c53d2edc8346db97c63010fefe60da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b6ebe945a9e4234a3588f375bfdb260":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3d088f42df44726b9bff0dd74f4f9c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cd1ad25ccc04a99a1bcebfbb91ffa6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"07e4304b9ea441ecbbd30c0571444390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2820755d5c0f4606887533770945491a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c657d343415b4b3a9225353461da0127":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3299928a9e8d416fbb2fdcf5c71e2750","IPY_MODEL_ecbc030dafa54caaa45a0c703ac0f839","IPY_MODEL_97ab3d4bb5ca495e90ba7ed0153b6031"],"layout":"IPY_MODEL_53088384236e4e8abb81dca3e806ad8f"}},"3299928a9e8d416fbb2fdcf5c71e2750":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_726975d1daa64681846aa895e1668462","placeholder":"​","style":"IPY_MODEL_52102dc7f3de418db0eeb2003e2c7839","value":"generation_config.json: 100%"}},"ecbc030dafa54caaa45a0c703ac0f839":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5075b7e58944ff28c9e65fbe067ee60","max":166,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba9d36b4c3424fb1aa0bd65e1a063d89","value":166}},"97ab3d4bb5ca495e90ba7ed0153b6031":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18693ab9f5a14286838e64708c340342","placeholder":"​","style":"IPY_MODEL_1ee5407a426d479d87332b5b3736d7ad","value":" 166/166 [00:00&lt;00:00, 19.6kB/s]"}},"53088384236e4e8abb81dca3e806ad8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"726975d1daa64681846aa895e1668462":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52102dc7f3de418db0eeb2003e2c7839":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5075b7e58944ff28c9e65fbe067ee60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba9d36b4c3424fb1aa0bd65e1a063d89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18693ab9f5a14286838e64708c340342":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ee5407a426d479d87332b5b3736d7ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60d978e69eb845c982be256dbfbcd1b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c46817662793409b832d061ce92dd303","IPY_MODEL_ec96dd4bc024433fbb93663f6502f5d6","IPY_MODEL_d3c5a796136e493bb80c93b9b76de073"],"layout":"IPY_MODEL_4518a0c1d8d64ac088c8cb82029a2b77"}},"c46817662793409b832d061ce92dd303":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93fa4c4bcc3042049bb10a24664809b9","placeholder":"​","style":"IPY_MODEL_d74de1a39b0e4eeb982b4f1b7e8677cb","value":"model.safetensors: 100%"}},"ec96dd4bc024433fbb93663f6502f5d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88b72e6b233446d4b50346a5ce28a9b6","max":3324205698,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e9021ee9b0244c19d854c7ea2a4f074","value":3324205698}},"d3c5a796136e493bb80c93b9b76de073":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f3f3517ce3447a58805e6d9fdc3db80","placeholder":"​","style":"IPY_MODEL_10e68b002d834300979c6767e66d6d02","value":" 3.32G/3.32G [00:07&lt;00:00, 663MB/s]"}},"4518a0c1d8d64ac088c8cb82029a2b77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93fa4c4bcc3042049bb10a24664809b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d74de1a39b0e4eeb982b4f1b7e8677cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88b72e6b233446d4b50346a5ce28a9b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e9021ee9b0244c19d854c7ea2a4f074":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f3f3517ce3447a58805e6d9fdc3db80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10e68b002d834300979c6767e66d6d02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86323caba22c46e782f5832eecebd5de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7fe978f265f4a2d9e2b12d402ec8cc2","IPY_MODEL_03437ec77b3a432fbdc777474add756a","IPY_MODEL_02cc0476b6ae421eb81ba325c6996371"],"layout":"IPY_MODEL_6f15c78aa1aa42f0b2cf87f22e922b02"}},"d7fe978f265f4a2d9e2b12d402ec8cc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1be894066ff427c9aef7bb583377879","placeholder":"​","style":"IPY_MODEL_da5f2302c4a54c4e8d30d3e4a75a6e38","value":""}},"03437ec77b3a432fbdc777474add756a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1245728220a4c87a4119b9024855370","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87d7e934e3f943b38de617d1a8ac90d2","value":1}},"02cc0476b6ae421eb81ba325c6996371":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50d7b70d9ef34c2abe455cdd3458b0f9","placeholder":"​","style":"IPY_MODEL_8700892b68af447185684a97ef82ce54","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00, 10.74it/s]\n"}},"6f15c78aa1aa42f0b2cf87f22e922b02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1be894066ff427c9aef7bb583377879":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da5f2302c4a54c4e8d30d3e4a75a6e38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1245728220a4c87a4119b9024855370":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87d7e934e3f943b38de617d1a8ac90d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50d7b70d9ef34c2abe455cdd3458b0f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8700892b68af447185684a97ef82ce54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52408356313d491fbb4683ea50d8fbbe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c0c2afd307cc46308cbaffc2fe6c383e","IPY_MODEL_7e8de4401f1c4207becd8f7940f62f48","IPY_MODEL_e36740ed19724c8b8aa3a7140c66419b"],"layout":"IPY_MODEL_5172a48ab2824b698228675d83440009"}},"c0c2afd307cc46308cbaffc2fe6c383e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b58610314bde4114a4a67e6fcd3f9070","placeholder":"​","style":"IPY_MODEL_54671f66e10f48e7bace465567a9f060","value":""}},"7e8de4401f1c4207becd8f7940f62f48":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31b041d77a8340cfa6663ec04f1479f4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3359e171713a44b8ae0dd8e4e0bd36ab","value":1}},"e36740ed19724c8b8aa3a7140c66419b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3335174f4d3b4e678634ea50ddef4546","placeholder":"​","style":"IPY_MODEL_7eea11cd89f14c57aef438fdd5afa5d3","value":"Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01&lt;00:00,  1.14s/it]\n"}},"5172a48ab2824b698228675d83440009":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b58610314bde4114a4a67e6fcd3f9070":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54671f66e10f48e7bace465567a9f060":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31b041d77a8340cfa6663ec04f1479f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3359e171713a44b8ae0dd8e4e0bd36ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3335174f4d3b4e678634ea50ddef4546":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eea11cd89f14c57aef438fdd5afa5d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bac00b7731784f69851d018971052134":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_991a12fd57cf439f9f6e885cbf43549a","IPY_MODEL_24fea4efafc543a4861b49fe5ec4dac8","IPY_MODEL_acf72109026e4e08834702a740eb5db9"],"layout":"IPY_MODEL_d2aca9da4bf34abdb0b67f0a42c449cb"}},"991a12fd57cf439f9f6e885cbf43549a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ada4ae8d125e429389fcefb06a4e1747","placeholder":"​","style":"IPY_MODEL_61c4ce9466e0466eaf3348cf8a8d52a1","value":"Adding requests: 100%"}},"24fea4efafc543a4861b49fe5ec4dac8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_037ed8a909f747d195390c8e3e389399","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc7084dd3f0e444699e5a7cd85ac4397","value":164}},"acf72109026e4e08834702a740eb5db9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5bf30d2a4484da38780ac37e21c2a5f","placeholder":"​","style":"IPY_MODEL_61bde22bcfcd4075baa764f66bbe6232","value":" 164/164 [00:00&lt;00:00, 927.82it/s]"}},"d2aca9da4bf34abdb0b67f0a42c449cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ada4ae8d125e429389fcefb06a4e1747":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61c4ce9466e0466eaf3348cf8a8d52a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"037ed8a909f747d195390c8e3e389399":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc7084dd3f0e444699e5a7cd85ac4397":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5bf30d2a4484da38780ac37e21c2a5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61bde22bcfcd4075baa764f66bbe6232":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2ac4aa106d34d508f3e1d486cd77177":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf7aa0942dbe4655affdbbf8a61ae6a2","IPY_MODEL_f476bdf66a2742b8a484eba4b477e0f3","IPY_MODEL_f239be2beaff4fea9c71d44e9dd30f5f"],"layout":"IPY_MODEL_fd743b489bab4da8a1b806892f45d279"}},"cf7aa0942dbe4655affdbbf8a61ae6a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9430edff75944a3db491f7b93f779ae5","placeholder":"​","style":"IPY_MODEL_80a22037f8da4c41b7758e9c7c80e87a","value":"Processed prompts: 100%"}},"f476bdf66a2742b8a484eba4b477e0f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c21dc12cdcfe401585c273664157d0c7","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0748ee2cf20e45cb926b283f31a6b935","value":164}},"f239be2beaff4fea9c71d44e9dd30f5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1b28075152e4d70bed129ed40bfb359","placeholder":"​","style":"IPY_MODEL_f1058bf3b3e24b0b83e3c1320fd1346c","value":" 164/164 [00:15&lt;00:00,  2.37it/s, est. speed input: 2179.37 toks/s, output: 2028.80 toks/s]"}},"fd743b489bab4da8a1b806892f45d279":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"9430edff75944a3db491f7b93f779ae5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80a22037f8da4c41b7758e9c7c80e87a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c21dc12cdcfe401585c273664157d0c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0748ee2cf20e45cb926b283f31a6b935":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1b28075152e4d70bed129ed40bfb359":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1058bf3b3e24b0b83e3c1320fd1346c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Step 1: Mounting Google Drive and Importing Libraries\n"],"metadata":{"id":"qMEo3HOH-slR"}},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrrwKNmg-lNR","executionInfo":{"status":"ok","timestamp":1767353581009,"user_tz":360,"elapsed":715,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"7529fe9b-ea2b-45a1-bb71-44ca63495647"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/grpo-verified-reasoner\n","data\t\t\t      LICENSE\t outputs    unsloth_compiled_cache\n","grpo_trainer_lora_model       models\t README.md  _unsloth_sentencepiece_temp\n","huggingface_tokenizers_cache  notebooks  src\t    wandb\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd /content/drive/MyDrive/grpo-verified-reasoner\n","!ls"]},{"cell_type":"code","source":["# Install UV (Faster pip)\n","!pip install --upgrade -qqq uv"],"metadata":{"id":"0q0FVqc67uRU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import subprocess"],"metadata":{"id":"ZKwjGl3V7zI1","executionInfo":{"status":"ok","timestamp":1767353294348,"user_tz":360,"elapsed":3,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:False\""],"metadata":{"id":"imojZdcn7ztn","executionInfo":{"status":"ok","timestamp":1767353277513,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\""],"metadata":{"id":"5Y5vd4i8712_","executionInfo":{"status":"ok","timestamp":1767353278790,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install -q unsloth vllm human-eval tqdm\n","else:\n","    # Version matching for Colab GPUs\n","    try:\n","        import numpy, PIL\n","        get_numpy = f\"numpy=={numpy.__version__}\"\n","        get_pil   = f\"pillow=={PIL.__version__}\"\n","    except Exception:\n","        get_numpy, get_pil = \"numpy\", \"pillow\"\n","\n","    try:\n","        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n","    except Exception:\n","        is_t4 = False\n","\n","    # A100/H100: vllm 0.10.2, T4: vllm 0.9.2 + pinned triton\n","    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n","\n","    !uv pip install -qqq --upgrade \\\n","        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers tqdm human-eval\n","    !uv pip install -qqq {get_triton}"],"metadata":{"id":"vAQLD8-a9blY","executionInfo":{"status":"ok","timestamp":1767348393071,"user_tz":360,"elapsed":36267,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import gc\n","import json\n","import re\n","import ast\n","import torch\n","import random\n","import textwrap\n","import numpy as np\n","from tqdm import tqdm\n","\n","from unsloth import FastLanguageModel\n","from vllm import SamplingParams\n","from pathlib import Path\n","\n","from human_eval.data import read_problems, write_jsonl\n","from human_eval.evaluation import evaluate_functional_correctness"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4hqdCI59tMU","executionInfo":{"status":"ok","timestamp":1767353345723,"user_tz":360,"elapsed":23998,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"b7974faf-a0eb-45c4-b781-9adad8605800"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","INFO 01-02 11:29:00 [__init__.py:216] Automatically detected platform cuda.\n","🦥 Unsloth Zoo will now patch everything to make training faster!\n"]}]},{"cell_type":"code","source":["SEED = 3407\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","!nvidia-smi -L"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_b5DzuU-S6z","executionInfo":{"status":"ok","timestamp":1767353345835,"user_tz":360,"elapsed":79,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"197bf618-df66-41ce-cfbe-4dfe4be46bb8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-bb257f27-7b11-6040-93de-72c8cff7bb91)\n"]}]},{"cell_type":"markdown","source":["# Step 2: Verifying GPU and Environment"],"metadata":{"id":"OWuA9pSe-0M8"}},{"cell_type":"code","source":["print(\"Torch version:\", torch.__version__)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KmlB9Uie-xLj","executionInfo":{"status":"ok","timestamp":1767353355034,"user_tz":360,"elapsed":11,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"12abebff-2290-411e-efde-10f83e3f82ed"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 2.8.0+cu128\n","CUDA available: True\n","GPU: NVIDIA A100-SXM4-80GB\n"]}]},{"cell_type":"markdown","source":["# Step 3: Setting Up the Main Variables"],"metadata":{"id":"zA7FVu96-anB"}},{"cell_type":"code","source":["# HumanEval evaluation settings\n","N_SAMPLES_PER_PROBLEM = 1          # pass@1 by default; raise to 5 or 10 if you later want pass@k\n","MAX_NEW_TOKENS_NON_COT = 512       # Non-CoT completions are usually short (function body)\n","MAX_NEW_TOKENS_COT = 2048           # CoT can be longer due to tags + full function\n","\n","TEMP_NON_COT = 0.0                # low sampling noise; stable for benchmarking\n","TEMP_COT = 0.0                     # encourages exploration under schema (optional)\n","\n","TOP_P = 0.95\n","MIN_P = 0.10\n","\n","# Stop guards (prevent rambling without clipping typical solutions)\n","STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]"],"metadata":{"id":"xjIWuUg0-aC4","executionInfo":{"status":"ok","timestamp":1767353357652,"user_tz":360,"elapsed":5,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Output paths (keep all artifacts under one folder)\n","EVAL_DIR = \"data/evaluation\"\n","os.makedirs(EVAL_DIR, exist_ok=True)"],"metadata":{"id":"SN2W2UUc-XgA","executionInfo":{"status":"ok","timestamp":1767353357788,"user_tz":360,"elapsed":1,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Model paths / identifiers\n","BASE_MODEL_PATH = \"unsloth/Qwen3-4B-Base\"\n","SFT_MODEL_PATH  = \"models/qwen3-4b-sft\"\n","GRPO_MODEL_PATH = \"models/qwen3-4b-grpo-final\""],"metadata":{"id":"fZp2YS7SABnS","executionInfo":{"status":"ok","timestamp":1767353522842,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["COT_SYSTEM_PROMPT_HUMANEVAL = \"\"\"You are a code-generation engine.\n","You must output your response in the following exact format:\n","<START_WORKING_OUT>\n","Concise reasoning steps required to solve the problem.\n","</END_WORKING_OUT>\n","<SOLUTION>\n","Valid Python code only.\n","</SOLUTION>\n","Do not output anything outside these tags.\"\"\""],"metadata":{"id":"4ezJrKFY8lGR","executionInfo":{"status":"ok","timestamp":1767353361159,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Loading HumanEval Problems"],"metadata":{"id":"tFeCRur7Aa7k"}},{"cell_type":"code","source":["problems = read_problems()  # dict: {task_id: {\"prompt\":..., \"test\":..., \"entry_point\":...}}\n","task_ids = list(problems.keys())\n","\n","print(f\"Loaded HumanEval problems: {len(task_ids)}\")\n","print(\"Example task_id:\", task_ids[0])\n","print(\"\\n--- Prompt Preview ---\")\n","print(problems[task_ids[0]][\"prompt\"][:500])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5BicgVzbAVcf","executionInfo":{"status":"ok","timestamp":1767353361769,"user_tz":360,"elapsed":9,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"fdc15b5f-63c8-4068-9625-9d013187fceb"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded HumanEval problems: 164\n","Example task_id: HumanEval/0\n","\n","--- Prompt Preview ---\n","from typing import List\n","\n","\n","def has_close_elements(numbers: List[float], threshold: float) -> bool:\n","    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n","    given threshold.\n","    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n","    False\n","    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n","    True\n","    \"\"\"\n","\n"]}]},{"cell_type":"markdown","source":["# Step 5: Prompt Builders"],"metadata":{"id":"B0hbt-vjEf-y"}},{"cell_type":"code","source":["def build_non_cot_prompt(problem: dict) -> str:\n","    \"\"\"\n","    Non-CoT: HumanEval-style continuation.\n","    We provide ONLY the HumanEval prompt.\n","    The harness will prepend this prompt again during execution.\n","    \"\"\"\n","    return problem[\"prompt\"]"],"metadata":{"id":"t6ZmWRrRAjGh","executionInfo":{"status":"ok","timestamp":1767353363684,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def build_cot_prompt(problem: dict, tokenizer) -> str:\n","    \"\"\"\n","    CoT: Uses the same chat template distribution as SFT/GRPO training.\n","    Returns a fully formatted ChatML prompt string.\n","    \"\"\"\n","    messages = [\n","        {\"role\": \"system\", \"content\": COT_SYSTEM_PROMPT_HUMANEVAL},\n","        {\"role\": \"user\", \"content\": problem[\"prompt\"]},\n","    ]\n","    return tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True,\n","    )"],"metadata":{"id":"na1taHI7Emmv","executionInfo":{"status":"ok","timestamp":1767353363992,"user_tz":360,"elapsed":3,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# Step 6: Output Post-processing Logic"],"metadata":{"id":"W_FSvWfHE24y"}},{"cell_type":"code","source":["# Stop strings (for safety against rambling)\n","# These are conservative: they stop the model from starting a NEW definition / class / main block.\n","STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]\n","\n","SOLUTION_RE = re.compile(r\"<SOLUTION>(.*?)</SOLUTION>\", re.DOTALL | re.IGNORECASE)"],"metadata":{"id":"mHbWyt3kH0PO","executionInfo":{"status":"ok","timestamp":1767353366144,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["\n","def extract_cot_solution(completion_only_text: str) -> str:\n","    \"\"\"\n","    Extract code from <SOLUTION>...</SOLUTION>.\n","    Returns \"\" on failure (schema violation → harness will fail, which is correct behavior).\n","    \"\"\"\n","    m = SOLUTION_RE.search(completion_only_text)\n","    if not m:\n","        return \"\"\n","    return m.group(1).strip()"],"metadata":{"id":"VN1M8SizH0Ne","executionInfo":{"status":"ok","timestamp":1767353368666,"user_tz":360,"elapsed":40,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def truncate_on_stop_strings(text: str, stop_strings: list[str]) -> str:\n","    \"\"\"\n","    Stops the completion if it begins a new unrelated block (def/class/main).\n","    This reduces harness crashes from rambling continuations.\n","    \"\"\"\n","    cut = len(text)\n","    for s in stop_strings:\n","        idx = text.find(s)\n","        if idx != -1:\n","            cut = min(cut, idx)\n","    return text[:cut].rstrip()"],"metadata":{"id":"LS7QqhbiH0LJ","executionInfo":{"status":"ok","timestamp":1767353370000,"user_tz":360,"elapsed":3,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def cleaner(code: str, entry_point: str, is_cot: bool) -> str:\n","    # A. EXTRACT CoT\n","    if is_cot:\n","        if \"<SOLUTION>\" in code:\n","            code = code.split(\"<SOLUTION>\")[-1]\n","            if \"</SOLUTION>\" in code:\n","                code = code.split(\"</SOLUTION>\")[0]\n","        else:\n","            return \"\"\n","\n","    # B. REMOVE MARKDOWN\n","    code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n","\n","    # C. FILTER LINES\n","    lines = code.split('\\n')\n","    filtered_lines = []\n","\n","    def_pattern = re.compile(rf\"^\\s*def\\s+{re.escape(entry_point)}(\\s*\\(|\\s*:)\")\n","\n","    for line in lines:\n","        # Remove redundant definition\n","        if def_pattern.match(line):\n","            continue\n","        # Remove \"from typing\" (Always redundant in HumanEval, creates noise)\n","        if line.strip().startswith(\"from typing\"):\n","            continue\n","        filtered_lines.append(line)\n","\n","    code = \"\\n\".join(filtered_lines)\n","\n","    # D. REMOVE DOCSTRINGS\n","    code = re.sub(r'(\\s*(\"\"\"|\\'\\'\\')[\\s\\S]*?\\2)', '', code, count=1)\n","    code = code.lstrip('\\n\\r')\n","\n","    # E. SMART RELATIVE NORMALIZATION (The Fix)\n","    if code.strip():\n","        lines = code.split('\\n')\n","\n","        # 1. Find the \"Reference Line\" to measure indentation.\n","        # We skip lines starting with 'import' so we measure the ACTUAL code body.\n","        reference_line = None\n","        for l in lines:\n","            stripped = l.strip()\n","            if stripped and not stripped.startswith((\"import \", \"from \")):\n","                reference_line = l\n","                break\n","\n","        # Fallback: If no code found (only imports?), use the first line.\n","        if reference_line is None:\n","            reference_line = next((l for l in lines if l.strip()), None)\n","\n","        if reference_line:\n","            # Measure indentation of the BODY\n","            body_indent = len(reference_line) - len(reference_line.lstrip())\n","\n","            normalized_lines = []\n","            for line in lines:\n","                # If the line is empty, just strip it\n","                if not line.strip():\n","                    normalized_lines.append(\"\")\n","                    continue\n","\n","                # Calculate current indent\n","                current_indent = len(line) - len(line.lstrip())\n","\n","                # Subtract the body_indent.\n","                # If an import is at 0 and body is at 4, this becomes -4.\n","                # We max(0, ...) to ensure we don't crash, effectively pulling the import in.\n","                new_indent = max(0, current_indent - body_indent)\n","\n","                # Reconstruct the line\n","                normalized_lines.append(\" \" * new_indent + line.lstrip())\n","\n","            code = \"\\n\".join(normalized_lines)\n","\n","        # Finally, indent everything by 4 for the harness\n","        code = textwrap.indent(code, '    ')\n","\n","    return code"],"metadata":{"id":"ZFsflZFYrXSM","executionInfo":{"status":"ok","timestamp":1767353379022,"user_tz":360,"elapsed":1,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# Step 7: Defining Evaluation Loop"],"metadata":{"id":"NO7zEeCjK8le"}},{"cell_type":"code","source":["def evaluate_model(\n","    model_path: str,\n","    problems: dict,\n","    task_ids: list[str],\n","    *,\n","    use_cot: bool,\n","    output_jsonl: str,\n","    max_new_tokens: int,\n","    temperature: float,\n","    top_p: float = 0.95,\n","    min_p: float = 0.10,\n","    load_in_4bit: bool = True,\n","    gpu_memory_utilization: float = 0.7,\n","):\n","    print(f\"\\n Loading: {model_path} | Mode: {'CoT' if use_cot else 'Non-CoT'}\")\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name=model_path,\n","        max_seq_length=4096,\n","        load_in_4bit=load_in_4bit,\n","        fast_inference=True,\n","        gpu_memory_utilization=gpu_memory_utilization,\n","    )\n","    FastLanguageModel.for_inference(model)\n","\n","    prompts = []\n","    print(f\" Preparing {len(task_ids)} prompts...\")\n","    for task_id in task_ids:\n","        problem = problems[task_id]\n","        if use_cot:\n","            prompt_text = build_cot_prompt(problem, tokenizer)\n","        else:\n","            prompt_text = build_non_cot_prompt(problem)\n","        prompts.append(prompt_text)\n","\n","    # Auto-Switching Stop Logic\n","    if use_cot:\n","        stop_tokens = [\"</SOLUTION>\"]\n","    else:\n","        stop_tokens = [\"\\nclass\", \"\\nif __name__\", \"\\nprint\", \"\\ndef \"]\n","\n","    sampling_params = SamplingParams(\n","        temperature=temperature,\n","        top_p=top_p,\n","        min_p=min_p,\n","        max_tokens=max_new_tokens,\n","        stop=stop_tokens,\n","        # repetition_penalty REMOVED per orders\n","    )\n","\n","    print(f\" Running vLLM Batch Generation...\")\n","    outputs = model.fast_generate(prompts, sampling_params=sampling_params)\n","\n","    samples = []\n","    for i, task_id in enumerate(task_ids):\n","        problem = problems[task_id]\n","        completion_only = outputs[i].outputs[0].text\n","\n","        completion = cleaner(completion_only, problem[\"entry_point\"], use_cot)\n","\n","        samples.append({\n","            \"task_id\": task_id,\n","            \"prompt\": problem[\"prompt\"],\n","            \"completion\": completion\n","        })\n","\n","    write_jsonl(output_jsonl, samples)\n","    print(f\" Saved {len(samples)} samples to: {output_jsonl}\")\n","\n","    del model, tokenizer\n","    gc.collect()\n","    torch.cuda.empty_cache()"],"metadata":{"id":"IGtRrQolGSLk","executionInfo":{"status":"ok","timestamp":1767353435055,"user_tz":360,"elapsed":19,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["# Step 8: Generating JSONL Files for Evaluation"],"metadata":{"id":"ZsDfO98zwS48"}},{"cell_type":"code","source":["GEN_DIR = Path(EVAL_DIR) / \"generations\"\n","GEN_DIR.mkdir(parents=True, exist_ok=True)"],"metadata":{"id":"czY25ERRwcWH","executionInfo":{"status":"ok","timestamp":1767353452034,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# BASE MODEL (Non-CoT)\n","# -------------------------\n","evaluate_model(\n","    model_path=BASE_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=False,\n","    output_jsonl=str(GEN_DIR / \"base_non_cot.jsonl\"),\n","    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n","    temperature=TEMP_NON_COT,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9e07d981d95b4b6fac218bf1a5cd4310","c97bc9dff1e54ca682f684f2ecda5f91","9517697050174a11a667421cf3b82dc8","4c8666538c57445dbc451d8eb56355cb","23703f7d48a249dc9c52a329e90111e4","7caa2ebfd52e4cd9975fae9c894db6b2","ddb5f0a4713e415eb1eb0fa7117cacd8","c37d9c2146af49f2a8871369274c7117","65d55abc399c46cda630fe6ce221dd70","56491b9d325e4efa934f324f8f7ac069","c59249c1219f4d8aa78f5640105f3d51","86453b4351bd458d94985eb2e2369478","e37c5f6d52164cf698863c6024f553ac","cb96ab4fbef44ca58be90f961029698a","fdb0c52bc2ef40d484b2efbbf10f60c0","dc5e8d1c4930442cb058d226df998809","ed1b82d393c84942a17e5f6a05109f2b","a425bef413e9416788939c8a7017a44b","1bb44fd0835c407885c4c5fe8e4d72da","fd005e42b5a24f379c99fb2c8d525a0f","7f325c8a677e4ac9a0fda1ee5ce7dbab","128d005d68bb48c986a1844810e62455","cfc1a7a603b548eeba145344d6c6380c","31dd11d1d36b4ec481c9835c7c9bd4f7","96bfd49bb45f44d080bc259c9a9021d0","6cd33c9a903a448c84ce655286c75240","5d92da5b6bb9425698d1f01c1e8393f3","e2c6b77ca36a43d2b5cb5bcab1e07443","756f24906c634b04b369818594729397","88fea147f555478195e17483b49808fb","558bea4d97b84928941f5760ba849b9f","a1c5409099154a8bb10cc70a15d875a2","10879fd7064a42d28ae3b330ce166e75","573e32f79cfc4a44ac6e9da4a133ec51","0f9691a3300545dab1057fbcd57f7dee","d791cd625ef44a79b68a7bd4ecefdc17","13f6bef9ac914ec881bbed09ca41654e","9aa5130192da4193907133cd94e88739","ad81c3d4627f4f779fa707604a324a29","9d5d1f2424d545e4a5c5dea1950889c8","6c4c54209d7344daad6f66fed3f805a9","39a4e4ecadc846138ebc364a8d46c577","4ced5aa9982342ecaf19419929d8cf9c","291e1af1a80c41c5a9bb5da37776cb6d"]},"id":"s2O5o6iww1uQ","executionInfo":{"status":"ok","timestamp":1767353974514,"user_tz":360,"elapsed":116992,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"6ae52a9c-a51a-4365-de86-3930c9a27d6d"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Loading: unsloth/Qwen3-4B-Base | Mode: Non-CoT\n","INFO 01-02 11:37:40 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 11:37:40 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 19.78%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 64.\n","Unsloth: vLLM's KV Cache can use up to 12.83 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 11:37:49 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.19782509162794168, 'max_num_batched_tokens': 6144, 'max_num_seqs': 64, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 11:37:51 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n","INFO 01-02 11:37:51 [__init__.py:1815] Using max model len 4096\n","INFO 01-02 11:37:52 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=6144.\n","WARNING 01-02 11:37:52 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n","INFO 01-02 11:37:55 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":128,\"local_cache_dir\":null}\n","INFO 01-02 11:37:56 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 11:37:57 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 11:37:57 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 11:37:58 [weight_utils.py:348] Using model weights format ['*.safetensors']\n","INFO 01-02 11:37:59 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e07d981d95b4b6fac218bf1a5cd4310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86453b4351bd458d94985eb2e2369478"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:38:02 [gpu_model_runner.py:2392] Model loading took 3.3523 GiB and 2.875338 seconds\n","INFO 01-02 11:38:17 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/0550210604/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 11:38:17 [backends.py:550] Dynamo bytecode transform time: 14.14 s\n","INFO 01-02 11:38:23 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.087 s\n","INFO 01-02 11:38:26 [monitor.py:34] torch.compile takes 14.14 s in total\n","INFO 01-02 11:38:28 [gpu_worker.py:298] Available KV cache memory: 11.71 GiB\n","INFO 01-02 11:38:29 [kv_cache_utils.py:864] GPU KV cache size: 85,296 tokens\n","INFO 01-02 11:38:29 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 20.82x\n","INFO 01-02 11:38:29 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n","INFO 01-02 11:38:29 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"output_type":"stream","name":"stderr","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:06<00:00,  3.07it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:03<00:00,  2.95it/s]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:38:39 [gpu_model_runner.py:3118] Graph capturing finished in 10 secs, took 0.27 GiB\n","INFO 01-02 11:38:39 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 10 secs.\n","INFO 01-02 11:38:39 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 10 secs.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:38:41 [gpu_worker.py:391] Free memory on device (22.42/79.32 GiB) on startup. Desired GPU memory utilization is (0.19782509162794168, 15.69 GiB). Actual usage is 3.35 GiB for weight, 0.62 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.27 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=12133338214` to fit into requested memory, or `--kv-cache-memory=19354002944` to fully utilize gpu memory. Current kv cache memory in use is 12577934438 bytes.\n","INFO 01-02 11:38:42 [core.py:218] init engine (profile, create kv cache, warmup model) took 39.97 seconds\n","INFO 01-02 11:38:44 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 11:38:44 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm2', 'q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm1', 'ffn_norm', 'k_norm', 'layer_norm1', 'norm', 'input_layernorm', 'post_attention_layernorm', 'norm2', 'attention_norm', 'post_layernorm']\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm2', 'cross_attn_input_layernorm', 'q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm1', 'ffn_norm', 'k_norm', 'layer_norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'post_attention_layernorm', 'norm2', 'attention_norm', 'post_layernorm']\n"," Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"output_type":"display_data","data":{"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfc1a7a603b548eeba145344d6c6380c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"573e32f79cfc4a44ac6e9da4a133ec51"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Saved 164 samples to: data/evaluation/generations/base_non_cot.jsonl\n"]}]},{"cell_type":"code","source":["# -------------------------\n","# SFT MODEL (Non-CoT)\n","# -------------------------\n","evaluate_model(\n","    model_path=SFT_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=False,\n","    output_jsonl=str(GEN_DIR / \"sft_non_cot.jsonl\"),\n","    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n","    temperature=TEMP_NON_COT,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["52dae0c139574d6b97726e49fbc4f499","76d06788dd484852bb1af7d7a00eedce","906c8feadcec4feda4c2ef5c71c58ad1","fad724606e4c42ab8ec76dbac81759bb","6a0f7b13060848929f3ab0f7c97f5018","77d699cc0a55478c85801098e92c0ac7","25ba8d7ce7c84e79b500e2fe8469d20b","a5271e0fa88b4bcf94b59c32f4fd4ad9","bd45b3f39f26457294feb175c29f4f84","2287da862c6047f58b0f6c2fd5406a72","81bcf1025a8f4d928072d3112baa0d5f","260af0bd31c543f78ac7890b26ec34cb","217f205297894f469c75ed0299e2811c","092f90f9b74f4cb6aa193d609c34f882","046614e4ca634ca299bf87263fcff97d","eb6fb60334a243379a8560fd0e155bbe","e8efce440d30465b8c26c1e4e26ff761","8959a054c6fd448e81f873d33531b8d4","88f7a819ea6149bbb3552eff51a81f37","df08567d5f99413b95aa9ee45569c191","a844930fb787424c8f6a778d5df54a0f","7ec9d9f2630e4cac97a6512b2faa2230","79a475f98248445db0139a3301493fbe","021e3768bdf449c1b4e0f2e1c0d07aa0","70cde0f9c934447e80f8124b961cd81d","47ae4883970c4d339b2f21a74ccdaf8c","793b9bf9231d4d529d28926e85c64302","bdda8c9bc3634a6894ca37e6eb8ff0c8","707cb13f5ff14b9c961f61a419c3660d","16e7c63350414f12b70ab8ffe3cc1664","ffef79f8dde4489ba108af5422b4f837","c64a75b5a77c41228c0cc00225faba42","d41ddc60d37e4436b2e7414e0c5d5a65","0a047b552d5b4c179453c4afda7ba89a","8315ce042ce945549fd198d7b4cb3d78","c883463a5b2b41b8b293437b0fb87edb","74c83541d6474bd391da1d41d46ed6dd","c5c031ff8c86479caf49f954548b1a08","a55db7e85e5841eeac6ba943b9b5a118","c254b2673a624e7aa74391b4998f3eb0","79285b34a9ad4ec48d35d6b56ce69cc6","7205d163371b4d5b966050dbe3fb9bec","f758cc49cff543afa37a4be57eb86d60","7478ec008e664cec9dcc38fd1ad6a2e2"]},"id":"yeEE_WbAxYOX","executionInfo":{"status":"ok","timestamp":1767353725454,"user_tz":360,"elapsed":125409,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"54556c95-5169-459d-dcd7-b7551e5e463f"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Loading: models/qwen3-4b-sft | Mode: Non-CoT\n","INFO 01-02 11:33:21 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 11:33:21 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 69.6%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n","Unsloth: vLLM's KV Cache can use up to 52.34 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 11:33:31 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.6960016128672332, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 11:33:50 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"]},{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:33:50 [__init__.py:1815] Using max model len 4096\n","WARNING 01-02 11:33:50 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n","INFO 01-02 11:33:53 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n","WARNING 01-02 11:33:53 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n","INFO 01-02 11:33:56 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n","INFO 01-02 11:33:57 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n","WARNING 01-02 11:33:57 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n","INFO 01-02 11:33:57 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 11:33:57 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 11:33:58 [cuda.py:362] Using Flash Attention backend on V1 engine.\n","INFO 01-02 11:33:58 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 11:33:59 [weight_utils.py:348] Using model weights format ['*.safetensors']\n","INFO 01-02 11:34:00 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52dae0c139574d6b97726e49fbc4f499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"260af0bd31c543f78ac7890b26ec34cb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:34:01 [punica_selector.py:19] Using PunicaWrapperGPU.\n","INFO 01-02 11:34:03 [gpu_model_runner.py:2392] Model loading took 3.3602 GiB and 3.964858 seconds\n","INFO 01-02 11:34:19 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ca83b4ba7b/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 11:34:19 [backends.py:550] Dynamo bytecode transform time: 15.01 s\n","INFO 01-02 11:34:26 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.177 s\n","INFO 01-02 11:34:29 [monitor.py:34] torch.compile takes 15.01 s in total\n","INFO 01-02 11:34:31 [gpu_worker.py:298] Available KV cache memory: 51.01 GiB\n","INFO 01-02 11:34:32 [kv_cache_utils.py:864] GPU KV cache size: 371,408 tokens\n","INFO 01-02 11:34:32 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 90.68x\n","INFO 01-02 11:34:32 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"output_type":"stream","name":"stderr","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:11<00:00,  3.09it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:06<00:00,  3.01it/s]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:34:50 [gpu_model_runner.py:3118] Graph capturing finished in 18 secs, took 1.21 GiB\n","INFO 01-02 11:34:50 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 18 secs.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:34:51 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.6960016128672332, 55.21 GiB). Actual usage is 3.36 GiB for weight, 0.82 GiB for peak activation, 0.02 GiB for non-torch memory, and 1.21 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=53313622835` to fit into requested memory, or `--kv-cache-memory=78638161920` to fully utilize gpu memory. Current kv cache memory in use is 54766949171 bytes.\n","INFO 01-02 11:34:52 [core.py:218] init engine (profile, create kv cache, warmup model) took 48.99 seconds\n","INFO 01-02 11:34:54 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 11:34:54 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm2', 'q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm1', 'ffn_norm', 'k_norm', 'layer_norm1', 'norm', 'input_layernorm', 'post_attention_layernorm', 'norm2', 'attention_norm', 'post_layernorm']\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm2', 'cross_attn_input_layernorm', 'q_norm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm1', 'ffn_norm', 'k_norm', 'layer_norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'post_attention_layernorm', 'norm2', 'attention_norm', 'post_layernorm']\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.12.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"]},{"output_type":"stream","name":"stdout","text":[" Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"output_type":"display_data","data":{"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79a475f98248445db0139a3301493fbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a047b552d5b4c179453c4afda7ba89a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Saved 164 samples to: data/evaluation/generations/sft_non_cot.jsonl\n"]}]},{"cell_type":"code","source":["# -------------------------\n","# SFT MODEL (CoT)\n","# -------------------------\n","evaluate_model(\n","    model_path=SFT_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=True,\n","    output_jsonl=str(GEN_DIR / \"sft_cot.jsonl\"),\n","    max_new_tokens=MAX_NEW_TOKENS_COT,\n","    temperature=TEMP_COT,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7de28f0af56c4637a5b0cdb56d41c24a","1d9e6327d2c94c3e9de690690d1d6dc1","bc36fec764324a3e8ad803bc0c6ab6a8","f4a2bc05061d41f4a0b364869195bf18","24cf7cf9a96841a7a45e6ace272bb7f9","58a0cbda19be4dadbb5d70b7ae610591","9e101b7855654afaa261a8e50f45e43c","3673be0514cd4717bda5e780e3ea1869","401127bea538400a9e9abd3cd529119d","cb74370ac3994457854b43b8c386e160","995ed386e17e4d90a6f9298ed9fc8ca3","e15a72c48a2645fb90ba60a9478ab96a","31f979ec37a64274960741b6926103d4","6e02b33090924ae786c53455e275854c","880dd4210db74f748d60539ddcf41b16","8faebbfbda944821afe9cf9e53117336","97de53a5ca994d99886f35ed36315985","7717dcdf798e430e84dafc68a4c27801","09ae04adcc774d70ba03e0d20f0b74f0","94983102be2d45e98c0ba17fbee21416","bf75a9c603544760911027e214ffb604","b84ead8524e243f59ad6bd05a8794f79","b38723147a504817a4db00afa4568c48","d78dfe1df6f6466b82dfda067d163019","e8e7aa6b20f0498db47ec6dde1e28828","1ea3a70931b344ba8aac50cc764b9078","8e67fb40859248d5b6578c80a5d95fb9","9a3d4e2138f64e0ba949d908484a932a","8fd88a2a7f7340ec93d3f6a241f5c577","52d418e654e74fd7b80e7e26d1c8c7a7","c012574146c345dcaa2342fe1b02dcb5","e2049941b8104e9eaf5986887fd0b222","2e92afebe0ec445cbd968b1cbc0eae8c","c28de286b97749479e130f0ffa8ae920","62b05eae8327439e81ed605f77178b8e","d4950467d4cd4ba7abfeba3e458e5800","82dee44ec54b4995b2cbd4e0f0884837","19dafd90627144e48934c47daa080c5e","95949ab50ea644a0bd60787b55c08eb7","44856bac63de4e62b16a4f04c1e5deb9","002556d328684773b0029a9a816807bd","b64d826707a040ca8e606cc619ce1d4c","717a62bb0b62431299b2e969d9bbbca4","00b46237fc6e4d8eb5fb58335134c7f5"]},"id":"LwL8oA_SMkdv","executionInfo":{"status":"ok","timestamp":1767352723316,"user_tz":360,"elapsed":196739,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"2b81fa11-4237-41ce-a40d-4a68486d8c76"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Loading: models/qwen3-4b-sft | Mode: CoT\n","INFO 01-02 11:15:28 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 11:15:28 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 6.49%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 16.\n","Unsloth: vLLM's KV Cache can use up to 2.28 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 11:15:38 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.06489528290199138, 'max_num_batched_tokens': 2048, 'max_num_seqs': 16, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 11:15:40 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n","INFO 01-02 11:15:40 [__init__.py:1815] Using max model len 4096\n","INFO 01-02 11:15:41 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n","WARNING 01-02 11:15:41 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n","INFO 01-02 11:15:44 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":32,\"local_cache_dir\":null}\n","INFO 01-02 11:15:46 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 11:15:47 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 11:15:47 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 11:15:47 [weight_utils.py:348] Using model weights format ['*.safetensors']\n","INFO 01-02 11:15:48 [weight_utils.py:369] Time spent downloading weights for unsloth/qwen3-4b-base-unsloth-bnb-4bit: 0.878051 seconds\n","INFO 01-02 11:15:49 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7de28f0af56c4637a5b0cdb56d41c24a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e15a72c48a2645fb90ba60a9478ab96a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:15:53 [gpu_model_runner.py:2392] Model loading took 3.3747 GiB and 3.275754 seconds\n","INFO 01-02 11:16:08 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/392892021c/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 11:16:08 [backends.py:550] Dynamo bytecode transform time: 14.29 s\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 500.45it/s, triton_poi_fused_view_6]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:16:10 [backends.py:194] Cache the graph for dynamic shape for later use\n"]},{"output_type":"stream","name":"stderr","text":["\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 536.58it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 555.90it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 532.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 532.19it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 516.41it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 509.57it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 536.51it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 535.46it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 535.42it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 534.71it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 511.74it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 519.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 534.29it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 484.97it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 494.36it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 509.05it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 531.44it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 482.41it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 523.78it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 479.50it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 519.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 517.11it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 544.71it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 524.11it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 515.59it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 530.83it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 518.65it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 536.99it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 523.89it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 524.21it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 509.98it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 518.18it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 520.71it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 521.06it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 499.24it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 466.56it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:16:19 [backends.py:215] Compiling a graph for dynamic shape takes 9.29 s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:16:22 [monitor.py:34] torch.compile takes 23.58 s in total\n","INFO 01-02 11:16:25 [gpu_worker.py:298] Available KV cache memory: 1.53 GiB\n","INFO 01-02 11:16:27 [kv_cache_utils.py:864] GPU KV cache size: 11,136 tokens\n","INFO 01-02 11:16:27 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 2.72x\n","INFO 01-02 11:16:27 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n","INFO 01-02 11:16:27 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n","INFO 01-02 11:16:27 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"output_type":"stream","name":"stderr","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:02<00:00,  2.95it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:01<00:00,  2.85it/s]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:16:31 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.05 GiB\n","INFO 01-02 11:16:31 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n","INFO 01-02 11:16:31 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n","INFO 01-02 11:16:31 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 11:16:34 [gpu_worker.py:391] Free memory on device (7.35/79.32 GiB) on startup. Desired GPU memory utilization is (0.06489528290199138, 5.15 GiB). Actual usage is 3.37 GiB for weight, 0.24 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=1429225369` to fit into requested memory, or `--kv-cache-memory=3797919232` to fully utilize gpu memory. Current kv cache memory in use is 1643134873 bytes.\n","INFO 01-02 11:16:34 [core.py:218] init engine (profile, create kv cache, warmup model) took 41.58 seconds\n","INFO 01-02 11:16:37 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 11:16:37 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'cross_attn_input_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"," Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"output_type":"display_data","data":{"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38723147a504817a4db00afa4568c48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c28de286b97749479e130f0ffa8ae920"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Saved 164 samples to: data/evaluation/generations/sft_cot.jsonl\n"]}]},{"cell_type":"code","source":["# -------------------------\n","# RPO MODEL (Non-CoT)\n","# -------------------------\n","evaluate_model(\n","    model_path=GRPO_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=False,\n","    output_jsonl=str(GEN_DIR / \"grpo_non_cot.jsonl\"),\n","    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n","    temperature=TEMP_NON_COT,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["01636da2ef524f05bd829019181ad415","67e58f0969b4499eba66fda09a242e1a","eae19252908244f4b68dc8f154acd083","05ae6026e8f346cda8a5a0a7d00e17fa","0bb0fbdda5db4e43b608c8375acde029","0b2b0f2f12584bb9bb1d2c67598889f7","54f338234f564c2ba54c99e847d7d693","e122dcb142514ce98c92bebb1598c7c7","b601e6a29afb4cee8a5007721d830151","9ea6415cddfc492da92768c335938cbf","8a95701cc0db4d6e9caa0420900d12db","399df7cd8a69427399578dbc3713acbf","b515915e02a84bde8915d354a3326e15","12bfca7c2d934018b46e508c750bff7d","662b00ad837646259828b7678beb5003","c2c15b3d54a443f2bf9cc5b9ca7b271e","92c75be1e78045079d54dcbe6e158fdb","372366e7b54a46f8a71d4cb61e4b7785","35e2a566afea4b3497d1fa2b5813db97","6aee1c7ac1cf4694892915a00f402d12","37e80677b07f4835bce151f80f8a573c","f749c55db6364c5d85302b1997937a3c","978e367ad00148cbbb4485225df7715d","42bd5c84229b4fbdab91471e87fcdb03","0693a1c390574722864ea0b2b171c052","3e0347ff60ec41deb9eb73e8c15e5d7a","faf0dc4eafc24f35b9f22d0d6133980f","cfad52bc2ebe46069f74fe208d6f165d","253ba4d2c7d447228b04602ac7c7c723","85666a43f063430f83dc5a3593254030","5e4087d7ccae4dca8d82f7836a922f4a","f608275508a4427aad84b873ab022d4d","eae556a8e0a84035843db7a56a95be5b","0676ecabe6164cf6aa4844ca5030d9e8","774b577f72094f8f81fa3703229936b7","5785a50bed224f7fa653bd8d993b7be7","3d396ac3fd684c9c8d35986bf186d5a3","2d6d2d2997c84e4897ee1a0da0c6b8d6","f52a92a6e435447796b75c3f03ba95ad","2ad6e74d83fb42ff91a48b6eb35438dc","73a14414d22c4873b9a693fdaeeda3d2","25f492a9e83946a5a040b5b9b82e2530","f6e1995166fc45b6ab6b6350da695e74","d97053da18934fdc9081aa47132cd3ca"]},"id":"8KMQzj0nVx6C","executionInfo":{"status":"ok","timestamp":1767350981604,"user_tz":360,"elapsed":168736,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"3ff6fb12-5750-44f0-8278-bc1c25298a07"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Loading: models/qwen3-4b-grpo-final | Mode: Non-CoT\n","INFO 01-02 10:46:53 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 10:46:53 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 20.41%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 64.\n","Unsloth: vLLM's KV Cache can use up to 13.33 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 10:47:03 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.20411650503366935, 'max_num_batched_tokens': 6144, 'max_num_seqs': 64, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 10:47:05 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n","INFO 01-02 10:47:05 [__init__.py:1815] Using max model len 4096\n","INFO 01-02 10:47:06 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=6144.\n","WARNING 01-02 10:47:06 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n","INFO 01-02 10:47:09 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":128,\"local_cache_dir\":null}\n","INFO 01-02 10:47:10 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 10:47:11 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 10:47:11 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 10:47:12 [weight_utils.py:348] Using model weights format ['*.safetensors']\n","INFO 01-02 10:47:13 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01636da2ef524f05bd829019181ad415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"399df7cd8a69427399578dbc3713acbf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:47:16 [gpu_model_runner.py:2392] Model loading took 3.3988 GiB and 2.904477 seconds\n","INFO 01-02 10:47:31 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/0550210604/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 10:47:31 [backends.py:550] Dynamo bytecode transform time: 14.14 s\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 412.70it/s, triton_poi_fused_view_6]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:47:35 [backends.py:194] Cache the graph for dynamic shape for later use\n"]},{"output_type":"stream","name":"stderr","text":["\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 402.89it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 417.91it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 414.07it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 400.58it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 442.91it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 432.42it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 619.74it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 601.42it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 618.25it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 627.06it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 657.19it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 635.03it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 630.86it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 612.09it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 606.70it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 596.61it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 588.25it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 634.72it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 623.19it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 637.70it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 633.17it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 625.32it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 607.94it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 649.29it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 651.71it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 658.94it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 642.74it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 558.31it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 566.97it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 640.22it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 592.09it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 623.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 597.86it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 619.56it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 597.04it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 384.81it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:48:34 [backends.py:215] Compiling a graph for dynamic shape takes 62.09 s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:48:41 [monitor.py:34] torch.compile takes 76.23 s in total\n","INFO 01-02 10:48:43 [gpu_worker.py:298] Available KV cache memory: 12.17 GiB\n","INFO 01-02 10:48:45 [kv_cache_utils.py:864] GPU KV cache size: 88,576 tokens\n","INFO 01-02 10:48:45 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 21.62x\n","INFO 01-02 10:48:45 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n","INFO 01-02 10:48:45 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"output_type":"stream","name":"stderr","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:06<00:00,  3.03it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:03<00:00,  2.92it/s]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:48:55 [gpu_model_runner.py:3118] Graph capturing finished in 10 secs, took 0.27 GiB\n","INFO 01-02 10:48:55 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 10 secs.\n","INFO 01-02 10:48:55 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 10 secs.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:48:57 [gpu_worker.py:391] Free memory on device (23.13/79.32 GiB) on startup. Desired GPU memory utilization is (0.20411650503366935, 16.19 GiB). Actual usage is 3.4 GiB for weight, 0.62 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.27 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=12618304614` to fit into requested memory, or `--kv-cache-memory=20068607488` to fully utilize gpu memory. Current kv cache memory in use is 13062900838 bytes.\n","INFO 01-02 10:48:57 [core.py:218] init engine (profile, create kv cache, warmup model) took 101.60 seconds\n","INFO 01-02 10:49:00 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 10:49:00 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'cross_attn_input_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"," Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"output_type":"display_data","data":{"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"978e367ad00148cbbb4485225df7715d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0676ecabe6164cf6aa4844ca5030d9e8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Saved 164 samples to: data/evaluation/generations/grpo_non_cot.jsonl\n"]}]},{"cell_type":"code","source":["# -------------------------\n","# GRPO MODEL (CoT)\n","# -------------------------\n","evaluate_model(\n","    model_path=GRPO_MODEL_PATH,\n","    problems=problems,\n","    task_ids=task_ids,\n","    use_cot=True,\n","    output_jsonl=str(GEN_DIR / \"grpo_cot.jsonl\"),\n","    max_new_tokens=2048, # Increased for safety, vLLM handles the speed\n","    temperature=TEMP_COT,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6a52934b8aeb45759a61e500c278e665","a190852104c14a6b87d719ebb8d6d912","7f98c8589b024b5bbe6d5c929b3b7981","1ec9bd92a4484cceae354ee4e1e5d890","f1adb3732fda47ec87c5511f8e23a659","e465c758682b457a80dca75b9530ef2c","a59bd86f96234614a60d74b60bfe5649","9a25a1efebd64039a785bf52b9176e20","84e834cc22694b31bc5604d67ac3bd6a","5ebaca34db3a4af1ac14e497f36f2d40","e16056f7f5304a4599c1fb65cbcd3ba2","9b808453ba704088a51e44ad8178c382","cee52e56dfa041be92d5947dde06f368","dfcdb300a18d4a8ca2e7ace50af48b20","f8a082d4c99f4b2e9d8543e5804f582f","bd5d748cb59540979509700c5ae30ef1","4247a676c68c4789bbbae0b5d961cf71","a7ff023db6cf459cb80a367b415d0a2c","c7a5a2454cf2412bb025c6c0792486a1","90691bb2fcf24790ac631d4e7c25d0f8","2480c3a740fd458ab468ad5668c04b11","9bdd4e57669b4cbfaa46e0bab8dc876d","a5c3561275e6474dab13cc8f021c8e01","0cef8a3cf8da4169bdb6a05676286027","9bb210d9db9f4fb787c57ec583a0cdd3","da9eabda62c842b6a7b21ab3661fb9d5","feb7cfa915224a47a8fae486ddb19578","7d8e7f625a99436f8387755ac12b784b","993e54c9466647bc8ad441860cb7623a","117c534d1b8742fe93d98856bfa6abd5","3862ea0c4555471685d890f321b1079a","55d809342c2c4ef48b5fea053589e62a","110b364877ff49d98ed6e3ce9045fc44","1cc41dd180934482a99716883c23867e","2081cec9668146a4aacaec480a3ac5b5","0375f74fe69c4b59bc128504e2755c09","e4fe97664f6e48e1bc8085ba551e62c0","1e3528036327443d953d8e70d7ae08eb","9020ef6705094a1eb845855835337f3e","db71615d176740cf94f4910a3d913527","f65139456d4846d39a07ea70d69beb26","38819b3450754189a2f5e92412e14a41","0d74d1891d464d2292c0d767b604c1f4","751595b0364241e38b47a53c7e231ff5","6febfdb59c0541618e5c749c7bd2cd50","c9b174d079e94e49b6bfbffe33f56534","92f7620bd45149e698c21a82127c816c","f4c7661dcd2441a1b7e9fbc018b667e6","65623eb5d7fb466bbbf429235302502d","d246786626444978b385c55076c3a7d2","40abe243d4ca49558cf9b064090f0b40","a70348ff14134a9b9a3ec97207b437a1","bd195ac815f7440a82d521e21b995ca6","65cac6e4708c4056b1db0ce49fbdc2b5","6dba11abceee4fb48a97a4cb218e3857","483ed27dfc6c4369bb7871eccee293fb","3b0f99baf5f349438efc7bb84926e4f7","41653c2073f24328b8ca93939aaea5fd","1de2c3c29e9e4c55a4137aadcfc1f8d6","707f0ab41ca94e02b7404e07aa084640","49c53d2edc8346db97c63010fefe60da","2b6ebe945a9e4234a3588f375bfdb260","e3d088f42df44726b9bff0dd74f4f9c8","6cd1ad25ccc04a99a1bcebfbb91ffa6a","07e4304b9ea441ecbbd30c0571444390","2820755d5c0f4606887533770945491a","c657d343415b4b3a9225353461da0127","3299928a9e8d416fbb2fdcf5c71e2750","ecbc030dafa54caaa45a0c703ac0f839","97ab3d4bb5ca495e90ba7ed0153b6031","53088384236e4e8abb81dca3e806ad8f","726975d1daa64681846aa895e1668462","52102dc7f3de418db0eeb2003e2c7839","f5075b7e58944ff28c9e65fbe067ee60","ba9d36b4c3424fb1aa0bd65e1a063d89","18693ab9f5a14286838e64708c340342","1ee5407a426d479d87332b5b3736d7ad","60d978e69eb845c982be256dbfbcd1b4","c46817662793409b832d061ce92dd303","ec96dd4bc024433fbb93663f6502f5d6","d3c5a796136e493bb80c93b9b76de073","4518a0c1d8d64ac088c8cb82029a2b77","93fa4c4bcc3042049bb10a24664809b9","d74de1a39b0e4eeb982b4f1b7e8677cb","88b72e6b233446d4b50346a5ce28a9b6","0e9021ee9b0244c19d854c7ea2a4f074","8f3f3517ce3447a58805e6d9fdc3db80","10e68b002d834300979c6767e66d6d02","86323caba22c46e782f5832eecebd5de","d7fe978f265f4a2d9e2b12d402ec8cc2","03437ec77b3a432fbdc777474add756a","02cc0476b6ae421eb81ba325c6996371","6f15c78aa1aa42f0b2cf87f22e922b02","c1be894066ff427c9aef7bb583377879","da5f2302c4a54c4e8d30d3e4a75a6e38","b1245728220a4c87a4119b9024855370","87d7e934e3f943b38de617d1a8ac90d2","50d7b70d9ef34c2abe455cdd3458b0f9","8700892b68af447185684a97ef82ce54","52408356313d491fbb4683ea50d8fbbe","c0c2afd307cc46308cbaffc2fe6c383e","7e8de4401f1c4207becd8f7940f62f48","e36740ed19724c8b8aa3a7140c66419b","5172a48ab2824b698228675d83440009","b58610314bde4114a4a67e6fcd3f9070","54671f66e10f48e7bace465567a9f060","31b041d77a8340cfa6663ec04f1479f4","3359e171713a44b8ae0dd8e4e0bd36ab","3335174f4d3b4e678634ea50ddef4546","7eea11cd89f14c57aef438fdd5afa5d3","bac00b7731784f69851d018971052134","991a12fd57cf439f9f6e885cbf43549a","24fea4efafc543a4861b49fe5ec4dac8","acf72109026e4e08834702a740eb5db9","d2aca9da4bf34abdb0b67f0a42c449cb","ada4ae8d125e429389fcefb06a4e1747","61c4ce9466e0466eaf3348cf8a8d52a1","037ed8a909f747d195390c8e3e389399","cc7084dd3f0e444699e5a7cd85ac4397","c5bf30d2a4484da38780ac37e21c2a5f","61bde22bcfcd4075baa764f66bbe6232","c2ac4aa106d34d508f3e1d486cd77177","cf7aa0942dbe4655affdbbf8a61ae6a2","f476bdf66a2742b8a484eba4b477e0f3","f239be2beaff4fea9c71d44e9dd30f5f","fd743b489bab4da8a1b806892f45d279","9430edff75944a3db491f7b93f779ae5","80a22037f8da4c41b7758e9c7c80e87a","c21dc12cdcfe401585c273664157d0c7","0748ee2cf20e45cb926b283f31a6b935","a1b28075152e4d70bed129ed40bfb359","f1058bf3b3e24b0b83e3c1320fd1346c"]},"id":"YNRyAc4MAAcP","executionInfo":{"status":"ok","timestamp":1767349014253,"user_tz":360,"elapsed":249480,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"411d5d18-af11-47ac-d7b1-5c062a86c621"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Loading: models/qwen3-4b-grpo-final | Mode: CoT\n","INFO 01-02 10:12:47 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n","INFO 01-02 10:12:47 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n","==((====))==  Unsloth 2025.12.9: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n","   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 69.6%\n","Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n","Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n","Unsloth: vLLM's KV Cache can use up to 52.34 GB. Also swap space = 6 GB.\n","Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n","Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n","INFO 01-02 10:12:57 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.6960016128672332, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n","INFO 01-02 10:13:17 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"]},{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:13:17 [__init__.py:1815] Using max model len 4096\n","WARNING 01-02 10:13:17 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n","INFO 01-02 10:13:20 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n","WARNING 01-02 10:13:20 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n","Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a52934b8aeb45759a61e500c278e665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b808453ba704088a51e44ad8178c382"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5c3561275e6474dab13cc8f021c8e01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc41dd180934482a99716883c23867e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6febfdb59c0541618e5c749c7bd2cd50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"483ed27dfc6c4369bb7871eccee293fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c657d343415b4b3a9225353461da0127"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:13:27 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n","INFO 01-02 10:13:28 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n","WARNING 01-02 10:13:28 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n","INFO 01-02 10:13:28 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n","INFO 01-02 10:13:28 [gpu_model_runner.py:2370] Loading model from scratch...\n","INFO 01-02 10:13:29 [cuda.py:362] Using Flash Attention backend on V1 engine.\n","INFO 01-02 10:13:29 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n","INFO 01-02 10:13:30 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/3.32G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60d978e69eb845c982be256dbfbcd1b4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:13:39 [weight_utils.py:369] Time spent downloading weights for unsloth/qwen3-4b-base-unsloth-bnb-4bit: 8.815815 seconds\n","INFO 01-02 10:13:39 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86323caba22c46e782f5832eecebd5de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52408356313d491fbb4683ea50d8fbbe"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:13:41 [punica_selector.py:19] Using PunicaWrapperGPU.\n","INFO 01-02 10:13:42 [gpu_model_runner.py:2392] Model loading took 3.3825 GiB and 12.377858 seconds\n","INFO 01-02 10:13:58 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ca83b4ba7b/rank_0_0/backbone for vLLM's torch.compile\n","INFO 01-02 10:13:58 [backends.py:550] Dynamo bytecode transform time: 15.21 s\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 10.44it/s, triton_poi_fused_view_6]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:14:05 [backends.py:194] Cache the graph for dynamic shape for later use\n"]},{"output_type":"stream","name":"stderr","text":["\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 19.16it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 347.40it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 326.81it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 355.44it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 334.50it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 327.11it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 463.73it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 472.54it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 454.92it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 469.06it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 483.57it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 461.87it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 462.39it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 467.36it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 461.77it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 466.29it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 475.26it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 461.62it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 476.88it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 501.06it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 494.11it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 493.94it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 492.50it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 501.82it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 453.55it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 471.54it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 498.16it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 479.40it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 456.33it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 480.59it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 513.26it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 437.21it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 517.29it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 487.53it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 441.83it/s, triton_poi_fused_view_10]\n","Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 24.52it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:15:03 [backends.py:215] Compiling a graph for dynamic shape takes 62.63 s\n","INFO 01-02 10:15:26 [monitor.py:34] torch.compile takes 77.84 s in total\n","INFO 01-02 10:15:28 [gpu_worker.py:298] Available KV cache memory: 50.98 GiB\n","INFO 01-02 10:15:29 [kv_cache_utils.py:864] GPU KV cache size: 371,216 tokens\n","INFO 01-02 10:15:29 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 90.63x\n","INFO 01-02 10:15:29 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"]},{"output_type":"stream","name":"stderr","text":["Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:20<00:00,  1.71it/s]\n","Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:06<00:00,  3.02it/s]"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:15:56 [gpu_model_runner.py:3118] Graph capturing finished in 27 secs, took 1.21 GiB\n","INFO 01-02 10:15:56 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 27 secs.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["INFO 01-02 10:15:58 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.6960016128672332, 55.21 GiB). Actual usage is 3.38 GiB for weight, 0.82 GiB for peak activation, 0.02 GiB for non-torch memory, and 1.21 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=53277184819` to fit into requested memory, or `--kv-cache-memory=78601723904` to fully utilize gpu memory. Current kv cache memory in use is 54738899763 bytes.\n","INFO 01-02 10:15:58 [core.py:218] init engine (profile, create kv cache, warmup model) took 135.94 seconds\n","INFO 01-02 10:16:01 [llm.py:295] Supported_tasks: ('generate',)\n","INFO 01-02 10:16:01 [__init__.py:36] No IOProcessor plugins requested by the model\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Performing substitution for additional_keys=set()\n","Unsloth: Just some info: will skip parsing ['layer_norm1', 'ffn_norm', 'post_feedforward_layernorm', 'cross_attn_input_layernorm', 'layer_norm2', 'q_norm', 'attention_norm', 'post_layernorm', 'norm1', 'norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'norm2', 'k_norm']\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.12.9 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"]},{"output_type":"stream","name":"stdout","text":[" Preparing 164 prompts...\n"," Running vLLM Batch Generation...\n"]},{"output_type":"display_data","data":{"text/plain":["Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac00b7731784f69851d018971052134"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ac4aa106d34d508f3e1d486cd77177"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Saved 164 samples to: data/evaluation/generations/grpo_cot.jsonl\n"]}]},{"cell_type":"code","source":["# REPLACE THIS with the path to the file you want to inspect\n","# Example: \"generation_results/grpo_cot_2048.jsonl\"\n","FILE_PATH = str(GEN_DIR / \"grpo_cot.jsonl\")\n","\n","print(f\" INSPECTING: {FILE_PATH}\\n\")\n","\n","with open(FILE_PATH, 'r') as f:\n","    for line in f:\n","        data = json.loads(line)\n","        task_id = data.get(\"task_id\", \"Unknown Task\")\n","        completion = data.get(\"completion\", \"\")\n","\n","        print(f\" TASK: {task_id}\")\n","        print(\"-\" * 80)\n","        # Printing completion directly renders newlines correctly\n","        print(completion)\n","        print(\"=\" * 80 + \"\\n\")"],"metadata":{"id":"-glzNHeLCvx8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767349396483,"user_tz":360,"elapsed":110,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"8f665da8-8e29-4768-df4e-a0bfb5e95a27"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":[" INSPECTING: data/evaluation/generations/grpo_cot.jsonl\n","\n"," TASK: HumanEval/0\n","--------------------------------------------------------------------------------\n","    for i in range(len(numbers)):\n","        for j in range(i+1, len(numbers)):\n","            if abs(numbers[i] - numbers[j]) < threshold:\n","                return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/1\n","--------------------------------------------------------------------------------\n","    paren_string = paren_string.replace(\" \", \"\")\n","    groups = []\n","    counter = 0\n","    current_group = \"\"\n","    for char in paren_string:\n","        if char == \"(\":\n","            counter += 1\n","            current_group += char\n","        elif char == \")\":\n","            counter -= 1\n","            current_group += char\n","            if counter == 0:\n","                groups.append(current_group)\n","                current_group = \"\"\n","    return groups\n","\n","================================================================================\n","\n"," TASK: HumanEval/2\n","--------------------------------------------------------------------------------\n","    integer_part = int(number)\n","    decimal_part = number - integer_part\n","    return decimal_part\n","\n","================================================================================\n","\n"," TASK: HumanEval/3\n","--------------------------------------------------------------------------------\n","    balance = 0\n","    for op in operations:\n","        balance += op\n","        if balance < 0:\n","            return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/4\n","--------------------------------------------------------------------------------\n","    x_mean = sum(numbers) / len(numbers)\n","    return sum(abs(x - x_mean) for x in numbers) / len(numbers)\n","\n","================================================================================\n","\n"," TASK: HumanEval/5\n","--------------------------------------------------------------------------------\n","    result = []\n","    for i, num in enumerate(numbers):\n","        result.append(num)\n","        if i < len(numbers) - 1:\n","            result.append(delimeter)\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/6\n","--------------------------------------------------------------------------------\n","    result = []\n","    groups = paren_string.split(' ')\n","    for group in groups:\n","        max_nesting_level = 0\n","        current_nesting_level = 0\n","        for char in group:\n","            if char == '(':\n","                current_nesting_level += 1\n","            elif char == ')':\n","                current_nesting_level -= 1\n","            if current_nesting_level > max_nesting_level:\n","                max_nesting_level = current_nesting_level\n","        result.append(max_nesting_level)\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/7\n","--------------------------------------------------------------------------------\n","    filtered_strings = [s for s in strings if substring in s]\n","    return filtered_strings\n","\n","================================================================================\n","\n"," TASK: HumanEval/8\n","--------------------------------------------------------------------------------\n","    sum_result = 0\n","    product_result = 1\n","    for num in numbers:\n","        sum_result += num\n","        product_result *= num\n","    return (sum_result, product_result)\n","\n","================================================================================\n","\n"," TASK: HumanEval/9\n","--------------------------------------------------------------------------------\n","    max_list = []\n","    current_max = float('-inf')\n","    for num in numbers:\n","        if num > current_max:\n","            current_max = num\n","        max_list.append(current_max)\n","    return max_list\n","\n","================================================================================\n","\n"," TASK: HumanEval/10\n","--------------------------------------------------------------------------------\n","    if not string:\n","        return string\n","    for i in range(len(string), -1, -1):\n","        if is_palindrome(string[:i]):\n","            return string + string[:i][::-1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/11\n","--------------------------------------------------------------------------------\n","    return ''.join([format(ord(a[i]) ^ ord(b[i]), '08b')[-1] for i in range(len(a))])\n","\n","================================================================================\n","\n"," TASK: HumanEval/12\n","--------------------------------------------------------------------------------\n","    if not strings:\n","        return None\n","    max_length = 0\n","    longest_string = None\n","    for string in strings:\n","        if len(string) > max_length:\n","            max_length = len(string)\n","            longest_string = string\n","    return longest_string\n","\n","================================================================================\n","\n"," TASK: HumanEval/13\n","--------------------------------------------------------------------------------\n","    while b:\n","        a, b = b, a % b\n","    return a\n","\n","================================================================================\n","\n"," TASK: HumanEval/14\n","--------------------------------------------------------------------------------\n","    prefixes = []\n","    for i in range(1, len(string) + 1):\n","        prefixes.append(string[:i])\n","    return prefixes\n","\n","================================================================================\n","\n"," TASK: HumanEval/15\n","--------------------------------------------------------------------------------\n","    result = \"\"\n","    for i in range(n + 1):\n","        result += str(i) + \" \"\n","    return result.rstrip()\n","\n","================================================================================\n","\n"," TASK: HumanEval/16\n","--------------------------------------------------------------------------------\n","    return len(set(string.lower()))\n","\n","================================================================================\n","\n"," TASK: HumanEval/17\n","--------------------------------------------------------------------------------\n","    note_to_beats = {\n","        'o': 4,\n","        'o|': 2,\n","        '.|': 1\n","    }\n","\n","    beats = []\n","    i = 0\n","    while i < len(music_string):\n","        if music_string[i] == 'o':\n","            beats.append(note_to_beats['o'])\n","            i += 1\n","        elif music_string[i:i+2] == 'o|':\n","            beats.append(note_to_beats['o|'])\n","            i += 2\n","        elif music_string[i:i+2] == '.|':\n","            beats.append(note_to_beats['.|'])\n","            i += 2\n","        else:\n","            i += 1\n","\n","    return beats\n","\n","================================================================================\n","\n"," TASK: HumanEval/18\n","--------------------------------------------------------------------------------\n","    count = 0\n","    for i in range(len(string) - len(substring) + 1):\n","        if string[i:i+len(substring)] == substring:\n","            count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/19\n","--------------------------------------------------------------------------------\n","    word_to_num = {\n","        'zero': 0,\n","        'one': 1,\n","        'two': 2,\n","        'three': 3,\n","        'four': 4,\n","        'five': 5,\n","        'six': 6,\n","        'seven': 7,\n","        'eight': 8,\n","        'nine': 9\n","    }\n","    num_to_word = {v: k for k, v in word_to_num.items()}\n","    words = numbers.split()\n","    nums = [word_to_num[word] for word in words]\n","    nums.sort()\n","    sorted_words = [num_to_word[num] for num in nums]\n","    return ' '.join(sorted_words)\n","\n","================================================================================\n","\n"," TASK: HumanEval/20\n","--------------------------------------------------------------------------------\n","    numbers.sort()\n","    min_diff = float('inf')\n","    closest_pair = (numbers[0], numbers[0])\n","    for i in range(1, len(numbers)):\n","        diff = numbers[i] - numbers[i-1]\n","        if diff < min_diff:\n","            min_diff = diff\n","            closest_pair = (numbers[i-1], numbers[i])\n","    return closest_pair\n","\n","================================================================================\n","\n"," TASK: HumanEval/21\n","--------------------------------------------------------------------------------\n","    min_val = min(numbers)\n","    max_val = max(numbers)\n","    range_val = max_val - min_val\n","    return [(x - min_val) / range_val for x in numbers]\n","\n","================================================================================\n","\n"," TASK: HumanEval/22\n","--------------------------------------------------------------------------------\n","    integers = []\n","    for value in values:\n","        if isinstance(value, int):\n","            integers.append(value)\n","    return integers\n","\n","================================================================================\n","\n"," TASK: HumanEval/23\n","--------------------------------------------------------------------------------\n","    return len(string)\n","\n","================================================================================\n","\n"," TASK: HumanEval/24\n","--------------------------------------------------------------------------------\n","    for i in range(n-1, 0, -1):\n","        if n % i == 0:\n","            return i\n","\n","================================================================================\n","\n"," TASK: HumanEval/25\n","--------------------------------------------------------------------------------\n","    factors = []\n","    divisor = 2\n","    while n > 1:\n","        while n % divisor == 0:\n","            factors.append(divisor)\n","            n = n // divisor\n","        divisor += 1\n","    return factors\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/26\n","--------------------------------------------------------------------------------\n","    result = []\n","    seen = set()\n","    for num in numbers:\n","        if num not in seen:\n","            result.append(num)\n","            seen.add(num)\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/27\n","--------------------------------------------------------------------------------\n","    result = \"\"\n","    for char in string:\n","        if char.islower():\n","            result += char.upper()\n","        elif char.isupper():\n","            result += char.lower()\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/28\n","--------------------------------------------------------------------------------\n","    return ''.join(strings)\n","\n","================================================================================\n","\n"," TASK: HumanEval/29\n","--------------------------------------------------------------------------------\n","    filtered_strings = [s for s in strings if s.startswith(prefix)]\n","    return filtered_strings\n","\n","================================================================================\n","\n"," TASK: HumanEval/30\n","--------------------------------------------------------------------------------\n","    positive_numbers = []\n","    for num in l:\n","        if num > 0:\n","            positive_numbers.append(num)\n","    return positive_numbers\n","\n","================================================================================\n","\n"," TASK: HumanEval/31\n","--------------------------------------------------------------------------------\n","    if n < 2:\n","        return False\n","    for i in range(2, int(n**0.5) + 1):\n","        if n % i == 0:\n","            return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/32\n","--------------------------------------------------------------------------------\n","    import math\n","\n","\n","    def poly(xs: list, x: float):\n","        return sum([coeff * math.pow(x, i) for i, coeff in enumerate(xs)])\n","\n","\n","        \"\"\" xs are coefficients of a polynomial.\n","        find_zero find x such that poly(x) = 0.\n","        find_zero returns only only zero point, even if there are many.\n","        Moreover, find_zero only takes list xs having even number of coefficients\n","        and largest non zero coefficient as it guarantees\n","        a solution.\n","        >>> round(find_zero([1, 2]), 2) # f(x) = 1 + 2x\n","        -0.5\n","        >>> round(find_zero([-6, 11, -6, 1]), 2) # (x - 1) * (x - 2) * (x - 3) = -6 + 11x - 6x^2 + x^3\n","        1.0\n","        \"\"\"\n","        if len(xs) % 2 == 0 and max(xs) > 0:\n","            for x in range(-100, 100):\n","                if round(poly(xs, x), 2) == 0:\n","                    return round(x, 2)\n","        return None\n","\n","================================================================================\n","\n"," TASK: HumanEval/33\n","--------------------------------------------------------------------------------\n","    # Extract elements at indices divisible by three\n","    third_elements = [l[i] for i in range(len(l)) if (i + 1) % 3 == 0]\n","\n","    # Sort the extracted elements\n","    third_elements_sorted = sorted(third_elements)\n","\n","    # Replace the elements at indices divisible by three with the sorted elements\n","    for i in range(len(third_elements_sorted)):\n","        l[i * 3 - 1] = third_elements_sorted[i]\n","\n","    return l\n","\n","================================================================================\n","\n"," TASK: HumanEval/34\n","--------------------------------------------------------------------------------\n","    return sorted(list(set(l)))\n","\n","================================================================================\n","\n"," TASK: HumanEval/35\n","--------------------------------------------------------------------------------\n","    if not l:\n","        return None\n","    max_val = l[0]\n","    for num in l[1:]:\n","        if num > max_val:\n","            max_val = num\n","    return max_val\n","\n","================================================================================\n","\n"," TASK: HumanEval/36\n","--------------------------------------------------------------------------------\n","    count = 0\n","    for i in range(1, n):\n","        if i % 11 == 0 or i % 13 == 0:\n","            count += str(i).count('7')\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/37\n","--------------------------------------------------------------------------------\n","    even_indices = [l[i] for i in range(0, len(l), 2)]\n","    even_indices.sort()\n","    l[::2] = even_indices\n","    return l\n","\n","================================================================================\n","\n"," TASK: HumanEval/38\n","--------------------------------------------------------------------------------\n","    def encode_cyclic(s: str):\n","        groups = [s[(3 * i):min((3 * i + 3), len(s))] for i in range((len(s) + 2) // 3)]\n","        groups = [(group[1:] + group[0]) if len(group) == 3 else group for group in groups]\n","        return \"\".join(groups)\n","\n","================================================================================\n","\n"," TASK: HumanEval/39\n","--------------------------------------------------------------------------------\n","    def is_prime(num: int) -> bool:\n","        if num < 2:\n","            return False\n","        for i in range(2, int(num ** 0.5) + 1):\n","            if num % i == 0:\n","                return False\n","        return True\n","\n","    fib_numbers = []\n","    a, b = 0, 1\n","    while len(fib_numbers) < n:\n","        a, b = b, a + b\n","        if is_prime(b):\n","            fib_numbers.append(b)\n","\n","    return fib_numbers[n - 1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/40\n","--------------------------------------------------------------------------------\n","    l.sort()\n","    n = len(l)\n","    for i in range(n - 2):\n","        if i > 0 and l[i] == l[i - 1]:\n","            continue\n","        left, right = i + 1, n - 1\n","        while left < right:\n","            total = l[i] + l[left] + l[right]\n","            if total == 0:\n","                return True\n","            elif total < 0:\n","                left += 1\n","            else:\n","                right -= 1\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/41\n","--------------------------------------------------------------------------------\n","    return n * n\n","\n","================================================================================\n","\n"," TASK: HumanEval/42\n","--------------------------------------------------------------------------------\n","    return [x + 1 for x in l]\n","\n","================================================================================\n","\n"," TASK: HumanEval/43\n","--------------------------------------------------------------------------------\n","    for i in range(len(l)):\n","        for j in range(i+1, len(l)):\n","            if l[i] + l[j] == 0:\n","                return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/44\n","--------------------------------------------------------------------------------\n","    result = \"\"\n","    while x > 0:\n","        result += str(x % base)\n","        x //= base\n","    return result[::-1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/45\n","--------------------------------------------------------------------------------\n","    return (a * h) / 2\n","\n","================================================================================\n","\n"," TASK: HumanEval/46\n","--------------------------------------------------------------------------------\n","    if n == 0 or n == 1:\n","        return 0\n","    elif n == 2:\n","        return 2\n","    elif n == 3:\n","        return 0\n","    else:\n","        a, b, c, d = 0, 0, 2, 0\n","        for _ in range(4, n + 1):\n","            a, b, c, d = b, c, d, a + b + c + d\n","        return d\n","\n","================================================================================\n","\n"," TASK: HumanEval/47\n","--------------------------------------------------------------------------------\n","    l.sort()\n","    n = len(l)\n","    if n % 2 == 1:\n","        return l[n // 2]\n","    else:\n","        return (l[n // 2 - 1] + l[n // 2]) / 2\n","\n","================================================================================\n","\n"," TASK: HumanEval/48\n","--------------------------------------------------------------------------------\n","    return text == text[::-1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/49\n","--------------------------------------------------------------------------------\n","    if n == 0:\n","        return 1\n","    if p == 1:\n","        return 0\n","    result = 1\n","    base = 2\n","    while n > 0:\n","        if n % 2 == 1:\n","            result = (result * base) % p\n","        base = (base * base) % p\n","        n = n // 2\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/50\n","--------------------------------------------------------------------------------\n","    def encode_shift(s: str):\n","        return \"\".join([chr(((ord(ch) + 5 - ord(\"a\")) % 26) + ord(\"a\")) for ch in s])\n","\n","================================================================================\n","\n"," TASK: HumanEval/51\n","--------------------------------------------------------------------------------\n","    return ''.join([char for char in text if char.lower() not in 'aeiou'])\n","\n","================================================================================\n","\n"," TASK: HumanEval/52\n","--------------------------------------------------------------------------------\n","    all_below = True\n","    for num in l:\n","        if num >= t:\n","            all_below = False\n","            break\n","    return all_below\n","\n","================================================================================\n","\n"," TASK: HumanEval/53\n","--------------------------------------------------------------------------------\n","    return x + y\n","\n","================================================================================\n","\n"," TASK: HumanEval/54\n","--------------------------------------------------------------------------------\n","    from collections import Counter\n","    return Counter(s0) == Counter(s1)\n","\n","================================================================================\n","\n"," TASK: HumanEval/55\n","--------------------------------------------------------------------------------\n","    a, b = 0, 1\n","    for _ in range(n):\n","        a, b = b, a + b\n","    return a\n","\n","================================================================================\n","\n"," TASK: HumanEval/56\n","--------------------------------------------------------------------------------\n","    balance = 0\n","    for char in brackets:\n","        if char == \"<\":\n","            balance += 1\n","        elif char == \">\":\n","            balance -= 1\n","        if balance < 0:\n","            return False\n","    return balance == 0\n","\n","================================================================================\n","\n"," TASK: HumanEval/57\n","--------------------------------------------------------------------------------\n","    if not l or len(l) == 1:\n","        return True\n","    increasing = True\n","    decreasing = True\n","    for i in range(len(l) - 1):\n","        if l[i] > l[i + 1]:\n","            increasing = False\n","        if l[i] < l[i + 1]:\n","            decreasing = False\n","    return increasing or decreasing\n","\n","================================================================================\n","\n"," TASK: HumanEval/58\n","--------------------------------------------------------------------------------\n","    return sorted(list(set(l1) & set(l2)))\n","\n","================================================================================\n","\n"," TASK: HumanEval/59\n","--------------------------------------------------------------------------------\n","    i = 2\n","    while i * i <= n:\n","        if n % i:\n","            i += 1\n","        else:\n","            n //= i\n","    return n\n","\n","================================================================================\n","\n"," TASK: HumanEval/60\n","--------------------------------------------------------------------------------\n","    return n * (n + 1) // 2\n","\n","================================================================================\n","\n"," TASK: HumanEval/61\n","--------------------------------------------------------------------------------\n","    balance = 0\n","    for char in brackets:\n","        if char == '(':\n","            balance += 1\n","        elif char == ')':\n","            balance -= 1\n","        if balance < 0:\n","            return False\n","    return balance == 0\n","\n","================================================================================\n","\n"," TASK: HumanEval/62\n","--------------------------------------------------------------------------------\n","    return [i * x for i, x in enumerate(xs)][1:]\n","\n","================================================================================\n","\n"," TASK: HumanEval/63\n","--------------------------------------------------------------------------------\n","    if n == 0 or n == 1:\n","        return 0\n","    elif n == 2:\n","        return 1\n","    else:\n","        dp = [0] * (n + 1)\n","        dp[0] = 0\n","        dp[1] = 0\n","        dp[2] = 1\n","        for i in range(3, n + 1):\n","            dp[i] = dp[i - 1] + dp[i - 2] + dp[i - 3]\n","        return dp[n]\n","\n","================================================================================\n","\n"," TASK: HumanEval/64\n","--------------------------------------------------------------------------------\n","    vowels = {'a', 'e', 'i', 'o', 'u'}\n","    count = sum(1 for char in s.lower() if char in vowels)\n","    if s.endswith('y'):\n","        count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/65\n","--------------------------------------------------------------------------------\n","    x_str = str(x)\n","    num_digits = len(x_str)\n","    if shift >= num_digits:\n","        return x_str[::-1]\n","    else:\n","        return x_str[-shift:] + x_str[:-shift]\n","\n","================================================================================\n","\n"," TASK: HumanEval/66\n","--------------------------------------------------------------------------------\n","    total = 0\n","    for char in s:\n","        if char.isupper():\n","            total += ord(char)\n","    return total\n","\n","================================================================================\n","\n"," TASK: HumanEval/67\n","--------------------------------------------------------------------------------\n","    # Extract the number of apples and oranges from the string\n","    num_apples = int(s.split(\"apples\")[0].split(\" \")[-1])\n","    num_oranges = int(s.split(\"oranges\")[0].split(\" \")[-1])\n","\n","    # Calculate the number of mangoes\n","    num_mangoes = n - num_apples - num_oranges\n","\n","    return num_mangoes\n","\n","================================================================================\n","\n"," TASK: HumanEval/68\n","--------------------------------------------------------------------------------\n","    smallest_even = float('inf')\n","    smallest_index = -1\n","    for i, num in enumerate(arr):\n","        if num % 2 == 0 and num < smallest_even:\n","            smallest_even = num\n","            smallest_index = i\n","    return [smallest_even, smallest_index] if smallest_even != float('inf') else []\n","\n","================================================================================\n","\n"," TASK: HumanEval/69\n","--------------------------------------------------------------------------------\n","    freq = {}\n","    for num in lst:\n","        if num in freq:\n","            freq[num] += 1\n","        else:\n","            freq[num] = 1\n","    for num in lst:\n","        if freq[num] >= num:\n","            return num\n","    return -1\n","\n","================================================================================\n","\n"," TASK: HumanEval/70\n","--------------------------------------------------------------------------------\n","    sorted_lst = []\n","    while lst:\n","        min_val = min(lst)\n","        sorted_lst.append(min_val)\n","        lst.remove(min_val)\n","        if lst:\n","            max_val = max(lst)\n","            sorted_lst.append(max_val)\n","            lst.remove(max_val)\n","    return sorted_lst\n","\n","================================================================================\n","\n"," TASK: HumanEval/71\n","--------------------------------------------------------------------------------\n","    if a + b > c and a + c > b and b + c > a:\n","        s = (a + b + c) / 2\n","        area = (s * (s - a) * (s - b) * (s - c)) ** 0.5\n","        return round(area, 2)\n","    else:\n","        return -1\n","\n","================================================================================\n","\n"," TASK: HumanEval/72\n","--------------------------------------------------------------------------------\n","    # Check if the list is balanced (palindromic)\n","    is_balanced = q == q[::-1]\n","\n","    # Calculate the sum of the elements\n","    total_weight = sum(q)\n","\n","    # Check if the sum is less than or equal to w\n","    can_fly = total_weight <= w\n","\n","    # Return True if both conditions are met\n","    return is_balanced and can_fly\n","\n","================================================================================\n","\n"," TASK: HumanEval/73\n","--------------------------------------------------------------------------------\n","    changes = 0\n","    for i in range(len(arr) // 2):\n","        if arr[i] != arr[-(i + 1)]:\n","            changes += 1\n","    return changes\n","\n","================================================================================\n","\n"," TASK: HumanEval/74\n","--------------------------------------------------------------------------------\n","    total1 = sum(len(s) for s in lst1)\n","    total2 = sum(len(s) for s in lst2)\n","    if total1 <= total2:\n","        return lst1\n","    else:\n","        return lst2\n","\n","================================================================================\n","\n"," TASK: HumanEval/75\n","--------------------------------------------------------------------------------\n","    if a >= 100:\n","        return False\n","\n","    def is_prime(n):\n","        if n < 2:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","    primes = [i for i in range(2, a + 1) if is_prime(i)]\n","\n","    for i in range(len(primes)):\n","        for j in range(i, len(primes)):\n","            for k in range(j, len(primes)):\n","                if primes[i] * primes[j] * primes[k] == a:\n","                    return True\n","    return False\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/76\n","--------------------------------------------------------------------------------\n","    while x > 1:\n","        if x % n != 0:\n","            return False\n","        x //= n\n","    return x == 1\n","\n","================================================================================\n","\n"," TASK: HumanEval/77\n","--------------------------------------------------------------------------------\n","    cube_root = round(a ** (1/3))\n","    return cube_root ** 3 == a\n","\n","================================================================================\n","\n"," TASK: HumanEval/78\n","--------------------------------------------------------------------------------\n","    prime_digits = {'2', '3', '5', '7', 'B', 'D'}\n","    count = 0\n","    for digit in num:\n","        if digit in prime_digits:\n","            count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/79\n","--------------------------------------------------------------------------------\n","    return f\"db{bin(decimal)[2:]}db\"\n","\n","================================================================================\n","\n"," TASK: HumanEval/80\n","--------------------------------------------------------------------------------\n","    if len(s) < 3:\n","        return False\n","    for i in range(len(s) - 2):\n","        if s[i] == s[i+1] or s[i] == s[i+2] or s[i+1] == s[i+2]:\n","            return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/81\n","--------------------------------------------------------------------------------\n","    letter_grades = []\n","    for gpa in grades:\n","        if gpa == 4.0:\n","            letter_grades.append('A+')\n","        elif gpa > 3.7:\n","            letter_grades.append('A')\n","        elif gpa > 3.3:\n","            letter_grades.append('A-')\n","        elif gpa > 3.0:\n","            letter_grades.append('B+')\n","        elif gpa > 2.7:\n","            letter_grades.append('B')\n","        elif gpa > 2.3:\n","            letter_grades.append('B-')\n","        elif gpa > 2.0:\n","            letter_grades.append('C+')\n","        elif gpa > 1.7:\n","            letter_grades.append('C')\n","        elif gpa > 1.3:\n","            letter_grades.append('C-')\n","        elif gpa > 1.0:\n","            letter_grades.append('D+')\n","        elif gpa > 0.7:\n","            letter_grades.append('D')\n","        elif gpa > 0.0:\n","            letter_grades.append('D-')\n","        else:\n","            letter_grades.append('E')\n","    return letter_grades\n","\n","================================================================================\n","\n"," TASK: HumanEval/82\n","--------------------------------------------------------------------------------\n","    length = len(string)\n","    if length < 2:\n","        return False\n","    for i in range(2, int(length ** 0.5) + 1):\n","        if length % i == 0:\n","            return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/83\n","--------------------------------------------------------------------------------\n","    if n == 1:\n","        return 1\n","    return 10**(n-1) + 9*10**(n-2) - 10**(n-2)\n","\n","================================================================================\n","\n"," TASK: HumanEval/84\n","--------------------------------------------------------------------------------\n","    return bin(sum(int(digit) for digit in str(N)))[2:]\n","\n","================================================================================\n","\n"," TASK: HumanEval/85\n","--------------------------------------------------------------------------------\n","    sum = 0\n","    for i in range(len(lst)):\n","        if i % 2 == 1 and lst[i] % 2 == 0:\n","            sum += lst[i]\n","    return sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/86\n","--------------------------------------------------------------------------------\n","    words = s.split(' ')\n","    sorted_words = [''.join(sorted(word)) for word in words]\n","    return ' '.join(sorted_words)\n","\n","================================================================================\n","\n"," TASK: HumanEval/87\n","--------------------------------------------------------------------------------\n","    coordinates = []\n","    for i, row in enumerate(lst):\n","        for j, value in enumerate(row):\n","            if value == x:\n","                coordinates.append((i, j))\n","    coordinates.sort(key=lambda x: (x[0], -x[1]))\n","    return coordinates\n","\n","================================================================================\n","\n"," TASK: HumanEval/88\n","--------------------------------------------------------------------------------\n","    if not array or len(array) == 1:\n","        return array\n","    first_last_sum = array[0] + array[-1]\n","    if first_last_sum % 2 == 1:\n","        return sorted(array)\n","    else:\n","        return sorted(array, reverse=True)\n","\n","================================================================================\n","\n"," TASK: HumanEval/89\n","--------------------------------------------------------------------------------\n","    result = []\n","    for char in s:\n","        if char.isalpha():\n","            shift = 2\n","            if char.islower():\n","                base = ord('a')\n","            else:\n","                base = ord('A')\n","            shifted_char = chr((ord(char) - base + shift) % 26 + base)\n","            result.append(shifted_char)\n","        else:\n","            result.append(char)\n","    return ''.join(result)\n","\n","================================================================================\n","\n"," TASK: HumanEval/90\n","--------------------------------------------------------------------------------\n","    if not lst:\n","        return None\n","    lst.sort()\n","    if len(lst) < 2:\n","        return None\n","    if len(lst) == 2:\n","        return lst[1]\n","    return lst[1]\n","\n","================================================================================\n","\n"," TASK: HumanEval/91\n","--------------------------------------------------------------------------------\n","    sentences = S.split('. ') + S.split('? ') + S.split('! ')\n","    boredom_count = 0\n","    for sentence in sentences:\n","        if sentence.startswith('I'):\n","            boredom_count += 1\n","    return boredom_count\n","\n","================================================================================\n","\n"," TASK: HumanEval/92\n","--------------------------------------------------------------------------------\n","    if isinstance(x, int) and isinstance(y, int) and isinstance(z, int):\n","        if x == y + z or y == x + z or z == x + y:\n","            return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/93\n","--------------------------------------------------------------------------------\n","    vowels = 'aeiou'\n","    encoded_message = ''\n","    for char in message:\n","        if char.isalpha():\n","            if char.lower() in vowels:\n","                char = chr((ord(char.lower()) - 97 + 2) % 26 + 97).upper() if char.islower() else chr((ord(char.lower()) - 97 + 2) % 26 + 97)\n","            else:\n","                char = char.swapcase()\n","        encoded_message += char\n","    return encoded_message\n","\n","================================================================================\n","\n"," TASK: HumanEval/94\n","--------------------------------------------------------------------------------\n","    def is_prime(n):\n","        if n < 2:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","    def sum_of_digits(n):\n","        return sum(int(digit) for digit in str(n))\n","\n","        largest_prime = max(filter(is_prime, lst))\n","        return sum_of_digits(largest_prime)\n","\n","================================================================================\n","\n"," TASK: HumanEval/95\n","--------------------------------------------------------------------------------\n","    if not dict:\n","        return False\n","    all_lower = True\n","    all_upper = True\n","    for key in dict.keys():\n","        if not isinstance(key, str):\n","            all_lower = False\n","            all_upper = False\n","            break\n","        if key.islower():\n","            all_lower = True\n","        else:\n","            all_upper = True\n","    return all_lower or all_upper\n","\n","================================================================================\n","\n"," TASK: HumanEval/96\n","--------------------------------------------------------------------------------\n","    if n <= 1:\n","        return []\n","\n","    is_prime = [True] * n\n","    is_prime[0] = is_prime[1] = False\n","\n","    for p in range(2, int(n**0.5) + 1):\n","        if is_prime[p]:\n","            for i in range(p*p, n, p):\n","                is_prime[i] = False\n","\n","    return [p for p in range(n) if is_prime[p]]\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/97\n","--------------------------------------------------------------------------------\n","    return (a % 10) * (b % 10)\n","\n","================================================================================\n","\n"," TASK: HumanEval/98\n","--------------------------------------------------------------------------------\n","    count = 0\n","    for i in range(0, len(s), 2):\n","        if s[i] in 'AEIOU':\n","            count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/99\n","--------------------------------------------------------------------------------\n","    num = float(value)\n","    if num - int(num) == 0.5:\n","        return int(num) + 1 if num > 0 else int(num) - 1\n","    else:\n","        return round(num)\n","\n","================================================================================\n","\n"," TASK: HumanEval/100\n","--------------------------------------------------------------------------------\n","    pile = [n]\n","    for i in range(1, n):\n","        if n % 2 == 0:\n","            next_stones = n + 2\n","        else:\n","            next_stones = n + 2\n","        pile.append(next_stones)\n","        n = next_stones\n","    return pile\n","\n","================================================================================\n","\n"," TASK: HumanEval/101\n","--------------------------------------------------------------------------------\n","    return s.split(', ')\n","\n","================================================================================\n","\n"," TASK: HumanEval/102\n","--------------------------------------------------------------------------------\n","    if x % 2 == 0 and x in range(x, y + 1):\n","        return x\n","    elif x % 2 == 1 and x + 1 in range(x, y + 1):\n","        return x + 1\n","    elif x % 2 == 1 and x + 2 in range(x, y + 1):\n","        return x + 2\n","    elif x % 2 == 1 and x + 3 in range(x, y + 1):\n","        return x + 3\n","    else:\n","        return -1\n","\n","================================================================================\n","\n"," TASK: HumanEval/103\n","--------------------------------------------------------------------------------\n","    if n > m:\n","        return -1\n","    avg = (n + m) / 2\n","    rounded_avg = round(avg)\n","    return bin(rounded_avg)[2:]\n","\n","================================================================================\n","\n"," TASK: HumanEval/104\n","--------------------------------------------------------------------------------\n","    result = []\n","    for num in x:\n","        if all(int(digit) % 2 != 0 for digit in str(num)):\n","            result.append(num)\n","    return sorted(result)\n","\n","================================================================================\n","\n"," TASK: HumanEval/105\n","--------------------------------------------------------------------------------\n","    # Filter the array to include only integers between 1 and 9 inclusive\n","    filtered_arr = [x for x in arr if 1 <= x <= 9]\n","\n","    # Sort the filtered array\n","    sorted_arr = sorted(filtered_arr)\n","\n","    # Reverse the sorted array\n","    reversed_arr = sorted_arr[::-1]\n","\n","    # Map each digit to its corresponding name\n","    digit_names = {\n","        1: \"One\",\n","        2: \"Two\",\n","        3: \"Three\",\n","        4: \"Four\",\n","        5: \"Five\",\n","        6: \"Six\",\n","        7: \"Seven\",\n","        8: \"Eight\",\n","        9: \"Nine\"\n","    }\n","\n","    # Return the resulting array\n","    return [digit_names[x] for x in reversed_arr]\n","\n","================================================================================\n","\n"," TASK: HumanEval/106\n","--------------------------------------------------------------------------------\n","    def factorial(i):\n","        if i == 0 or i == 1:\n","            return 1\n","        return i * factorial(i - 1)\n","\n","    def sum_to_i(i):\n","        return i * (i + 1) // 2\n","\n","    result = []\n","    for i in range(1, n + 1):\n","        if i % 2 == 0:\n","            result.append(factorial(i))\n","        else:\n","            result.append(sum_to_i(i))\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/107\n","--------------------------------------------------------------------------------\n","    def is_palindrome(x):\n","        return str(x) == str(x)[::-1]\n","\n","    even_count = 0\n","    odd_count = 0\n","\n","    for i in range(1, n + 1):\n","        if is_palindrome(i):\n","            if i % 2 == 0:\n","                even_count += 1\n","            else:\n","                odd_count += 1\n","\n","    return (even_count, odd_count)\n","\n","================================================================================\n","\n"," TASK: HumanEval/108\n","--------------------------------------------------------------------------------\n","    def sum_of_digits(n):\n","        return sum(int(digit) for digit in str(abs(n)))\n","\n","    count = 0\n","    for num in arr:\n","        if sum_of_digits(num) > 0:\n","            count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/109\n","--------------------------------------------------------------------------------\n","    if not arr:\n","        return True\n","    sorted_arr = sorted(arr)\n","    n = len(arr)\n","    for i in range(n):\n","        if arr == sorted_arr[i:] + sorted_arr[:i]:\n","            return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/110\n","--------------------------------------------------------------------------------\n","    odd_count_lst1 = sum(1 for x in lst1 if x % 2 != 0)\n","    even_count_lst2 = sum(1 for x in lst2 if x % 2 == 0)\n","\n","    if odd_count_lst1 <= even_count_lst2:\n","        return \"YES\"\n","    else:\n","        return \"NO\"\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/111\n","--------------------------------------------------------------------------------\n","    # Split the input string into individual characters\n","    chars = test.split()\n","\n","    # Count the occurrences of each character\n","    char_counts = {char: chars.count(char) for char in set(chars)}\n","\n","    # Find the maximum count\n","    max_count = max(char_counts.values())\n","\n","    # Filter characters that have the maximum count\n","    result = {char: count for char, count in char_counts.items() if count == max_count}\n","\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/112\n","--------------------------------------------------------------------------------\n","    result = ''.join([char for char in s if char not in c])\n","    return (result, result == result[::-1])\n","\n","================================================================================\n","\n"," TASK: HumanEval/113\n","--------------------------------------------------------------------------------\n","    result = []\n","    for s in lst:\n","        odd_count = sum(1 for char in s if int(char) % 2 != 0)\n","        result.append(f\"the number of odd elements {odd_count}n the str{odd_count}ng {odd_count} of the {odd_count}nput.\")\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/114\n","--------------------------------------------------------------------------------\n","    current_sum = min_sum = nums[0]\n","    for num in nums[1:]:\n","        current_sum = min(num, current_sum + num)\n","        min_sum = min(min_sum, current_sum)\n","    return min_sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/115\n","--------------------------------------------------------------------------------\n","    total_water = sum(sum(row) for row in grid)\n","    if total_water <= 0:\n","        return 0\n","    return math.ceil(total_water / capacity)\n","\n","\n","================================================================================\n","\n"," TASK: HumanEval/116\n","--------------------------------------------------------------------------------\n","    sorted_arr = []\n","    for num in arr:\n","        count = bin(num).count('1')\n","        sorted_arr.append((num, count))\n","    sorted_arr.sort(key=lambda x: (x[1], x[0]))\n","    return [x[0] for x in sorted_arr]\n","\n","================================================================================\n","\n"," TASK: HumanEval/117\n","--------------------------------------------------------------------------------\n","    vowels = \"aeiou\"\n","    result = []\n","    for word in s.split():\n","        consonant_count = sum(1 for char in word if char.isalpha() and char.lower() not in vowels)\n","        if consonant_count == n:\n","            result.append(word)\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/118\n","--------------------------------------------------------------------------------\n","    vowels = \"aeiouAEIOU\"\n","    for i in range(len(word) - 2, 0, -1):\n","        if word[i] in vowels and word[i-1] not in vowels and word[i-2] not in vowels:\n","            return word[i]\n","    return \"\"\n","\n","================================================================================\n","\n"," TASK: HumanEval/119\n","--------------------------------------------------------------------------------\n","    if lst[0].count('(') > lst[1].count('('):\n","        return 'No'\n","    if lst[1].count(')') > lst[0].count(')'):\n","        return 'No'\n","    return 'Yes'\n","\n","================================================================================\n","\n"," TASK: HumanEval/120\n","--------------------------------------------------------------------------------\n","    import heapq\n","\n","    return sorted(heapq.nlargest(k, arr))\n","\n","================================================================================\n","\n"," TASK: HumanEval/121\n","--------------------------------------------------------------------------------\n","    sum_odd = 0\n","    for index, value in enumerate(lst):\n","        if index % 2 == 0 and value % 2 != 0:\n","            sum_odd += value\n","    return sum_odd\n","\n","================================================================================\n","\n"," TASK: HumanEval/122\n","--------------------------------------------------------------------------------\n","    sum = 0\n","    for i in range(k):\n","        if arr[i] < 100:\n","            sum += arr[i]\n","    return sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/123\n","--------------------------------------------------------------------------------\n","    collatz_sequence = [n]\n","    while n != 1:\n","        if n % 2 == 0:\n","            n = n // 2\n","        else:\n","            n = 3 * n + 1\n","        collatz_sequence.append(n)\n","    odd_numbers = [num for num in collatz_sequence if num % 2 != 0]\n","    return sorted(odd_numbers)\n","\n","================================================================================\n","\n"," TASK: HumanEval/124\n","--------------------------------------------------------------------------------\n","    if not date:\n","        return False\n","    if len(date) != 10 or date[2] != '-' or date[5] != '-':\n","        return False\n","    month = int(date[:2])\n","    day = int(date[3:5])\n","    year = int(date[6:])\n","    if month < 1 or month > 12:\n","        return False\n","    if month in [1, 3, 5, 7, 8, 10, 12]:\n","        if day < 1 or day > 31:\n","            return False\n","    elif month in [4, 6, 9, 11]:\n","        if day < 1 or day > 30:\n","            return False\n","    elif month == 2:\n","        if day < 1 or day > 29:\n","            return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/125\n","--------------------------------------------------------------------------------\n","    if ' ' in txt:\n","        return txt.split(' ')\n","    elif ',' in txt:\n","        return txt.split(',')\n","    else:\n","        return sum(1 for char in txt if char.islower() and (ord(char) - ord('a')) % 2 == 1)\n","\n","================================================================================\n","\n"," TASK: HumanEval/126\n","--------------------------------------------------------------------------------\n","    is_sorted = True\n","    for i in range(1, len(lst)):\n","        if lst[i] < lst[i-1]:\n","            is_sorted = False\n","            break\n","    if lst.count(lst[0]) > 1:\n","        is_sorted = False\n","    return is_sorted\n","\n","================================================================================\n","\n"," TASK: HumanEval/127\n","--------------------------------------------------------------------------------\n","    def is_prime(n):\n","        if n < 2:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","        start1, end1 = interval1\n","        start2, end2 = interval2\n","        start = max(start1, start2)\n","        end = min(end1, end2)\n","        if start > end:\n","            return \"NO\"\n","        length = end - start + 1\n","        return \"YES\" if is_prime(length) else \"NO\"\n","\n","================================================================================\n","\n"," TASK: HumanEval/128\n","--------------------------------------------------------------------------------\n","    if not arr:\n","        return None\n","    sign_product = 1\n","    magnitude_sum = 0\n","    for num in arr:\n","        magnitude_sum += abs(num)\n","        sign_product *= 1 if num > 0 else -1 if num < 0 else 0\n","    return sign_product * magnitude_sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/129\n","--------------------------------------------------------------------------------\n","    n = len(grid)\n","    dp = [[[float('inf')] * 4 for _ in range(n)] for _ in range(n)]\n","    for i in range(n):\n","        for j in range(n):\n","            dp[i][j][0] = grid[i][j]\n","    for _ in range(k - 1):\n","        new_dp = [[[float('inf')] * 4 for _ in range(n)] for _ in range(n)]\n","        for i in range(n):\n","            for j in range(n):\n","                for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n","                    x, y = i + dx, j + dy\n","                    if 0 <= x < n and 0 <= y < n:\n","                        for d in range(4):\n","                            new_dp[x][y][d] = min(new_dp[x][y][d], dp[i][j][d] + grid[x][y])\n","    min_val = float('inf')\n","    min_path = []\n","    for i in range(n):\n","        for j in range(n):\n","            for d in range(4):\n","                if dp[i][j][d] < min_val:\n","                    min_val = dp[i][j][d]\n","                    min_path = [grid[i][j]]\n","                    x, y = i, j\n","                    for _ in range(k - 1):\n","                        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n","                            nx, ny = x + dx, y + dy\n","                            if 0 <= nx < n and 0 <= ny < n and dp[nx][ny][d] == dp[x][y][d] + grid[nx][ny]:\n","                                x, y = nx, ny\n","                                min_path.append(grid[x][y])\n","    return min_path\n","\n","================================================================================\n","\n"," TASK: HumanEval/130\n","--------------------------------------------------------------------------------\n","    tribonacci = [1, 3, 2]\n","    for i in range(3, n + 1):\n","        if i % 2 == 0:\n","            tribonacci.append(1 + i / 2)\n","        else:\n","            tribonacci.append(tribonacci[i - 1] + tribonacci[i - 2] + tribonacci[i + 1])\n","    return tribonacci\n","\n","================================================================================\n","\n"," TASK: HumanEval/131\n","--------------------------------------------------------------------------------\n","    product = 1\n","    for digit in str(n):\n","        if int(digit) % 2 != 0:\n","            product *= int(digit)\n","    return product if product != 1 else 0\n","\n","================================================================================\n","\n"," TASK: HumanEval/132\n","--------------------------------------------------------------------------------\n","    depth = 0\n","    for char in string:\n","        if char == '[':\n","            depth += 1\n","        elif char == ']':\n","            depth -= 1\n","        if depth > 1:\n","            return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/133\n","--------------------------------------------------------------------------------\n","    total = 0\n","    for num in lst:\n","        total += round(num) ** 2\n","    return total\n","\n","================================================================================\n","\n"," TASK: HumanEval/134\n","--------------------------------------------------------------------------------\n","    if not txt:\n","        return False\n","    last_char = txt[-1]\n","    if last_char.isalpha() and (len(txt) == 1 or not txt[-2].isalpha()):\n","        return True\n","    return False\n","\n","================================================================================\n","\n"," TASK: HumanEval/135\n","--------------------------------------------------------------------------------\n","    for i in range(1, len(arr)):\n","        if arr[i] >= arr[i - 1]:\n","            return i\n","    return -1\n","\n","================================================================================\n","\n"," TASK: HumanEval/136\n","--------------------------------------------------------------------------------\n","    largest_negative = None\n","    smallest_positive = None\n","    for num in lst:\n","        if num < 0:\n","            if largest_negative is None or num > largest_negative:\n","                largest_negative = num\n","        elif num > 0:\n","            if smallest_positive is None or num < smallest_positive:\n","                smallest_positive = num\n","    return (largest_negative, smallest_positive)\n","\n","================================================================================\n","\n"," TASK: HumanEval/137\n","--------------------------------------------------------------------------------\n","    try:\n","        a_float = float(a)\n","        b_float = float(b)\n","        if a_float > b_float:\n","            return a\n","        elif b_float > a_float:\n","            return b\n","        else:\n","            return None\n","    except ValueError:\n","        a_str = str(a)\n","        b_str = str(b)\n","        if a_str > b_str:\n","            return a\n","        elif b_str > a_str:\n","            return b\n","        else:\n","            return None\n","\n","================================================================================\n","\n"," TASK: HumanEval/138\n","--------------------------------------------------------------------------------\n","    if n % 2 != 0:\n","        return False\n","    if n < 8:\n","        return False\n","    return True\n","\n","================================================================================\n","\n"," TASK: HumanEval/139\n","--------------------------------------------------------------------------------\n","    def factorial(x):\n","        if x == 0 or x == 1:\n","            return 1\n","        else:\n","            return x * factorial(x - 1)\n","\n","        result = 1\n","        for i in range(1, n + 1):\n","            result *= factorial(i)\n","        return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/140\n","--------------------------------------------------------------------------------\n","    result = \"\"\n","    previous_char_was_space = False\n","    for char in text:\n","        if char == \" \":\n","            if previous_char_was_space:\n","                result += \"_\"\n","            else:\n","                result += \" \"\n","            previous_char_was_space = True\n","        else:\n","            result += char\n","            previous_char_was_space = False\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/141\n","--------------------------------------------------------------------------------\n","    # Check if the file name contains exactly one dot '.'\n","    if file_name.count('.') != 1:\n","        return 'No'\n","\n","    # Split the file name into prefix and suffix\n","    prefix, suffix = file_name.split('.')\n","\n","    # Check if the prefix starts with a latin alphapet letter\n","    if not prefix[0].isalpha():\n","        return 'No'\n","\n","    # Check if the suffix is one of ['txt', 'exe', 'dll']\n","    if suffix not in ['txt', 'exe', 'dll']:\n","        return 'No'\n","\n","    # Count the number of digits in the prefix\n","    digit_count = sum(c.isdigit() for c in prefix)\n","\n","    # Check if there are more than three digits in the prefix\n","    if digit_count > 3:\n","        return 'No'\n","\n","    # Return 'Yes' if all conditions are met\n","    return 'Yes'\n","\n","================================================================================\n","\n"," TASK: HumanEval/142\n","--------------------------------------------------------------------------------\n","    total = 0\n","    for i, num in enumerate(lst):\n","        if i % 3 == 0:\n","            total += num ** 2\n","        elif i % 4 == 0:\n","            total += num ** 3\n","        else:\n","            total += num\n","    return total\n","\n","================================================================================\n","\n"," TASK: HumanEval/143\n","--------------------------------------------------------------------------------\n","    def is_prime(n):\n","        if n < 2:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","        words = sentence.split()\n","        prime_length_words = [word for word in words if is_prime(len(word))]\n","        return ' '.join(prime_length_words)\n","\n","================================================================================\n","\n"," TASK: HumanEval/144\n","--------------------------------------------------------------------------------\n","    x_num, x_den = map(int, x.split('/'))\n","    n_num, n_den = map(int, n.split('/'))\n","    return (x_num * n_num) % (x_den * n_den) == 0\n","\n","================================================================================\n","\n"," TASK: HumanEval/145\n","--------------------------------------------------------------------------------\n","    def sum_of_digits(n):\n","        return sum(int(digit) for digit in str(abs(n)))\n","\n","    return sorted(nums, key=lambda x: (sum_of_digits(x), nums.index(x)))\n","\n","================================================================================\n","\n"," TASK: HumanEval/146\n","--------------------------------------------------------------------------------\n","    count = 0\n","    for num in nums:\n","        if num > 10:\n","            first_digit = int(str(num)[0])\n","            last_digit = int(str(num)[-1])\n","            if first_digit % 2 != 0 and last_digit % 2 != 0:\n","                count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/147\n","--------------------------------------------------------------------------------\n","    a = [i * i - i + 1 for i in range(1, n + 1)]\n","    count = 0\n","    for i in range(n):\n","        for j in range(i + 1, n):\n","            for k in range(j + 1, n):\n","                if (a[i] + a[j] + a[k]) % 3 == 0:\n","                    count += 1\n","    return count\n","\n","================================================================================\n","\n"," TASK: HumanEval/148\n","--------------------------------------------------------------------------------\n","    planets = [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"]\n","    if planet1 not in planets or planet2 not in planets:\n","        return ()\n","    index1 = planets.index(planet1)\n","    index2 = planets.index(planet2)\n","    if index1 > index2:\n","        index1, index2 = index2, index1\n","    return tuple(planets[index1 + 1:index2])\n","\n","================================================================================\n","\n"," TASK: HumanEval/149\n","--------------------------------------------------------------------------------\n","    # Filter out strings with odd lengths\n","    even_length_strings = [s for s in lst if len(s) % 2 == 0]\n","\n","    # Sort the list first by length, then alphabetically\n","    even_length_strings.sort(key=lambda x: (len(x), x))\n","\n","    return even_length_strings\n","\n","================================================================================\n","\n"," TASK: HumanEval/150\n","--------------------------------------------------------------------------------\n","    def is_prime(n):\n","        if n <= 1:\n","            return False\n","        for i in range(2, int(n**0.5) + 1):\n","            if n % i == 0:\n","                return False\n","        return True\n","\n","        return x if is_prime(n) else y\n","\n","================================================================================\n","\n"," TASK: HumanEval/151\n","--------------------------------------------------------------------------------\n","    sum = 0\n","    for num in lst:\n","        if isinstance(num, int) and num >= 0:\n","            if num % 2 != 0:\n","                sum += num ** 2\n","    return sum\n","\n","================================================================================\n","\n"," TASK: HumanEval/152\n","--------------------------------------------------------------------------------\n","    differences = []\n","    for score, g in zip(game, guess):\n","        differences.append(abs(score - g))\n","    return differences\n","\n","================================================================================\n","\n"," TASK: HumanEval/153\n","--------------------------------------------------------------------------------\n","    max_strength = float('-inf')\n","    strongest_extension = ''\n","    for extension in extensions:\n","        cap = sum(1 for char in extension if char.isupper())\n","        sm = sum(1 for char in extension if char.islower())\n","        strength = cap - sm\n","        if strength > max_strength:\n","            max_strength = strength\n","            strongest_extension = extension\n","    return f\"{class_name}.{strongest_extension}\"\n","\n","================================================================================\n","\n"," TASK: HumanEval/154\n","--------------------------------------------------------------------------------\n","    # Concatenate the first word with itself\n","    concatenated_a = a + a\n","    # Check if the second word is a substring of the concatenated string\n","    return b in concatenated_a\n","\n","================================================================================\n","\n"," TASK: HumanEval/155\n","--------------------------------------------------------------------------------\n","    num_str = str(abs(num))\n","    even_count = 0\n","    odd_count = 0\n","    for char in num_str:\n","        if char.isdigit():\n","            digit = int(char)\n","            if digit % 2 == 0:\n","                even_count += 1\n","            else:\n","                odd_count += 1\n","    return (even_count, odd_count)\n","\n","================================================================================\n","\n"," TASK: HumanEval/156\n","--------------------------------------------------------------------------------\n","    roman_numerals = {\n","        1000: 'm',\n","        900: 'cm',\n","        500: 'd',\n","        400: 'cd',\n","        100: 'c',\n","        90: 'xc',\n","        50: 'l',\n","        40: 'xl',\n","        10: 'x',\n","        9: 'ix',\n","        5: 'v',\n","        4: 'iv',\n","        1: 'i'\n","    }\n","    result = ''\n","    for value, symbol in sorted(roman_numerals.items(), reverse=True):\n","        while number >= value:\n","            result += symbol\n","            number -= value\n","    return result.lower()\n","\n","================================================================================\n","\n"," TASK: HumanEval/157\n","--------------------------------------------------------------------------------\n","    sides = [a, b, c]\n","    sides.sort()\n","    return sides[0]**2 + sides[1]**2 == sides[2]**2\n","\n","================================================================================\n","\n"," TASK: HumanEval/158\n","--------------------------------------------------------------------------------\n","    max_unique = 0\n","    max_word = \"\"\n","    for word in words:\n","        unique_chars = len(set(word))\n","        if unique_chars > max_unique:\n","            max_unique = unique_chars\n","            max_word = word\n","        elif unique_chars == max_unique:\n","            if word < max_word:\n","                max_word = word\n","    return max_word\n","\n","================================================================================\n","\n"," TASK: HumanEval/159\n","--------------------------------------------------------------------------------\n","    total_eaten = number\n","    remaining_carrots = remaining\n","    if remaining_carrots < need:\n","        total_eaten += remaining_carrots\n","        remaining_carrots = 0\n","    else:\n","        total_eaten += need\n","        remaining_carrots -= need\n","    return [total_eaten, remaining_carrots]\n","\n","================================================================================\n","\n"," TASK: HumanEval/160\n","--------------------------------------------------------------------------------\n","    result = operand[0]\n","    for op, num in zip(operator, operand[1:]):\n","        if op == '+':\n","            result += num\n","        elif op == '-':\n","            result -= num\n","        elif op == '*':\n","            result *= num\n","        elif op == '//':\n","            result //= num\n","        elif op == '**':\n","            result **= num\n","    return result\n","\n","================================================================================\n","\n"," TASK: HumanEval/161\n","--------------------------------------------------------------------------------\n","    if not any(c.isalpha() for c in s):\n","        return s[::-1]\n","    return ''.join(c.lower() if c.isupper() else c.upper() for c in s)\n","\n","================================================================================\n","\n"," TASK: HumanEval/162\n","--------------------------------------------------------------------------------\n","    import hashlib\n","\n","    if not text:\n","        return None\n","    return hashlib.md5(text.encode()).hexdigest()\n","\n","================================================================================\n","\n"," TASK: HumanEval/163\n","--------------------------------------------------------------------------------\n","    result = []\n","    for num in range(a, b + 1):\n","        if num % 2 == 0:\n","            result.append(num)\n","    return result\n","\n","================================================================================\n","\n"]}]}]}