{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlKrZ4W3YkZm"
   },
   "source": [
    "# Notebook: GRPO Reinforcement Learning (Run 2 â€“ Stabilized Partial-Credit Training)\n",
    "\n",
    "This notebook performs the **second GRPO reinforcement learning run** on top of the **SFT warm-start model**: `models/qwen3-4b-sft`.\n",
    "\n",
    "The focus of this run is **controlled performance improvement** on MBPP-style coding tasks while explicitly preventing reward hacking, verbosity collapse, and masking failures observed in the initial GRPO attempt.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this stage is to **improve pass@1 correctness** beyond the SFT baseline by applying **GRPO with dense, verifiable rewards**, while maintaining:\n",
    "\n",
    "- Concise, efficient reasoning traces  \n",
    "- Stable output formatting  \n",
    "- Controlled policy drift (no degeneration into rambling or schema exploitation)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Changes from the First GRPO Run\n",
    "\n",
    "### 1. Dense Correctness Reward (Partial Credit)\n",
    "- Replaced binary pass/fail correctness with **fractional credit**:\n",
    "  - Reward = `passed_tests / total_tests`\n",
    "- Added a small **victory bonus** for passing all tests to preserve a global optimum.\n",
    "- Prevents flat reward landscapes where near-miss solutions receive no gradient.\n",
    "\n",
    "### 2. Anti-Filibuster Reasoning Reward\n",
    "- Replaced linear length-based reward with a **capped + penalized profile**:\n",
    "  - Linear ascent up to 400 characters\n",
    "  - Flat reward plateau from 400â€“800 characters\n",
    "  - Aggressive negative slope beyond 800 characters\n",
    "- Explicitly disincentivizes infinite rambling while still encouraging meaningful reasoning.\n",
    "\n",
    "### 3. Format Reward Demotion\n",
    "- Reduced format reward to a **small hygiene incentive** (0.02 max).\n",
    "- Ensures schema compliance without allowing formatting to dominate learning.\n",
    "\n",
    "### 4. Stop Condition Correction\n",
    "- Reverted to `stop = [tokenizer.eos_token]`.\n",
    "- Removed string-based stop conditions that caused masking failures, padding leakage, and clipped-ratio explosions.\n",
    "- Restores correct termination detection and loss masking.\n",
    "\n",
    "### 5. KL Term Removal (Default GRPO Behavior)\n",
    "- Explicitly **did not use a KL penalty** (`beta` omitted / default 0.0).\n",
    "- Aligns with modern GRPO practice and avoids unnecessary instability.\n",
    "- KL metrics are monitored diagnostically only.\n",
    "\n",
    "### 6. Stability-Oriented Training Setup\n",
    "- Higher exploration via `num_generations = 16`\n",
    "- Gradient accumulation for variance reduction\n",
    "\n",
    "- Two-epoch training with early-stop awareness\n",
    "- Careful learning-rate selection for short-horizon RL\n",
    "\n",
    "---\n",
    "\n",
    "## Training Procedure Summary\n",
    "\n",
    "- Load the **SFT warm-start model**: `models/qwen3-4b-sft`\n",
    "- Apply ChatML-style prompt formatting\n",
    "- Generate multiple rollouts per prompt via vLLM\n",
    "- Compute rewards using:\n",
    "  - Schema validation\n",
    "  - Reasoning length shaping\n",
    "  - Partial-credit unit test execution\n",
    "- Optimize policy using GRPO with clipped ratios\n",
    "- Monitor reward decomposition, KL drift, and generation length\n",
    "- Save checkpoints for post-hoc evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L637dg8aVl1t"
   },
   "source": [
    "# Step 1: Mounting Google Drive and Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26318,
     "status": "ok",
     "timestamp": 1767579100219,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Qzxre_ImVbHp",
    "outputId": "2165ab9e-2313-4c9f-959c-f0580cd30887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/grpo-verified-reasoner\n",
      "data\t\t\t      LICENSE\t outputs    unsloth_compiled_cache\n",
      "grpo_trainer_lora_model       models\t README.md  _unsloth_sentencepiece_temp\n",
      "huggingface_tokenizers_cache  notebooks  src\t    wandb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd /content/drive/MyDrive/grpo-verified-reasoner\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3IoN0USiBmR"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Install UV (Faster pip)\n",
    "!pip install --upgrade -qqq uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8iXM_15a_mn"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install -U evalplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1767579109531,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Oq8Azhmyiu6E"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579109534,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "lv-Ki6Vcrntd"
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767579109539,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "NeqIAhUsd5Dg"
   },
   "outputs": [],
   "source": [
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1767579109562,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "kTXAQ4pB0NsE"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"mbpp-rl-project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34344,
     "status": "ok",
     "timestamp": 1767579143909,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "ZuucNkH5imSg",
    "outputId": "da4fc281-51f3-4674-e3cb-fa892a65fe52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m18 packages\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 296ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 188ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 36ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 24ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.24.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.22.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Environment Logic (Colab vs Local)\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # Version Matching\n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    try: is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "    except: is_t4 = False\n",
    "\n",
    "    # A100 gets vllm 0.10.2 (Fast), T4 gets 0.9.2 (Stable)\n",
    "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
    "\n",
    "    # Install Everything\n",
    "    !uv pip install -qqq --upgrade \\\n",
    "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
    "    !uv pip install -qqq {get_triton}\n",
    "\n",
    "# Install TRL\n",
    "!uv pip install transformers==4.56.2\n",
    "!uv pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100069,
     "status": "ok",
     "timestamp": 1767579243990,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "HPDizgKUc3Fx",
    "outputId": "33bdce6b-862b-4f0e-cfab-64f5e90c8114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-05 02:12:48 [__init__.py:216] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import evalplus\n",
    "import traceback\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from evalplus.data import get_mbpp_plus\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from vllm import SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiKxGqbQx_r6"
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bFALRzXVpfD"
   },
   "source": [
    "# Step 2: Verifying GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1767579263181,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "j6gsvZOiavZf",
    "outputId": "c3c3a845-f152-4847-c2dc-0fed592e3e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-PeyUR5dE0m"
   },
   "source": [
    "# Step 3: Loading Base Model and LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767579264383,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "OGf88PMBdN3B"
   },
   "outputs": [],
   "source": [
    "\n",
    "MODEL_PATH = \"models/qwen3-4b-sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLoe3PBmsHXz"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_PATH,\n",
    "    max_seq_length = 3072,      # Aligned with GRPO + schema\n",
    "    load_in_4bit = False,       # Full precision for RL stability\n",
    "    fast_inference = True,      # Required for vLLM\n",
    "    gpu_memory_utilization = 0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1767579432578,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "o4dnSm0bIHz5",
    "outputId": "b295c6f6-5d48-474b-e0a5-6d0b4e47acde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 66,060,288 / 4,088,528,384 = 1.6157%\n",
      "Example trainable params: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight']\n"
     ]
    }
   ],
   "source": [
    "trainable = 0\n",
    "total = 0\n",
    "trainable_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    n = p.numel()\n",
    "    total += n\n",
    "    if p.requires_grad:\n",
    "        trainable += n\n",
    "        trainable_names.append(name)\n",
    "\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} = {100*trainable/total:.4f}%\")\n",
    "print(\"Example trainable params:\", trainable_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1767579434324,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "3wgYFoDcv283",
    "outputId": "7d609867-aa61-4be9-c664-f1ec98399c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen3ForCausalLM(\n",
      "      (model): Qwen3Model(\n",
      "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
      "        (layers): ModuleList(\n",
      "          (0-35): 36 x Qwen3DecoderLayer(\n",
      "            (self_attn): Qwen3Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen3MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AtswtDWt3T4"
   },
   "source": [
    "# Step 4: Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767579434807,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "pt0hmcEetnJ7"
   },
   "outputs": [],
   "source": [
    "# This is the same prompt that we used during SFT\n",
    "system_prompt = \"\"\"You are a code-generation engine.\n",
    "You must output your response in the following exact format:\n",
    "<START_WORKING_OUT>\n",
    "Concise reasoning steps required to solve the problem.\n",
    "</END_WORKING_OUT>\n",
    "<SOLUTION>\n",
    "Valid Python code only.\n",
    "</SOLUTION>\n",
    "Do not output anything outside these tags.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579434952,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "FnsS5mubtnGM"
   },
   "outputs": [],
   "source": [
    "user_prompt = \"Write a Python function that returns the factorial of a number.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579435204,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "9buanb82vgtZ"
   },
   "outputs": [],
   "source": [
    "# Move the dictionary to GPU manually\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4594,
     "status": "ok",
     "timestamp": 1767579439992,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "m1gvTQsdtnEa"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Temporarily enable inference mode for the test\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.0, # Deterministic check\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1767579440005,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "xglp4DortnCS"
   },
   "outputs": [],
   "source": [
    "\n",
    "decoded = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767579440012,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zxb_myK-tnAO",
    "outputId": "e77d9b09-4c46-4cb2-d997-648d19bbde35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MODEL OUTPUT ---\n",
      "<START_WORKING_OUT>\n",
      "Define a function factorial(n) that calculates the product of all positive integers up to n.\n",
      "Handle non-positive integers by returning 1 (factorial of 0 and negative is 1).\n",
      "Implement iterative approach for efficiency.\n",
      "Return the computed factorial.\n",
      "</END_WORKING_OUT>\n",
      "<SOLUTION>\n",
      "def factorial(n):\n",
      "    if n <= 0:\n",
      "        return 1\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    return result\n",
      "</SOLUTION>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- MODEL OUTPUT ---\")\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "print(tokenizer.decode(output[0][input_len:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8MkD65mYBx5"
   },
   "source": [
    "Comment:  No schema check, extractor, or reward function ever sees the full decoded sequence. They only ever see generated_text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_hw6md1LSm3"
   },
   "source": [
    "\n",
    "# Step 6: Defining Output Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579440014,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "O3zR1mkFtm-N"
   },
   "outputs": [],
   "source": [
    "# Regular expressions for tag validation (case-insensitive)\n",
    "RE_START = re.compile(r\"<START_WORKING_OUT>\", re.IGNORECASE)\n",
    "RE_END   = re.compile(r\"</END_WORKING_OUT>\", re.IGNORECASE)\n",
    "RE_SOL   = re.compile(r\"<SOLUTION>\", re.IGNORECASE)\n",
    "RE_SOL_END = re.compile(r\"</SOLUTION>\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767579440020,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "5oxttpUDtm0G"
   },
   "outputs": [],
   "source": [
    "def validate_schema(text: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Checks whether the model output follows the exact required schema.\n",
    "    Returns (is_valid, reason).\n",
    "    \"\"\"\n",
    "    # Ensure reasoning START tag exists\n",
    "    if not RE_START.search(text):\n",
    "        return False, \"Missing <START_WORKING_OUT>\"\n",
    "    # Ensure reasoning END tag exists\n",
    "    if not RE_END.search(text):\n",
    "        return False, \"Missing </END_WORKING_OUT>\"\n",
    "    # Ensure solution START tag exists\n",
    "    if not RE_SOL.search(text):\n",
    "        return False, \"Missing <SOLUTION>\"\n",
    "    # Ensure solution END tag exists\n",
    "    if not RE_SOL_END.search(text):\n",
    "        return False, \"Missing </SOLUTION>\"\n",
    "\n",
    "    # Optional: check order consistency\n",
    "    # Make sure the reasoning block appears before the solution block\n",
    "    start_idx = RE_START.search(text).start()\n",
    "    sol_idx   = RE_SOL.search(text).start()\n",
    "    if sol_idx < start_idx:\n",
    "        return False, \"Tag order incorrect (<SOLUTION> before reasoning block).\"\n",
    "\n",
    "    # All validations passed\n",
    "    return True, \"Schema valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1767579440064,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "YA_UNFQgLrF0",
    "outputId": "41172f46-60a8-48cc-de29-354cc8d8ecb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Check: True | Schema valid\n"
     ]
    }
   ],
   "source": [
    "# Run a sanity test using the previous decoded output\n",
    "is_valid, reason = validate_schema(decoded)\n",
    "print(\"Schema Check:\", is_valid, \"|\", reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EN-nTksMUff"
   },
   "source": [
    "# Step 7: Solution Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767579440068,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Iv-5aUVNL3aC"
   },
   "outputs": [],
   "source": [
    "# Regex to extract the code block between <SOLUTION> ... </SOLUTION>\n",
    "RE_SOLUTION = re.compile(r\"<SOLUTION>\\s*(.*?)\\s*</SOLUTION>\", re.IGNORECASE | re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1767579440069,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "nlcNKSrIMjBy"
   },
   "outputs": [],
   "source": [
    "def extract_solution(text: str) -> tuple[str | None, str]:\n",
    "    \"\"\"\n",
    "    Extract the Python snippet enclosed by <SOLUTION>...</SOLUTION>.\n",
    "    Returns (code or None, status message).\n",
    "    \"\"\"\n",
    "    # Find the solution block using the compiled regex (handles multiline, case-insensitive)\n",
    "    match = RE_SOLUTION.search(text)\n",
    "    if not match:\n",
    "        return None, \"No <SOLUTION> block found.\"\n",
    "\n",
    "    # Trim surrounding whitespace from the captured code block\n",
    "    code = match.group(1).strip()\n",
    "    if not code:\n",
    "        return None, \"Empty <SOLUTION> block.\"\n",
    "\n",
    "    # Quick syntax sanity check: ensure the extracted text is valid Python\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        return None, f\"Syntax error in code: {e}\"\n",
    "    return code, \"Valid Python code extracted.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1767579440070,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "hgaK_ceNV4Le"
   },
   "outputs": [],
   "source": [
    "# Calculate where the prompt ends\n",
    "input_len = inputs[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1767579440071,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "8hwepazlV-Ho"
   },
   "outputs": [],
   "source": [
    "# Decode ONLY the new tokens (The Assistant's reply)\n",
    "generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767579440074,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "CKKZsiDRMnKy",
    "outputId": "af6ddfe2-2ac5-4709-de3f-fedc3faf9e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Valid Python code extracted.\n"
     ]
    }
   ],
   "source": [
    "# Now run the check on ONLY the generated text\n",
    "code, status = extract_solution(generated_text) # Use the new variable\n",
    "print(\"Status:\", status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1767579440102,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "G8DcMGzTMtQ0",
    "outputId": "c2857aa3-ca71-495c-c594-9fbfc9c6f483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extracted Python Code ---\n",
      "\n",
      "def factorial(n):\n",
      "    if n <= 0:\n",
      "        return 1\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    return result\n"
     ]
    }
   ],
   "source": [
    "# Show snippet of the extracted code\n",
    "if code:\n",
    "    print(\"\\n--- Extracted Python Code ---\\n\")\n",
    "    print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1urFQMgaulu"
   },
   "source": [
    "# Step 8: Verifier Integration (EvalPlus MBPP+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1186,
     "status": "ok",
     "timestamp": 1767579441835,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "dYJ4b4wiN-PR",
    "outputId": "7e2e7fa9-751c-4e65-a99d-9fb6a5051015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from https://github.com/evalplus/mbppplus_release/releases/download/v0.2.0/MbppPlus.jsonl.gz\n",
      "Loaded MBPP+ tasks: 378\n"
     ]
    }
   ],
   "source": [
    "# Load MBPP+ tasks as a dict: {task_id: problem_dict}\n",
    "MBPP_TASKS = get_mbpp_plus()\n",
    "print(f\"Loaded MBPP+ tasks: {len(MBPP_TASKS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767579441839,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "A3fpg80PbTlc",
    "outputId": "4c2a30ea-c510-4b96-bd46-15929df49cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Task ID: Mbpp/2\n",
      "Keys: ['task_id', 'prompt', 'entry_point', 'canonical_solution', 'base_input', 'atol', 'plus_input', 'contract', 'assertion']\n",
      "\n",
      "Prompt (first 400 chars):\n",
      " \"\"\"\n",
      "Write a function to find the shared elements from the given two lists.\n",
      "assert set(similar_elements((3, 4, 5, 6),(5, 7, 4, 10))) == set((4, 5))\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick peek at one task to confirm fields & shape\n",
    "sample_task_id = next(iter(MBPP_TASKS.keys()))\n",
    "sample_task = MBPP_TASKS[sample_task_id]\n",
    "\n",
    "print(\"\\nSample Task ID:\", sample_task_id)\n",
    "print(\"Keys:\", list(sample_task.keys()))\n",
    "print(\"\\nPrompt (first 400 chars):\\n\", sample_task[\"prompt\"][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767579441841,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "RZsHYa9wbdUH"
   },
   "outputs": [],
   "source": [
    "# Normalize MBPP+ task -> list of test assertion strings across EvalPlus variants.\n",
    "def get_tests_from_task(task: dict) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of test assertion strings from a task dict.\n",
    "    Supports list-style fields and a single multiline 'assertion' string.\n",
    "    \"\"\"\n",
    "    # Prefer explicit list-like fields (most common shapes)\n",
    "    for k in (\"test_list\", \"tests\", \"plus_tests\", \"base_tests\"):\n",
    "        if k in task and task[k]:\n",
    "            return list(task[k])\n",
    "\n",
    "    # Fallback: split a single multiline assertion string into non-empty lines\n",
    "    if \"assertion\" in task and task[\"assertion\"]:\n",
    "        lines = task[\"assertion\"].strip().splitlines()\n",
    "        return [line for line in lines if line.strip()]\n",
    "\n",
    "    # Nothing found â€” surface available keys for debugging\n",
    "    raise KeyError(f\"No tests found in task keys: {list(task.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6wcAKaBfVwn"
   },
   "source": [
    "# Step 9: Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767579441861,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "ugV0PZjq6WS0"
   },
   "outputs": [],
   "source": [
    "def _exec_code_and_tests_worker(code: str, tests: list[str], queue: mp.Queue) -> None:\n",
    "    \"\"\"\n",
    "    Run user code and all tests in a forked process, returning (passed, total, first_error).\n",
    "    Key points:\n",
    "      - Executes all tests to provide partial credit.\n",
    "      - Catches and truncates tracebacks to keep IPC payload small.\n",
    "      - Never raises outwards; always writes a tuple to the queue.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Isolated execution environment that exposes builtins only\n",
    "        env = {\"__builtins__\": __builtins__}\n",
    "\n",
    "        # Load/define user-provided code into the environment\n",
    "        exec(code, env, env)\n",
    "\n",
    "        passed_count = 0\n",
    "        total_tests = len(tests)\n",
    "        first_error = None\n",
    "\n",
    "        # Execute every test; tally passes and capture only the first error (truncated)\n",
    "        for t in tests:\n",
    "            try:\n",
    "                exec(t, env, env)\n",
    "                passed_count += 1\n",
    "            except Exception:\n",
    "                # Save the first traceback for debugging, but truncate for IPC safety\n",
    "                if first_error is None:\n",
    "                    tb = traceback.format_exc()\n",
    "                    first_error = tb[:500] + \"\\n...[TRUNCATED]...\" if len(tb) > 500 else tb\n",
    "                # Continue running remaining tests regardless of failures\n",
    "                continue\n",
    "\n",
    "        # Send results back to parent: (passed, total, first_error_or_None)\n",
    "        queue.put((passed_count, total_tests, first_error))\n",
    "\n",
    "    except Exception:\n",
    "        # If user code itself crashes (e.g., SyntaxError), return zero passes and truncated traceback\n",
    "        tb = traceback.format_exc()\n",
    "        truncated_error = tb[:500] + \"\\n...[TRUNCATED]...\" if len(tb) > 500 else tb\n",
    "        queue.put((0, len(tests), truncated_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579441863,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "7qvlGFIjfpRV"
   },
   "outputs": [],
   "source": [
    "def run_mbpp_tests(code: str, task: dict, timeout_s: float = 2.0) -> tuple[int, int, str | None]:\n",
    "    \"\"\"\n",
    "    Executes tests and returns (passed_count, total_count, first_error).\n",
    "    \"\"\"\n",
    "    # Extract test assertion strings from the task dict\n",
    "    tests = get_tests_from_task(task)\n",
    "    if not tests:\n",
    "        return 0, 0, \"No tests found.\"\n",
    "\n",
    "    # Use a forked process for isolation (prevents user code from affecting the main process)\n",
    "    ctx = mp.get_context(\"fork\")\n",
    "    q = ctx.Queue()\n",
    "\n",
    "    # Spawn worker that execs the user code and runs each test, placing results on the queue\n",
    "    p = ctx.Process(target=_exec_code_and_tests_worker, args=(code, tests, q))\n",
    "    p.start()\n",
    "\n",
    "    # Wait up to timeout_s for the worker to finish\n",
    "    p.join(timeout_s)\n",
    "\n",
    "    # If still running, kill the worker and report a timeout\n",
    "    if p.is_alive():\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "        return 0, len(tests), f\"Timeout after {timeout_s:.1f}s\"\n",
    "\n",
    "    # If worker exited but produced no IPC payload, treat as failure\n",
    "    if q.empty():\n",
    "        return 0, len(tests), \"No result returned from worker.\"\n",
    "\n",
    "    # Unpack the worker's (passed_count, total_count, first_error_or_None)\n",
    "    passed_count, total_count, err = q.get()\n",
    "    return passed_count, total_count, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlE3bAZ3LjwO"
   },
   "source": [
    "# Step 10: Defining Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1767579443336,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "q5SEd65CLneK"
   },
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Rewards the model for strictly following the XML schema.\n",
    "    Args:\n",
    "        completions: List of generated strings from the model.\n",
    "    Returns:\n",
    "        List of rewards (0.02 for valid schema, 0.0 for invalid).\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        # Uses your existing validator from Step 6\n",
    "        is_valid, _ = validate_schema(completion)\n",
    "        rewards.append(0.02 if is_valid else 0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579443510,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "bfVxKTg9L6cd"
   },
   "outputs": [],
   "source": [
    "def reasoning_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    - Ascent: Linear 0-400 chars.\n",
    "    - Plateau: 400-800 chars (Max Reward 0.1).\n",
    "    - Penalty: Aggressive slope (0.03) starts > 800.\n",
    "    - Zero Point: Reward hits 0.0 at ~1133 chars.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        # Extract reasoning block between the required tags (case-insensitive, multiline)\n",
    "        match = re.search(r\"<START_WORKING_OUT>(.*?)</END_WORKING_OUT>\", completion, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "        if match:\n",
    "            content = match.group(1).strip()\n",
    "            length = len(content)\n",
    "\n",
    "            # Too short: no useful reasoning\n",
    "            if length < 50:\n",
    "                rewards.append(0.0)\n",
    "\n",
    "            elif length <= 400:\n",
    "                # Linear ascent from 0.0 -> 0.1 as length goes 0 -> 400\n",
    "                score = (length / 400.0) * 0.1\n",
    "                rewards.append(score)\n",
    "\n",
    "            elif length <= 800:\n",
    "                # Plateau: encourage sufficient but concise reasoning (cap reward)\n",
    "                rewards.append(0.1)\n",
    "\n",
    "            else:\n",
    "                # Aggressive penalty beyond 800 chars to disincentivize filibustering\n",
    "                overage = length - 800\n",
    "                penalty = (overage / 100.0) * 0.03  # 0.03 per 100 chars over\n",
    "                score = 0.1 - penalty\n",
    "                # Floor at -0.1 to avoid unbounded negative incentives\n",
    "                rewards.append(max(-0.1, score))\n",
    "\n",
    "        else:\n",
    "            # Missing reasoning block -> no reward\n",
    "            rewards.append(0.0)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767579444067,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "hLNU6pML0gqN"
   },
   "outputs": [],
   "source": [
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Rewards the model based on the PERCENTAGE of tests passed.\n",
    "    Includes a \"Clean Sweep Bonus\" for 100% completion.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for prompt, completion, task_data in zip(prompts, completions, answer):\n",
    "        # Extract the code snippet from the generated completion (<SOLUTION>...</SOLUTION>)\n",
    "        code, status = extract_solution(completion)\n",
    "        # If extraction failed (missing/invalid code), assign zero reward\n",
    "        if not code:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Run the MBPP tests in an isolated worker; returns (passed, total, first_error)\n",
    "        passed, total, err = run_mbpp_tests(code, task_data)\n",
    "\n",
    "        # If the task has no tests, treat as zero-reward (safety)\n",
    "        if total == 0:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Fractional credit: proportion of tests passed (0.0 - 1.0)\n",
    "        score = passed / total\n",
    "\n",
    "        # Clean-sweep bonus: small extra reward for passing all tests (distinguishes perfect)\n",
    "        if passed == total:\n",
    "            score += 0.1\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI6Eut_JXUZW"
   },
   "source": [
    "# Step 11: Dataset Formatting and Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767579447152,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "jJF74XFesHNc"
   },
   "outputs": [],
   "source": [
    "# Clean the Data\n",
    "# The raw MBPP+ dataset has inconsistent schemas (some fields are lists, some are None).\n",
    "# We fix this by extracting ONLY what we need: the test cases.\n",
    "dict_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767579447341,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Fx2h97Kfp9sf"
   },
   "outputs": [],
   "source": [
    "for task_id, task_data in MBPP_TASKS.items():\n",
    "    # Extract the test cases using our helper from Step 8\n",
    "    # This handles the \"messy\" parsing right now, so the Dataset is clean.\n",
    "    try:\n",
    "        tests = get_tests_from_task(task_data)\n",
    "    except KeyError:\n",
    "        # If a task is broken/empty, skip it to prevent crashes\n",
    "        print(f\"Skipping task {task_id}: No tests found.\")\n",
    "        continue\n",
    "\n",
    "    # Create a CLEAN 'answer' dictionary\n",
    "    # This guarantees every row has the exact same structure.\n",
    "    # This prevents the \"ArrowInvalid\" error.\n",
    "    clean_answer = {\n",
    "        \"task_id\": str(task_id),\n",
    "        \"test_list\": tests  # Always a List of Strings\n",
    "    }\n",
    "\n",
    "    # Append to our list\n",
    "    dict_data.append({\n",
    "        \"prompt\": task_data[\"prompt\"],\n",
    "        \"answer\": clean_answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579447590,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "O3VW7urjp9qi"
   },
   "outputs": [],
   "source": [
    "# Creating a Hugging Face compatible dataset\n",
    "dataset = Dataset.from_list(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1767579447751,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "IZsJGtmHp9oL",
    "outputId": "117c2c71-a1e6-43a0-9199-71589f38ba8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Features: {'prompt': Value('string'), 'answer': {'task_id': Value('string'), 'test_list': List(Value('string'))}}\n",
      "Sample Row Answer Keys: dict_keys(['task_id', 'test_list'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Features:\", dataset.features)\n",
    "print(\"Sample Row Answer Keys:\", dataset[0][\"answer\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1767579447976,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "pv2zGhEauI7Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pick the 2nd task again for consistency\n",
    "task = dataset[2][\"answer\"] # We grab it from our NEW dataset column\n",
    "prompt = dataset[2][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767579448117,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "THY3eewTxW9T",
    "outputId": "bec8d082-a1a8-490f-ec2f-fd519f4e34f8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\"\"\"\\nWrite a function to find the n largest integers from a given list of numbers, returned in descending order.\\nassert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65]\\n\"\"\"\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1767579448348,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "J5qx7l6duI5A"
   },
   "outputs": [],
   "source": [
    "# Build the prompt structure\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579448554,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zp95CUQouI25"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ")\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 6083,
     "status": "ok",
     "timestamp": 1767579454697,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zB24zGc3uI0m"
   },
   "outputs": [],
   "source": [
    "# Generate\n",
    "FastLanguageModel.for_inference(model)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767579454737,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "yEa1OUdJuIxm"
   },
   "outputs": [],
   "source": [
    "# Slice to get only the generated text\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1767579454746,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "1-hJ_d1-p9j3",
    "outputId": "692c123d-b979-474b-8676-ea44caa003a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START_WORKING_OUT>\n",
      "Problem: Find n largest integers from a list, return in descending order.\n",
      "Approach: Use heapq.nlargest which returns n largest elements in order from largest to smallest.\n",
      "Parameters: List of numbers, integer n.\n",
      "Return: List of n largest numbers in descending order.\n",
      "</END_WORKING_OUT>\n",
      "<SOLUTION>\n",
      "import heapq\n",
      "\n",
      "def heap_queue_largest(nums, n):\n",
      "    \"\"\"\n",
      "    Return the n largest numbers from nums in descending order.\n",
      "    \n",
      "    Args:\n",
      "        nums: List of numbers (integers or floats)\n",
      "        n: Number of largest elements to return\n",
      "        \n",
      "    Returns:\n",
      "        List of n largest numbers in descending order\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return []\n",
      "    if n >= len(nums):\n",
      "        nums_sorted = sorted(nums, reverse=True)\n",
      "        return nums_sorted[:n]\n",
      "    return heapq.nlargest(n, nums)\n",
      "</SOLUTION>\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579454748,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "mGvSjCOcun8N"
   },
   "outputs": [],
   "source": [
    "# CRITICAL PART: Testing the Reward Functions\n",
    "# The Reward Functions expect LISTS (Batches), so we wrap our single item in a list.\n",
    "# This simulates a batch size of 1.\n",
    "batch_prompts = [prompt]\n",
    "batch_completions = [generated_text]\n",
    "batch_answers = [task] # This is the \"answer\" column data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1767579454783,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "QKcwBYWcun5J",
    "outputId": "29cac1f3-a25b-4bf7-c4e9-2824bacc6929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format Reward (Expect 0.1): 0.02\n"
     ]
    }
   ],
   "source": [
    "# 1. Test Format Reward\n",
    "r_format = format_reward_func(completions=batch_completions)\n",
    "print(f\"Format Reward (Expect 0.1): {r_format[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1767579454790,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "h8Mo1AnZun3L",
    "outputId": "d23bb222-5039-4be5-9dee-636755c526ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning Reward (Expect 0.0-0.15): 0.0663\n"
     ]
    }
   ],
   "source": [
    "# 2. Test Reasoning Reward\n",
    "r_reason = reasoning_reward_func(completions=batch_completions)\n",
    "print(f\"Reasoning Reward (Expect 0.0-0.15): {r_reason[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1767579454804,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "07ZCMGfIun08",
    "outputId": "ee3f0248-eaeb-4b58-869b-d11fb091a7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness Reward (Expect 1.0 or 0.0): 1.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Test Correctness Reward (The complex one)\n",
    "# Note: We pass 'answer' explicitly, just like the Trainer will.\n",
    "r_correct = correctness_reward_func(\n",
    "    prompts=batch_prompts,\n",
    "    completions=batch_completions,\n",
    "    answer=batch_answers\n",
    ")\n",
    "print(f\"Correctness Reward (Expect 1.0 or 0.0): {r_correct[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767579454807,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "fADybjDeunyh",
    "outputId": "46906e1b-ddb4-41e8-d7d9-bf78b3009371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SUCCESS: All reward functions accepted the inputs and returned scores.\n",
      " The plumbing is connected correctly.\n"
     ]
    }
   ],
   "source": [
    "if r_format[0] > 0 and (r_correct[0] == 0.0 or r_correct[0] == 1.1):\n",
    "    print(\" SUCCESS: All reward functions accepted the inputs and returned scores.\")\n",
    "    print(\" The plumbing is connected correctly.\")\n",
    "else:\n",
    "    print(\" FAIL: Something returned an unexpected format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24UBloc4drcX"
   },
   "source": [
    "# Step 12: Apply Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1767579455837,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "RO2L3cPMdG0H"
   },
   "outputs": [],
   "source": [
    "def apply_chat_template(row):\n",
    "    # Build ChatML-style message list for this example\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": row[\"prompt\"]}\n",
    "    ]\n",
    "\n",
    "    # Produce the raw chat-formatted string (not tokenized).\n",
    "    # GRPOTrainer expects the 'prompt' column to contain this exact ChatML-style text.\n",
    "    # add_generation_prompt=True appends the model's generation prompt marker.\n",
    "    row[\"prompt\"] = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBJ_nPqWdzC-"
   },
   "outputs": [],
   "source": [
    "# Apply it to the whole dataset\n",
    "original_prompt = dataset[0][\"prompt\"]\n",
    "dataset = dataset.map(apply_chat_template)\n",
    "\n",
    "print(\"\\n--- BEFORE ---\")\n",
    "print(original_prompt)\n",
    "print(\"\\n--- AFTER (What the Model Sees) ---\")\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX_1Z2XBgGJk"
   },
   "source": [
    "# Step 13: Setting up GRPO Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767579496389,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "HArFazRbd3w5"
   },
   "outputs": [],
   "source": [
    "# We give the model ample room so it never gets cut off\n",
    "max_prompt_length = 512\n",
    "max_completion_length = 2048  # doubled from T4 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1767579496880,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "s3bSpC1ngg_K"
   },
   "outputs": [],
   "source": [
    "# vLLM sampling params tuned for GRPO rollouts (concise, reproducible, RL-friendly)\n",
    "# Use nucleus sampling with a modest randomness and explicit EOS-only stopping.\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    # Minimum cumulative probability floor to guarantee baseline diversity\n",
    "    min_p = 0.1,\n",
    "    # Nucleus cutoff (retain top 95% mass)\n",
    "    top_p = 0.95,\n",
    "    # Disable top-k (use top-p only)\n",
    "    top_k = -1,\n",
    "    # Fixed seed for reproducibility across rollouts\n",
    "    seed = 3407,\n",
    "    # Controls randomness; 0.8 = moderate diversity\n",
    "    temperature = 0.8, # High enough to get diverse answers for GRPO\n",
    "    # Stop exclusively on the tokenizer's EOS to avoid brittle string stops\n",
    "    stop = [tokenizer.eos_token],\n",
    "    # Preserve the stop token in the returned text for reliable termination detection\n",
    "    include_stop_str_in_output = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1767579507381,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "IfEtXGoIgrEV",
    "outputId": "e5d2a5c5-ecc9-4e8c-9cc8-0fd600433874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` * `gradient_accumulation_steps` * `world_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 16\n"
     ]
    }
   ],
   "source": [
    "# 3. The Trainer Config: GRPO hyperparams and integration knobs\n",
    "training_args = GRPOConfig(\n",
    "    # Integration with sampling, outputs, and experiment tracking\n",
    "    vllm_sampling_params = vllm_sampling_params, # vLLM sampling behavior used during rollouts\n",
    "    output_dir = \"outputs\",                      # checkpoint / artifact dir\n",
    "    report_to = \"wandb\",                         # logger backend\n",
    "    run_name = \"mbpp-grpo-h100-run4-full\",       # experiment name\n",
    "\n",
    "    # Optimization hyperparameters\n",
    "    learning_rate = 5e-6,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",                        # memory-efficient optimizer\n",
    "\n",
    "    # Batch / generation sizing tuned for large GPU\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    num_generations = 16,             # number of rollouts per prompt (G)\n",
    "\n",
    "    # Sequence length caps\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "\n",
    "    # Training duration\n",
    "    num_train_epochs = 2,            # conservative for RL on limited data\n",
    "    #max_steps = 5,\n",
    "\n",
    "    # Logging / checkpointing / runtime flags\n",
    "    logging_steps = 5,\n",
    "    save_steps = 30,                 # checkpoint frequency\n",
    "    use_vllm = True,                 # enable vLLM-backed generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz18F7NsrHPI"
   },
   "source": [
    "# Step 14: Initialize and Run GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767579507709,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "bDJjRr2srGn1"
   },
   "outputs": [],
   "source": [
    "# Select the Reward Functions we defined in Step 10\n",
    "# These are the \"Judges\" that will score the model's outputs.\n",
    "reward_functions = [\n",
    "    format_reward_func,       # Did it use <START_WORKING_OUT> and <SOLUTION>? (0.1)\n",
    "    reasoning_reward_func,    # Did it write ~500 chars of thought? (0.2)\n",
    "    correctness_reward_func   # Did the code actually pass the tests? (1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1767579508035,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "cpXKD9MYhJaI"
   },
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = reward_functions,\n",
    "    args = training_args,         # The A100 Config we just built\n",
    "    train_dataset = dataset,      # The dataset with the Chat Template applied\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5448239,
     "status": "ok",
     "timestamp": 1767584956418,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "TRNxA8IKraoK",
    "outputId": "60382391-6d7d-4bf6-ca85-bd1c130262ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 378 | Num Epochs = 2 | Total steps = 188\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 4 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/drive/MyDrive/grpo-verified-reasoner/wandb/run-20260105_021832-x0n9lpib</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/x0n9lpib' target=\"_blank\">mbpp-grpo-h100-run4-full</a></strong> to <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/x0n9lpib' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/x0n9lpib</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Detected [huggingface_hub.inference, openai] in use.\n",
      "wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 1:29:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / format_reward_func / mean</th>\n",
       "      <th>rewards / format_reward_func / std</th>\n",
       "      <th>rewards / reasoning_reward_func / mean</th>\n",
       "      <th>rewards / reasoning_reward_func / std</th>\n",
       "      <th>rewards / correctness_reward_func / mean</th>\n",
       "      <th>rewards / correctness_reward_func / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.917053</td>\n",
       "      <td>0.175341</td>\n",
       "      <td>145.737500</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>739.800000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>139.805557</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>399.200000</td>\n",
       "      <td>8.317562</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.060240</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.836875</td>\n",
       "      <td>0.407541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.014248</td>\n",
       "      <td>0.223273</td>\n",
       "      <td>193.525000</td>\n",
       "      <td>69.200000</td>\n",
       "      <td>1280.200000</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>176.250284</td>\n",
       "      <td>69.200000</td>\n",
       "      <td>780.400000</td>\n",
       "      <td>3.859137</td>\n",
       "      <td>0.019875</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.065154</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.929219</td>\n",
       "      <td>0.368260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.901061</td>\n",
       "      <td>0.330024</td>\n",
       "      <td>183.212500</td>\n",
       "      <td>79.600000</td>\n",
       "      <td>892.200000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>177.288196</td>\n",
       "      <td>79.600000</td>\n",
       "      <td>557.400000</td>\n",
       "      <td>0.542674</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.026858</td>\n",
       "      <td>0.812083</td>\n",
       "      <td>0.442199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.910160</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>244.290625</td>\n",
       "      <td>63.600000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>221.132184</td>\n",
       "      <td>63.600000</td>\n",
       "      <td>1173.800000</td>\n",
       "      <td>0.637628</td>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.066649</td>\n",
       "      <td>0.034955</td>\n",
       "      <td>0.823698</td>\n",
       "      <td>0.437160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.048846</td>\n",
       "      <td>0.212669</td>\n",
       "      <td>197.840625</td>\n",
       "      <td>59.200000</td>\n",
       "      <td>1055.400000</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>180.495294</td>\n",
       "      <td>59.200000</td>\n",
       "      <td>522.000000</td>\n",
       "      <td>1.120589</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.071929</td>\n",
       "      <td>0.023150</td>\n",
       "      <td>0.956979</td>\n",
       "      <td>0.299423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.752737</td>\n",
       "      <td>0.221269</td>\n",
       "      <td>256.506250</td>\n",
       "      <td>68.800000</td>\n",
       "      <td>1627.800000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>233.955322</td>\n",
       "      <td>68.800000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>0.649474</td>\n",
       "      <td>0.019750</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.071581</td>\n",
       "      <td>0.036045</td>\n",
       "      <td>0.661406</td>\n",
       "      <td>0.447654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.903331</td>\n",
       "      <td>0.243252</td>\n",
       "      <td>190.984375</td>\n",
       "      <td>63.600000</td>\n",
       "      <td>1123.400000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>185.278128</td>\n",
       "      <td>63.600000</td>\n",
       "      <td>907.200000</td>\n",
       "      <td>45.043564</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073904</td>\n",
       "      <td>0.027420</td>\n",
       "      <td>0.809427</td>\n",
       "      <td>0.451101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.713682</td>\n",
       "      <td>0.328387</td>\n",
       "      <td>274.953125</td>\n",
       "      <td>73.600000</td>\n",
       "      <td>1341.600000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>247.035092</td>\n",
       "      <td>73.600000</td>\n",
       "      <td>1007.000000</td>\n",
       "      <td>5.703990</td>\n",
       "      <td>0.019750</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.074870</td>\n",
       "      <td>0.037124</td>\n",
       "      <td>0.619063</td>\n",
       "      <td>0.497398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.891254</td>\n",
       "      <td>0.210045</td>\n",
       "      <td>262.625000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>1604.200000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>240.103711</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>0.860802</td>\n",
       "      <td>0.019875</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.073462</td>\n",
       "      <td>0.035464</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.463865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.999469</td>\n",
       "      <td>0.299818</td>\n",
       "      <td>237.096875</td>\n",
       "      <td>90.800000</td>\n",
       "      <td>1051.200000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>231.536609</td>\n",
       "      <td>90.800000</td>\n",
       "      <td>772.400000</td>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.080990</td>\n",
       "      <td>0.032890</td>\n",
       "      <td>0.898542</td>\n",
       "      <td>0.357206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.048051</td>\n",
       "      <td>0.162779</td>\n",
       "      <td>251.765625</td>\n",
       "      <td>69.200000</td>\n",
       "      <td>1416.600000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>223.593784</td>\n",
       "      <td>69.200000</td>\n",
       "      <td>938.600000</td>\n",
       "      <td>0.539351</td>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.076989</td>\n",
       "      <td>0.036684</td>\n",
       "      <td>0.951250</td>\n",
       "      <td>0.290710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.908891</td>\n",
       "      <td>0.286242</td>\n",
       "      <td>295.306250</td>\n",
       "      <td>85.800000</td>\n",
       "      <td>1829.200000</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>256.510431</td>\n",
       "      <td>85.800000</td>\n",
       "      <td>1015.600000</td>\n",
       "      <td>0.635492</td>\n",
       "      <td>0.019750</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.069766</td>\n",
       "      <td>0.045317</td>\n",
       "      <td>0.819375</td>\n",
       "      <td>0.397898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.802528</td>\n",
       "      <td>0.309861</td>\n",
       "      <td>243.418750</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>977.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>243.418750</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>977.800000</td>\n",
       "      <td>0.658965</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074611</td>\n",
       "      <td>0.042932</td>\n",
       "      <td>0.707917</td>\n",
       "      <td>0.494022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.819935</td>\n",
       "      <td>0.254476</td>\n",
       "      <td>303.312500</td>\n",
       "      <td>84.400000</td>\n",
       "      <td>1825.000000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>281.155670</td>\n",
       "      <td>84.400000</td>\n",
       "      <td>1072.000000</td>\n",
       "      <td>0.419592</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.076196</td>\n",
       "      <td>0.042867</td>\n",
       "      <td>0.723802</td>\n",
       "      <td>0.479978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.836737</td>\n",
       "      <td>0.235765</td>\n",
       "      <td>331.793750</td>\n",
       "      <td>103.200000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>277.537939</td>\n",
       "      <td>103.200000</td>\n",
       "      <td>948.600000</td>\n",
       "      <td>2.738034</td>\n",
       "      <td>0.019625</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.078154</td>\n",
       "      <td>0.041262</td>\n",
       "      <td>0.738958</td>\n",
       "      <td>0.490608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.798150</td>\n",
       "      <td>0.298776</td>\n",
       "      <td>291.103125</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>251.866513</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>955.800000</td>\n",
       "      <td>0.562828</td>\n",
       "      <td>0.019625</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.078369</td>\n",
       "      <td>0.036578</td>\n",
       "      <td>0.700156</td>\n",
       "      <td>0.469152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.944133</td>\n",
       "      <td>0.316302</td>\n",
       "      <td>257.209375</td>\n",
       "      <td>96.800000</td>\n",
       "      <td>1413.600000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>234.629800</td>\n",
       "      <td>96.800000</td>\n",
       "      <td>567.800000</td>\n",
       "      <td>2.042440</td>\n",
       "      <td>0.019875</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.083112</td>\n",
       "      <td>0.033568</td>\n",
       "      <td>0.841146</td>\n",
       "      <td>0.420335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.937997</td>\n",
       "      <td>0.260665</td>\n",
       "      <td>248.696875</td>\n",
       "      <td>81.200000</td>\n",
       "      <td>1181.800000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>243.095688</td>\n",
       "      <td>81.200000</td>\n",
       "      <td>1019.000000</td>\n",
       "      <td>0.795987</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078413</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.839583</td>\n",
       "      <td>0.410716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.934665</td>\n",
       "      <td>0.223424</td>\n",
       "      <td>215.234375</td>\n",
       "      <td>86.200000</td>\n",
       "      <td>864.600000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>209.480161</td>\n",
       "      <td>86.200000</td>\n",
       "      <td>541.800000</td>\n",
       "      <td>0.613555</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.032668</td>\n",
       "      <td>0.832865</td>\n",
       "      <td>0.383025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.070177</td>\n",
       "      <td>0.246393</td>\n",
       "      <td>274.712500</td>\n",
       "      <td>103.800000</td>\n",
       "      <td>1427.400000</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>257.763547</td>\n",
       "      <td>103.800000</td>\n",
       "      <td>940.000000</td>\n",
       "      <td>0.451052</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.081490</td>\n",
       "      <td>0.042297</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.349872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.819020</td>\n",
       "      <td>0.276125</td>\n",
       "      <td>293.884375</td>\n",
       "      <td>100.800000</td>\n",
       "      <td>1767.200000</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>260.729630</td>\n",
       "      <td>100.800000</td>\n",
       "      <td>1196.800000</td>\n",
       "      <td>0.709320</td>\n",
       "      <td>0.019750</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.082499</td>\n",
       "      <td>0.036711</td>\n",
       "      <td>0.716771</td>\n",
       "      <td>0.481464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.783876</td>\n",
       "      <td>0.297969</td>\n",
       "      <td>308.765625</td>\n",
       "      <td>108.600000</td>\n",
       "      <td>1504.800000</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>292.608466</td>\n",
       "      <td>108.600000</td>\n",
       "      <td>1254.400000</td>\n",
       "      <td>0.505704</td>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.079220</td>\n",
       "      <td>0.038706</td>\n",
       "      <td>0.684844</td>\n",
       "      <td>0.404036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.880284</td>\n",
       "      <td>0.289017</td>\n",
       "      <td>302.606250</td>\n",
       "      <td>80.800000</td>\n",
       "      <td>1789.600000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>258.028439</td>\n",
       "      <td>80.800000</td>\n",
       "      <td>1015.600000</td>\n",
       "      <td>0.508625</td>\n",
       "      <td>0.019687</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.077680</td>\n",
       "      <td>0.043298</td>\n",
       "      <td>0.782917</td>\n",
       "      <td>0.418157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.963055</td>\n",
       "      <td>0.261982</td>\n",
       "      <td>236.534375</td>\n",
       "      <td>88.600000</td>\n",
       "      <td>1059.800000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>230.865778</td>\n",
       "      <td>88.600000</td>\n",
       "      <td>828.600000</td>\n",
       "      <td>0.528580</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083420</td>\n",
       "      <td>0.032172</td>\n",
       "      <td>0.859635</td>\n",
       "      <td>0.405787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.010831</td>\n",
       "      <td>0.280232</td>\n",
       "      <td>272.809375</td>\n",
       "      <td>89.800000</td>\n",
       "      <td>1010.800000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>267.418256</td>\n",
       "      <td>89.800000</td>\n",
       "      <td>856.400000</td>\n",
       "      <td>0.405537</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.082144</td>\n",
       "      <td>0.035214</td>\n",
       "      <td>0.908750</td>\n",
       "      <td>0.378931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.926139</td>\n",
       "      <td>0.261707</td>\n",
       "      <td>327.412500</td>\n",
       "      <td>103.600000</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>255.344666</td>\n",
       "      <td>103.600000</td>\n",
       "      <td>1179.600000</td>\n",
       "      <td>0.564745</td>\n",
       "      <td>0.019437</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.075712</td>\n",
       "      <td>0.043390</td>\n",
       "      <td>0.830990</td>\n",
       "      <td>0.397211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.074616</td>\n",
       "      <td>0.195279</td>\n",
       "      <td>245.215625</td>\n",
       "      <td>88.400000</td>\n",
       "      <td>1431.200000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>222.586249</td>\n",
       "      <td>88.400000</td>\n",
       "      <td>552.600000</td>\n",
       "      <td>0.495118</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.084158</td>\n",
       "      <td>0.036635</td>\n",
       "      <td>0.970521</td>\n",
       "      <td>0.296879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.938030</td>\n",
       "      <td>0.295756</td>\n",
       "      <td>302.740625</td>\n",
       "      <td>113.400000</td>\n",
       "      <td>1806.200000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>274.819766</td>\n",
       "      <td>113.400000</td>\n",
       "      <td>1062.600000</td>\n",
       "      <td>0.544476</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.076321</td>\n",
       "      <td>0.050234</td>\n",
       "      <td>0.841771</td>\n",
       "      <td>0.414048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.982933</td>\n",
       "      <td>0.223077</td>\n",
       "      <td>277.481250</td>\n",
       "      <td>108.800000</td>\n",
       "      <td>1649.200000</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>266.526495</td>\n",
       "      <td>108.800000</td>\n",
       "      <td>1278.200000</td>\n",
       "      <td>0.440563</td>\n",
       "      <td>0.019875</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.080662</td>\n",
       "      <td>0.039247</td>\n",
       "      <td>0.882396</td>\n",
       "      <td>0.403316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.869949</td>\n",
       "      <td>0.267323</td>\n",
       "      <td>330.546875</td>\n",
       "      <td>97.400000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>319.904919</td>\n",
       "      <td>97.400000</td>\n",
       "      <td>1169.600000</td>\n",
       "      <td>0.524577</td>\n",
       "      <td>0.019750</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.067438</td>\n",
       "      <td>0.054461</td>\n",
       "      <td>0.782760</td>\n",
       "      <td>0.444609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.698878</td>\n",
       "      <td>0.301432</td>\n",
       "      <td>343.440625</td>\n",
       "      <td>118.800000</td>\n",
       "      <td>1582.200000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>316.882776</td>\n",
       "      <td>118.800000</td>\n",
       "      <td>1069.000000</td>\n",
       "      <td>0.480214</td>\n",
       "      <td>0.019750</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.076940</td>\n",
       "      <td>0.045368</td>\n",
       "      <td>0.602188</td>\n",
       "      <td>0.522861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.022698</td>\n",
       "      <td>0.196326</td>\n",
       "      <td>256.506250</td>\n",
       "      <td>93.800000</td>\n",
       "      <td>1350.200000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>251.194647</td>\n",
       "      <td>93.800000</td>\n",
       "      <td>1155.200000</td>\n",
       "      <td>0.747510</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.082240</td>\n",
       "      <td>0.031849</td>\n",
       "      <td>0.920521</td>\n",
       "      <td>0.294702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.991776</td>\n",
       "      <td>0.258878</td>\n",
       "      <td>320.156250</td>\n",
       "      <td>85.800000</td>\n",
       "      <td>1815.800000</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>281.685196</td>\n",
       "      <td>85.800000</td>\n",
       "      <td>1137.000000</td>\n",
       "      <td>0.602839</td>\n",
       "      <td>0.019562</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.074870</td>\n",
       "      <td>0.047406</td>\n",
       "      <td>0.897344</td>\n",
       "      <td>0.332155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.848510</td>\n",
       "      <td>0.291594</td>\n",
       "      <td>314.265625</td>\n",
       "      <td>114.200000</td>\n",
       "      <td>1802.000000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>292.660333</td>\n",
       "      <td>114.200000</td>\n",
       "      <td>1161.200000</td>\n",
       "      <td>0.444122</td>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.075885</td>\n",
       "      <td>0.043752</td>\n",
       "      <td>0.752813</td>\n",
       "      <td>0.409131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.002514</td>\n",
       "      <td>0.255594</td>\n",
       "      <td>296.990625</td>\n",
       "      <td>102.600000</td>\n",
       "      <td>1737.200000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>268.868008</td>\n",
       "      <td>102.600000</td>\n",
       "      <td>1006.000000</td>\n",
       "      <td>0.753369</td>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.081660</td>\n",
       "      <td>0.038570</td>\n",
       "      <td>0.901042</td>\n",
       "      <td>0.364280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.782033</td>\n",
       "      <td>0.267435</td>\n",
       "      <td>391.437500</td>\n",
       "      <td>131.800000</td>\n",
       "      <td>1737.200000</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>354.767572</td>\n",
       "      <td>131.800000</td>\n",
       "      <td>1513.600000</td>\n",
       "      <td>0.546412</td>\n",
       "      <td>0.019438</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.071762</td>\n",
       "      <td>0.054369</td>\n",
       "      <td>0.690833</td>\n",
       "      <td>0.489857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.907148</td>\n",
       "      <td>0.248970</td>\n",
       "      <td>271.646875</td>\n",
       "      <td>100.800000</td>\n",
       "      <td>1374.000000</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>254.726657</td>\n",
       "      <td>100.800000</td>\n",
       "      <td>950.200000</td>\n",
       "      <td>0.473860</td>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.080095</td>\n",
       "      <td>0.036991</td>\n",
       "      <td>0.807240</td>\n",
       "      <td>0.413769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "-inf\n",
      "20\n",
      "13\n",
      "1\n",
      "12\n",
      "64\n",
      "[('Social sciences', 82), ('English', 88), ('Science', 90), ('Maths', 97)]\n",
      "(0, 0, '')\n",
      "[3, 4, 5, 6, 7, 10]\n",
      "30\n",
      "[(6, 24, 12)]\n",
      "[1, 4, 9, 16, 25]\n",
      "Computed angle: 1.5707963267948966\n",
      "[('Red',), ('Green',), ('Blue',)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204.20352248333654\n",
      "243\n",
      "345\n",
      "513\n",
      "243\n",
      "243\n",
      "345\n",
      "513\n",
      "243\n",
      "243\n",
      "345\n",
      "513\n",
      "243\n",
      "345\n",
      "513\n",
      "16.0\n",
      "106\n",
      "1256.6370614359173\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>profiling/Time taken: UnslothGRPOTrainer._calculate_rewards</td><td>â–ˆâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–ˆâ–‚â–‚â–â–ˆâ–‚â–‚â–ƒâ–‚â–ƒâ–â–â–‚</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer._prepare_inputs</td><td>â–â–â–â–†â–â–†â–â–â–ˆâ–ˆâ–…â–â–ˆâ–â–â–â–ˆâ–ˆâ–â–ˆâ–â–â–â–…â–â–â–â–ˆâ–â–â–â–â–â–â–ˆâ–ˆâ–â–ˆâ–â–</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.correctness_reward_func</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.format_reward_func</td><td>â–‚â–‚â–‚â–â–â–…â–‚â–‚â–†â–„â–ƒâ–ƒâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–†â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–„â–…â–†â–‡â–…â–†â–‚â–…</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.reasoning_reward_func</td><td>â–â–‚â–â–‚â–‚â–ƒâ–ƒâ–ˆâ–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–ˆâ–ƒâ–„â–…â–ƒâ–ƒâ–†â–ƒâ–„â–„â–„â–ƒâ–„â–„â–‡â–„â–†â–†â–‡â–„â–„â–…â–ƒâ–†</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.vLLM.generate</td><td>â–ˆâ–„â–‡â–â–â–â–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ƒâ–„â–ƒâ–ˆâ–ˆâ–ƒâ–„â–‡â–„â–‡â–ˆâ–ˆâ–ˆâ–…â–‚â–„â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆâ–‡â–ˆâ–ˆâ–‚â–†â–ˆ</td></tr><tr><td>train/completion_length</td><td>â–â–‚â–‚â–„â–‚â–„â–‚â–…â–„â–„â–„â–…â–„â–…â–†â–…â–„â–„â–ƒâ–…â–…â–†â–…â–„â–…â–†â–„â–…â–…â–†â–‡â–„â–†â–†â–…â–ˆâ–…â–†</td></tr><tr><td>train/completions/clipped_ratio</td><td>â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–‚â–„â–…â–â–ƒâ–†â–…â–ƒâ–‚â–‚â–ƒâ–„â–ƒâ–…â–‚â–‚â–ˆâ–ƒâ–„â–‚â–‚â–„â–‚â–…â–ƒâ–„â–…â–ƒâ–„</td></tr><tr><td>train/completions/max_length</td><td>â–â–„â–‚â–‡â–ƒâ–†â–ƒâ–„â–†â–ƒâ–…â–‡â–‚â–‡â–ˆâ–ˆâ–…â–ƒâ–‚â–…â–†â–…â–‡â–ƒâ–‚â–ˆâ–…â–‡â–†â–…â–†â–„â–‡â–‡â–†â–†â–„â–†</td></tr><tr><td>train/completions/max_terminated_length</td><td>â–â–ƒâ–‚â–†â–‚â–†â–„â–…â–…â–ƒâ–„â–…â–…â–…â–„â–„â–‚â–…â–‚â–„â–†â–†â–…â–„â–„â–†â–‚â–…â–‡â–†â–…â–†â–†â–†â–…â–ˆâ–„â–‡</td></tr><tr><td>+20</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>profiling/Time taken: UnslothGRPOTrainer._calculate_rewards</td><td>5.18347</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer._prepare_inputs</td><td>1e-05</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.correctness_reward_func</td><td>5.18033</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.format_reward_func</td><td>0.00032</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.reasoning_reward_func</td><td>0.0006</td></tr><tr><td>profiling/Time taken: UnslothGRPOTrainer.vLLM.generate</td><td>11.72052</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/completion_length</td><td>317.44271</td></tr><tr><td>train/completions/clipped_ratio</td><td>0.01562</td></tr><tr><td>train/completions/max_length</td><td>1581.66667</td></tr><tr><td>+25</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mbpp-grpo-h100-run4-full</strong> at: <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/x0n9lpib' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project/runs/x0n9lpib</a><br> View project at: <a href='https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project' target=\"_blank\">https://wandb.ai/samyakshrestha-university-of-texas-at-dallas/mbpp-rl-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260105_021832-x0n9lpib/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=188, training_loss=0.0022916886509653737, metrics={'train_runtime': 5441.8689, 'train_samples_per_second': 0.139, 'train_steps_per_second': 0.035, 'total_flos': 0.0, 'train_loss': 0.0022916886509653737})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbb39NqiB9E8"
   },
   "source": [
    "# Step 15: Sanity Check\n",
    "\n",
    "Let us now check the model that we just trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1767585024455,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "6rXx16M4-i8j",
    "outputId": "0f86134c-9419-4548-9429-c8a01f552357"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch to Inference Mode\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767585026530,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "KDUXcF5WCVAQ"
   },
   "outputs": [],
   "source": [
    "test_question = \"Write a function to find the volume of a sphere given its radius.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767585026733,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "pheP84NQCZ0U"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": test_question},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1767585027283,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "yzUBfniDCdY2"
   },
   "outputs": [],
   "source": [
    "# Tokenize (Exactly as before)\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ")\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 6064,
     "status": "ok",
     "timestamp": 1767585033556,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Ypr3vQCmCevo"
   },
   "outputs": [],
   "source": [
    "# Generate\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.8, # Slight creativity to encourage reasoning\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1767585033579,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "TkVWR8DUCkV2",
    "outputId": "b6d0856d-2c5d-4b09-cc8a-01ac6f6521cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL MODEL OUTPUT ===\n",
      "<START_WORKING_OUT>\n",
      "The formula for the volume of a sphere is V = (4/3) * Ï€ * rÂ³.\n",
      "We need a function that takes the radius r as input and returns the volume.\n",
      "We can use the math module for the value of Ï€.\n",
      "The function should handle positive radius values.\n",
      "We'll return the calculated volume as a float.\n",
      "</END_WORKING_OUT>\n",
      "<SOLUTION>\n",
      "import math\n",
      "\n",
      "def sphere_volume(radius):\n",
      "    \"\"\"\n",
      "    Calculate the volume of a sphere given its radius.\n",
      "    \n",
      "    Parameters:\n",
      "    radius (float): The radius of the sphere.\n",
      "    \n",
      "    Returns:\n",
      "    float: The volume of the sphere.\n",
      "    \"\"\"\n",
      "    if radius < 0:\n",
      "        raise ValueError(\"Radius must be non-negative\")\n",
      "    return (4/3) * math.pi * (radius ** 3)\n",
      "</SOLUTION>\n"
     ]
    }
   ],
   "source": [
    "# 5. Decode (Slicing input_len just like before)\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "generated_text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== FINAL MODEL OUTPUT ===\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KycC-7u-DdY9"
   },
   "source": [
    "# Step 16: Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1767585112526,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "AwIpGowdDfgG"
   },
   "outputs": [],
   "source": [
    "MODEL_OUT = \"models/qwen3-4b-grpo-final-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1767585113319,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Hxf16u_-DtKQ"
   },
   "outputs": [],
   "source": [
    "model.save_lora(MODEL_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1767585113478,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "HKiRos6BDych",
    "outputId": "a8a8ef52-ae50-4ba8-fe9f-a2eb5dcca989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/qwen3-4b-grpo-final-2/tokenizer_config.json',\n",
       " 'models/qwen3-4b-grpo-final-2/special_tokens_map.json',\n",
       " 'models/qwen3-4b-grpo-final-2/chat_template.jinja',\n",
       " 'models/qwen3-4b-grpo-final-2/vocab.json',\n",
       " 'models/qwen3-4b-grpo-final-2/merges.txt',\n",
       " 'models/qwen3-4b-grpo-final-2/added_tokens.json',\n",
       " 'models/qwen3-4b-grpo-final-2/tokenizer.json')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(MODEL_OUT)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM0233NXl609H4brWGN7Sgg",
   "gpuType": "H100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
