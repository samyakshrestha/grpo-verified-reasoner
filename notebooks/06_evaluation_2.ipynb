{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwCYejay3dmd"
   },
   "source": [
    "# Notebook: HumanEval Evaluation\n",
    "\n",
    "This notebook performs the **final evaluation phase** of the GRPO verifiable-reward coding project using the **HumanEval benchmark**.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Evaluate and compare the functional correctness of multiple models trained under different regimes:\n",
    "\n",
    "- **Base model** (no fine-tuning)\n",
    "- **SFT model** (supervised fine-tuning warm-up)\n",
    "- **GRPO model** (verifiable-reward reinforcement learning)\n",
    "\n",
    "Each model is evaluated in both:\n",
    "- **Non-CoT mode** (direct code completion)\n",
    "- **CoT mode** (reasoning + solution, schema-constrained)\n",
    "\n",
    "The goal is to measure **pass@1 performance** under a **strict execution harness**, ensuring all outputs are runnable, well-formed Python.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Protocol\n",
    "\n",
    "1. **Prompt Construction**\n",
    "   - Non-CoT: model completes the function body directly.\n",
    "   - CoT: model is instructed to emit reasoning followed by a `<SOLUTION>` block.\n",
    "\n",
    "2. **Batch Generation**\n",
    "   - All HumanEval tasks are generated using **vLLM batch inference** for speed and determinism.\n",
    "   - Stop tokens are applied to prevent over-generation.\n",
    "\n",
    "3. **Output Sanitization**\n",
    "   A custom `cleaner` function enforces:\n",
    "   - Strict `<SOLUTION>` extraction (CoT only)\n",
    "   - Markdown fence removal (```python … ```)\n",
    "   - Redundant `def` removal\n",
    "   - Docstring stripping\n",
    "   - Robust indentation normalization compatible with the HumanEval harness\n",
    "\n",
    "4. **Execution-Based Scoring**\n",
    "   - Each completion is executed against the official HumanEval tests.\n",
    "   - Results are aggregated into a single results table.\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics\n",
    "\n",
    "- **pass@1**: fraction of problems solved correctly on the first attempt\n",
    "- Results are reported per model and per generation mode (CoT / non-CoT)\n",
    "\n",
    "Duplicate rows from batched generation are explicitly deduplicated before scoring.\n",
    "\n",
    "---\n",
    "\n",
    "## Output Artifacts\n",
    "\n",
    "- `*_non_cot.jsonl` — raw non-CoT completions\n",
    "- `*_cot.jsonl` — raw CoT completions\n",
    "- `df_results` — consolidated evaluation table with pass@1 scores\n",
    "\n",
    "These artifacts form the **final empirical evidence** for assessing GRPO effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "## Result Summary\n",
    "\n",
    "- Generation stability issues (empty outputs, truncation) were resolved by:\n",
    "  - Removing aggressive repetition penalties\n",
    "  - Correcting stop-token logic\n",
    "  - Fixing markdown and tag-handling bugs in the cleaner\n",
    "- Final runs achieved **near-complete coverage** (≤1 empty completion out of 164)\n",
    "- GRPO shows measurable gains over Base and SFT under controlled conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMEo3HOH-slR"
   },
   "source": [
    "# Step 1: Mounting Google Drive and Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1768119483472,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "yrrwKNmg-lNR",
    "outputId": "67172365-d1b6-4450-f6df-d35fbb8f3307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/grpo-verified-reasoner\n",
      "data\t\t\t      LICENSE\t outputs    unsloth_compiled_cache\n",
      "grpo_trainer_lora_model       models\t README.md  _unsloth_sentencepiece_temp\n",
      "huggingface_tokenizers_cache  notebooks  src\t    wandb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd /content/drive/MyDrive/grpo-verified-reasoner\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0q0FVqc67uRU"
   },
   "outputs": [],
   "source": [
    "# Install UV (Faster pip)\n",
    "!pip install --upgrade -qqq uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1768119492926,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "ZKwjGl3V7zI1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1768119492937,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "imojZdcn7ztn"
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1768119492939,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "5Y5vd4i8712_"
   },
   "outputs": [],
   "source": [
    "# os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1768119493860,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "vAQLD8-a9blY"
   },
   "outputs": [],
   "source": [
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install -q unsloth vllm human-eval tqdm\n",
    "else:\n",
    "    # Version matching for Colab GPUs\n",
    "    try:\n",
    "        import numpy, PIL\n",
    "        get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "        get_pil   = f\"pillow=={PIL.__version__}\"\n",
    "    except Exception:\n",
    "        get_numpy, get_pil = \"numpy\", \"pillow\"\n",
    "\n",
    "    try:\n",
    "        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "    except Exception:\n",
    "        is_t4 = False\n",
    "\n",
    "    # A100/H100: vllm 0.10.2, T4: vllm 0.9.2 + pinned triton\n",
    "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
    "\n",
    "    !uv pip install -qqq --upgrade \\\n",
    "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers tqdm human-eval\n",
    "    !uv pip install -qqq {get_triton}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768119493866,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "f4hqdCI59tMU"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "import textwrap\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from vllm import SamplingParams\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from human_eval.data import read_problems, write_jsonl\n",
    "from human_eval.evaluation import evaluate_functional_correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1768117993511,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "3_b5DzuU-S6z",
    "outputId": "3919c6e0-c739-4bce-a1b0-9ea715d2c573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-33b9d626-6ba6-d058-aec3-b7e7ebe44803)\n"
     ]
    }
   ],
   "source": [
    "SEED = 3407\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWuA9pSe-0M8"
   },
   "source": [
    "# Step 2: Verifying GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1768117994653,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "KmlB9Uie-xLj",
    "outputId": "50349990-ae71-4b81-8829-a55c88fcf20d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA7FVu96-anB"
   },
   "source": [
    "# Step 3: Setting Up the Main Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1768119507780,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "xjIWuUg0-aC4"
   },
   "outputs": [],
   "source": [
    "# HumanEval evaluation settings\n",
    "N_SAMPLES_PER_PROBLEM = 1          # pass@1 by default; raise to 5 or 10 if you later want pass@k\n",
    "MAX_NEW_TOKENS_NON_COT = 1024       # Non-CoT completions are usually short (function body)\n",
    "MAX_NEW_TOKENS_COT = 2048           # CoT can be longer due to tags + full function\n",
    "\n",
    "TEMP_NON_COT = 0.0                # low sampling noise; stable for benchmarking\n",
    "TEMP_COT = 0.0                     # encourages exploration under schema (optional)\n",
    "\n",
    "TOP_P = 0.95\n",
    "MIN_P = 0.10\n",
    "\n",
    "# Stop guards (prevent rambling without clipping typical solutions)\n",
    "STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1768119508302,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "SN2W2UUc-XgA"
   },
   "outputs": [],
   "source": [
    "# Output paths (keep all artifacts under one folder)\n",
    "EVAL_DIR = \"data/evaluation\"\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1768119509780,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "fZp2YS7SABnS"
   },
   "outputs": [],
   "source": [
    "# Model paths / identifiers\n",
    "BASE_MODEL_PATH = \"unsloth/Qwen3-4B-Base\"\n",
    "SFT_MODEL_PATH  = \"models/qwen3-4b-sft-merged-f32\"\n",
    "GRPO_MODEL_PATH = \"models/qwen3-4b-grpo-merged-f32-final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768119510035,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "4ezJrKFY8lGR"
   },
   "outputs": [],
   "source": [
    "COT_SYSTEM_PROMPT_HUMANEVAL = \"\"\"You are a code-generation engine.\n",
    "You must output your response in the following exact format:\n",
    "<START_WORKING_OUT>\n",
    "Concise reasoning steps required to solve the problem.\n",
    "</END_WORKING_OUT>\n",
    "<SOLUTION>\n",
    "Valid Python code only.\n",
    "</SOLUTION>\n",
    "Do not output anything outside these tags.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFeCRur7Aa7k"
   },
   "source": [
    "# Step 4: Loading HumanEval Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1768119512680,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "5BicgVzbAVcf",
    "outputId": "0382d136-3e29-4e0b-e084-a78809cfa457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HumanEval problems: 164\n",
      "Example task_id: HumanEval/0\n",
      "\n",
      "--- Prompt Preview ---\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problems = read_problems()  # dict: {task_id: {\"prompt\":..., \"test\":..., \"entry_point\":...}}\n",
    "task_ids = list(problems.keys())\n",
    "\n",
    "print(f\"Loaded HumanEval problems: {len(task_ids)}\")\n",
    "print(\"Example task_id:\", task_ids[0])\n",
    "print(\"\\n--- Prompt Preview ---\")\n",
    "print(problems[task_ids[0]][\"prompt\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0hbt-vjEf-y"
   },
   "source": [
    "# Step 5: Prompt Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768119513676,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "t6ZmWRrRAjGh"
   },
   "outputs": [],
   "source": [
    "def build_non_cot_prompt(problem: dict) -> str:\n",
    "    \"\"\"\n",
    "    Non-CoT: HumanEval-style continuation.\n",
    "    We provide ONLY the HumanEval prompt.\n",
    "    The harness will prepend this prompt again during execution.\n",
    "    \"\"\"\n",
    "    return problem[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1768119513877,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "na1taHI7Emmv"
   },
   "outputs": [],
   "source": [
    "def build_cot_prompt(problem: dict, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    CoT: Uses the same chat template distribution as SFT/GRPO training.\n",
    "    Returns a fully formatted ChatML prompt string.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": COT_SYSTEM_PROMPT_HUMANEVAL},\n",
    "        {\"role\": \"user\", \"content\": problem[\"prompt\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_FSvWfHE24y"
   },
   "source": [
    "# Step 6: Output Post-processing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1768119514780,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "mHbWyt3kH0PO"
   },
   "outputs": [],
   "source": [
    "# Stop strings (for safety against rambling)\n",
    "# These are conservative: they stop the model from starting a NEW definition / class / main block.\n",
    "STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]\n",
    "\n",
    "SOLUTION_RE = re.compile(r\"<SOLUTION>(.*?)</SOLUTION>\", re.DOTALL | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1768119515028,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "VN1M8SizH0Ne"
   },
   "outputs": [],
   "source": [
    "def extract_cot_solution(completion_only_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract code from <SOLUTION>...</SOLUTION>.\n",
    "    Returns \"\" on failure (schema violation → harness will fail, which is correct behavior).\n",
    "    \"\"\"\n",
    "    m = SOLUTION_RE.search(completion_only_text)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    return m.group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768119515166,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "LS7QqhbiH0LJ"
   },
   "outputs": [],
   "source": [
    "def truncate_on_stop_strings(text: str, stop_strings: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Stops the completion if it begins a new unrelated block (def/class/main).\n",
    "    This reduces harness crashes from rambling continuations.\n",
    "    \"\"\"\n",
    "    cut = len(text)\n",
    "    for s in stop_strings:\n",
    "        idx = text.find(s)\n",
    "        if idx != -1:\n",
    "            cut = min(cut, idx)\n",
    "    return text[:cut].rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1768120371635,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Jjhfpht0CGdF"
   },
   "outputs": [],
   "source": [
    "def cleaner(code: str, entry_point: str, use_cot: bool) -> str:\n",
    "    \"\"\"\n",
    "    Cleans model completions for HumanEval.\n",
    "    Handles GRPO models that redundantly define the entry function.\n",
    "    Preserves helper functions and relative nesting.\n",
    "    REMOVED: One-liner split logic to prevent signature parsing errors.\n",
    "    \"\"\"\n",
    "    # A. EXTRACT CoT (Standard): Extract code enclosed within <SOLUTION> tags if CoT is enabled.\n",
    "    if use_cot:\n",
    "        if \"<SOLUTION>\" in code:\n",
    "            code = code.split(\"<SOLUTION>\")[-1]\n",
    "            if \"</SOLUTION>\" in code:\n",
    "                code = code.split(\"</SOLUTION>\")[0]\n",
    "        else:\n",
    "            # If CoT is requested but tags are missing, return empty to indicate failure.\n",
    "            return \"\"\n",
    "\n",
    "    # B. REMOVE MARKDOWN & DOCSTRINGS: Strip markdown fences and docstrings.\n",
    "    code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    # Remove docstrings early to prevent them from messing up indentation detection.\n",
    "    code = re.sub(r'(\\s*(\"\"\"|\"\\\"\\\")[[\\s\\S]]*?\\2)', '', code, count=1)\n",
    "\n",
    "    lines = code.split('\\n')\n",
    "    filtered_lines = []\n",
    "\n",
    "    # C. PRE-FILTERING: Remove 'from typing' imports, which are noise in HumanEval.\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"from typing\"):\n",
    "            continue\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "    lines = filtered_lines\n",
    "\n",
    "    # D. SURGICAL DEDENT LOGIC: Adjust indentation based on function definition.\n",
    "    # We look for the line defining the entry point to decide our strategy.\n",
    "    def_pattern = re.compile(rf\"^\\s*def\\s+{re.escape(entry_point)}(\\s*\\(|\\s*:)\")\n",
    "\n",
    "    header_index = -1\n",
    "    for i, line in enumerate(lines):\n",
    "        if def_pattern.match(line):\n",
    "            header_index = i\n",
    "            break\n",
    "\n",
    "    final_lines = []\n",
    "\n",
    "    if header_index != -1:\n",
    "        # --- CASE 1: Redundant Header Found (GRPO Style) ---\n",
    "\n",
    "        # 1. Keep everything BEFORE the header as-is (Helpers).\n",
    "        pre_header_lines = lines[:header_index]\n",
    "\n",
    "        # 2. Process everything AFTER the header (The Body).\n",
    "        # We strictly discard the header line itself.\n",
    "        # [REMOVED] The logic that looked for code on the same line.\n",
    "        post_header_lines = lines[header_index+1:]\n",
    "\n",
    "        # 3. Calculate Body Indentation from the first valid body line.\n",
    "        body_indent = 0\n",
    "        for line in post_header_lines:\n",
    "            if line.strip():\n",
    "                body_indent = len(line) - len(line.lstrip())\n",
    "                break\n",
    "\n",
    "        # 4. Dedent the body lines by that specific amount.\n",
    "        normalized_body = []\n",
    "        for line in post_header_lines:\n",
    "            if not line.strip():\n",
    "                normalized_body.append(\"\")\n",
    "                continue\n",
    "            # Shift left, but don't go negative.\n",
    "            current = len(line) - len(line.lstrip())\n",
    "            new_indent = max(0, current - body_indent)\n",
    "            normalized_body.append(\" \" * new_indent + line.lstrip())\n",
    "\n",
    "        final_lines = pre_header_lines + normalized_body\n",
    "\n",
    "    else:\n",
    "        # --- CASE 2: No Header Found (SFT Style / Nested) ---\n",
    "        # Normalize the entire block based on the first line's indentation.\n",
    "        global_indent = 0\n",
    "        for line in lines:\n",
    "            if line.strip() and not line.strip().startswith((\"import \", \"from \")):\n",
    "                global_indent = len(line) - len(line.lstrip())\n",
    "                break\n",
    "\n",
    "        for line in lines:\n",
    "            if not line.strip():\n",
    "                final_lines.append(\"\")\n",
    "                continue\n",
    "            current = len(line) - len(line.lstrip())\n",
    "            new_indent = max(0, current - global_indent)\n",
    "            final_lines.append(\" \" * new_indent + line.lstrip())\n",
    "\n",
    "    # E. FINAL GLOBAL INDENT: Indent everything by 4 spaces for harness compatibility.\n",
    "    result = \"\\n\".join(final_lines)\n",
    "    return textwrap.indent(result, '    ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LP-Ekw0FNoQ"
   },
   "source": [
    "# Step 7: Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768119521075,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "3WT5QxUSFW63"
   },
   "outputs": [],
   "source": [
    "# Helper: syntax validator\n",
    "def assert_valid_python(code: str):\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        raise AssertionError(f\"Invalid Python code:\\n{code}\\n\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768119523378,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "7w9btY8aFgxh"
   },
   "outputs": [],
   "source": [
    "# Test runner\n",
    "def run_test(name, raw, entry_point, use_cot=True):\n",
    "    print(f\"\\n=== TEST: {name} ===\")\n",
    "    cleaned = cleaner(raw, entry_point=entry_point, use_cot=use_cot)\n",
    "    print(cleaned)\n",
    "    assert_valid_python(\n",
    "        f\"def {entry_point}():\\n{cleaned}\"\n",
    "    )\n",
    "    print(\"Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1768119523531,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "4vpW8nleFviU",
    "outputId": "a71d12e7-0815-4617-e23a-ab52f0264ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST: Non-CoT simple continuation ===\n",
      "\n",
      "    return x + 1\n",
      "\n",
      "Passed\n"
     ]
    }
   ],
   "source": [
    "# 1. Non-CoT baseline (pure continuation)\n",
    "run_test(\n",
    "    name=\"Non-CoT simple continuation\",\n",
    "    raw=\"\"\"\n",
    "return x + 1\n",
    "\"\"\",\n",
    "    entry_point=\"f\",\n",
    "    use_cot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1768119523782,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "-Dva6dQHF8D5",
    "outputId": "5d673932-0a96-4d88-9cee-c04755e473f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST: CoT redundant entry point ===\n",
      "\n",
      "    return x + 1\n",
      "\n",
      "Passed\n"
     ]
    }
   ],
   "source": [
    "# 2. CoT with redundant entry point\n",
    "run_test(\n",
    "    name=\"CoT redundant entry point\",\n",
    "    raw=\"\"\"\n",
    "<SOLUTION>\n",
    "def f(x):\n",
    "    return x + 1\n",
    "</SOLUTION>\n",
    "\"\"\",\n",
    "    entry_point=\"f\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1768119524207,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "7eKVH3w9GHtm",
    "outputId": "145faff7-42e7-4f6a-96c5-e4c87bc32d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST: Helper before entry point ===\n",
      "\n",
      "    def helper(x):\n",
      "        return x * 2\n",
      "\n",
      "    return helper(n)\n",
      "\n",
      "Passed\n"
     ]
    }
   ],
   "source": [
    "# 3. Helper BEFORE entry point (critical bug case)\n",
    "run_test(\n",
    "    name=\"Helper before entry point\",\n",
    "    raw=\"\"\"\n",
    "<SOLUTION>\n",
    "def helper(x):\n",
    "    return x * 2\n",
    "\n",
    "def f(n):\n",
    "    return helper(n)\n",
    "</SOLUTION>\n",
    "\"\"\",\n",
    "    entry_point=\"f\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1768119524481,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "FzvDSRGlGN07",
    "outputId": "08e3a8d9-687e-47bb-b63b-682c27a3d48e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST: Helper inside entry point ===\n",
      "\n",
      "    def digit_sum(n):\n",
      "        return sum(map(int, str(abs(n))))\n",
      "    return sorted(nums, key=digit_sum)\n",
      "\n",
      "Passed\n"
     ]
    }
   ],
   "source": [
    "# 4. Helper INSIDE entry point (must remain untouched)\n",
    "run_test(\n",
    "    name=\"Helper inside entry point\",\n",
    "    raw=\"\"\"\n",
    "<SOLUTION>\n",
    "def f(nums):\n",
    "    def digit_sum(n):\n",
    "        return sum(map(int, str(abs(n))))\n",
    "    return sorted(nums, key=digit_sum)\n",
    "</SOLUTION>\n",
    "\"\"\",\n",
    "    entry_point=\"f\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1768119524903,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "PqMIrkRBGgEv",
    "outputId": "a9f44094-6d29-4465-b131-5484f45d2b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST: Mixed helpers and body ===\n",
      "\n",
      "    def helper1(x):\n",
      "        return x + 1\n",
      "\n",
      "    def helper2(x):\n",
      "        return x * 2\n",
      "\n",
      "    a = helper1(n)\n",
      "    b = helper2(a)\n",
      "    return b\n",
      "\n",
      "Passed\n"
     ]
    }
   ],
   "source": [
    "# 5. Mixed helpers + logic (stress test)\n",
    "run_test(\n",
    "    name=\"Mixed helpers and body\",\n",
    "    raw=\"\"\"\n",
    "<SOLUTION>\n",
    "def helper1(x):\n",
    "    return x + 1\n",
    "\n",
    "def helper2(x):\n",
    "    return x * 2\n",
    "\n",
    "def f(n):\n",
    "    a = helper1(n)\n",
    "    b = helper2(a)\n",
    "    return b\n",
    "</SOLUTION>\n",
    "\"\"\",\n",
    "    entry_point=\"f\",\n",
    "    use_cot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO7zEeCjK8le"
   },
   "source": [
    "# Step 8: Defining Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1768119532077,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "yzrnQkGXyUtB"
   },
   "outputs": [],
   "source": [
    "# This is the new function that accounts for the fp32 models that we just recently merged\n",
    "# We found out that the reason why the SFT and the GRPO models were giving the same outputs was because the models were being loaded in low-repcision\n",
    "# Which effectively destroyed the (tiny) updates that we made to the weights via SFT and GRPO\n",
    "def evaluate_model(\n",
    "    model_path: str,\n",
    "    problems: dict,\n",
    "    task_ids: list[str],\n",
    "    *,\n",
    "    use_cot: bool,\n",
    "    output_jsonl: str,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float,\n",
    "    top_p: float = 0.95,\n",
    "    min_p: float = 0.10,\n",
    "    load_in_4bit: bool = False, # Controls 4-bit quantization for model loading\n",
    "    dtype = torch.float16,      # Specifies the floating point precision (e.g., FP16, BF16)\n",
    "    gpu_memory_utilization: float = 0.8, # Sets GPU memory allocation for vLLM\n",
    "):\n",
    "    # Log current model and mode for clarity\n",
    "    print(f\"\\n Loading: {model_path} | Mode: {'CoT' if use_cot else 'Non-CoT'}\")\n",
    "    print(f\" Precision: {dtype} | 4-Bit Quantization: {load_in_4bit}\")\n",
    "\n",
    "    # Clear GPU memory before loading a new model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load the language model with specified precision and vLLM for inference\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_path,\n",
    "        max_seq_length=4096,\n",
    "        load_in_4bit=load_in_4bit, # Use 4-bit quantization if specified\n",
    "        dtype=dtype,               # Load model with the desired data type\n",
    "        fast_inference=True,       # Enable vLLM for faster inference\n",
    "        gpu_memory_utilization=gpu_memory_utilization,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Prepare model for inference mode\n",
    "\n",
    "    prompts = []\n",
    "    print(f\" Preparing {len(task_ids)} prompts...\")\n",
    "    # Generate prompts for each task based on CoT or Non-CoT mode\n",
    "    for task_id in task_ids:\n",
    "        problem = problems[task_id]\n",
    "        if use_cot:\n",
    "            prompt_text = build_cot_prompt(problem, tokenizer)\n",
    "        else:\n",
    "            prompt_text = build_non_cot_prompt(problem)\n",
    "        prompts.append(prompt_text)\n",
    "\n",
    "    # Define stop tokens based on generation mode (CoT or Non-CoT)\n",
    "    if use_cot:\n",
    "        stop_tokens = [\"</SOLUTION>\"]\n",
    "    else:\n",
    "        stop_tokens = [\"\\nclass\", \"\\nif __name__\", \"\\nprint\", \"\\ndef \"]\n",
    "\n",
    "    # Configure sampling parameters for vLLM generation\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        min_p=min_p,\n",
    "        max_tokens=max_new_tokens,\n",
    "        stop=stop_tokens,\n",
    "    )\n",
    "\n",
    "    print(f\" Running vLLM Batch Generation...\")\n",
    "    # Perform batch generation using the loaded model and sampling parameters\n",
    "    outputs = model.fast_generate(prompts, sampling_params=sampling_params)\n",
    "\n",
    "    samples = []\n",
    "    # Process each generated output\n",
    "    for i, task_id in enumerate(task_ids):\n",
    "        problem = problems[task_id]\n",
    "        completion_only = outputs[i].outputs[0].text # Extract raw completion text\n",
    "\n",
    "        # Clean and format the completion using the 'cleaner' function\n",
    "        completion = cleaner(completion_only, problem[\"entry_point\"], use_cot)\n",
    "\n",
    "        # Store task_id, original prompt, and cleaned completion\n",
    "        samples.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"prompt\": problem[\"prompt\"],\n",
    "            \"completion\": completion\n",
    "        })\n",
    "\n",
    "    # Write all processed samples to a JSONL file\n",
    "    write_jsonl(output_jsonl, samples)\n",
    "    print(f\" Saved {len(samples)} samples to: {output_jsonl}\")\n",
    "\n",
    "    # Clean up model and tokenizer from GPU memory\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsDfO98zwS48"
   },
   "source": [
    "# Step 9: Generating JSONL Files for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1768119532707,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "czY25ERRwcWH"
   },
   "outputs": [],
   "source": [
    "GEN_DIR = Path(EVAL_DIR) / \"generations_run_2\"\n",
    "GEN_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ffa1ef73eb1a4c689634d37a4b7d9d9a",
      "969c191db0f747ae91a2803ce27c600c",
      "12ff255b88974d6db90bc42859c89e3b",
      "5f6a96e6b8224ba1af3a25d3fcbb2466",
      "dc53f41d6b4c4b39a11ae70d0931985d",
      "ee1b73f3f67848e2aacada8b634dcd02",
      "8c4cb390195a4f61a0c88a6a513fc034",
      "bef4447de8fd457bb0e3ea69d0f1170a",
      "6589065f0b3b49559dbfc73bc64ff7ea",
      "18bfd087f2724d91a32eb7c3c6ab12a0",
      "f9e9cd025f3240b58b0b8d76f0e56d2a",
      "be63d4ebf71540498277abda0c61b810",
      "d995d63dc50146a987eea08bde2d3d4e",
      "74f3241f41c843ab93dbdcafedd1cf30",
      "96a745441750468e9e2cc82dc033ad70",
      "933d6beb695e48c8be764c9b906d4ecd",
      "d875cdce2ab8489ab38525b05a348545",
      "742173a4ca6b4f0ab71597d2b93892be",
      "783fde9dbbc94717acd0da07257ce3a1",
      "b31160ab7b7c4295811d40de68fe83e8",
      "9b30e85c4f704f42a1bfb7fc7434fddc",
      "740f395654ca4c8a92a3405d10b89d16",
      "01d7ee4c7a884cae9e5b9dec5ad10150",
      "6eaa4745ad214edf969a71fac7e512ef",
      "f7e3d28d798f451287eb49b493b23ed1",
      "a0fb68ab338644aabe51ad72b1f15afb",
      "2bab39e4d2704f71a08478bc64553871",
      "df6b0b569580423195fff6df9af09a0b",
      "4a5bef7776684a8b82e6f88f78b6da9c",
      "2be6b79c594b43cfa9a8e057a896515c",
      "1d6992191799463c83f6440b3db11429",
      "69773b9eb6db492c9de728253e20b79c",
      "190b524ea425490983b1a4b961ec672f",
      "2d22f4a5daa74f6bbae736b2d5e200e6",
      "8efd094abf2e47c784b2f5c6c3172cf1",
      "b7e107f450234650950f996deb06fcc7",
      "045a8697005e478fa58cfa5d929756c3",
      "c3d51b33e2664651b7ae29693f13e2c0",
      "8ba63e608db94bed9970f4209977ed04",
      "c04f643f995647b2ad0668a44adc5665",
      "015ee9b7025f478ab0cad27f4d5acc49",
      "5e7aa1cd4ab94f619c5e94acce699908",
      "9bf7deb3d2894008858599655ad5e4c9",
      "cf16c960e6434cdd853bea3353a4ccb4"
     ]
    },
    "executionInfo": {
     "elapsed": 118926,
     "status": "ok",
     "timestamp": 1767590636329,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "s2O5o6iww1uQ",
    "outputId": "d828ee37-7817-4048-998a-3546d5397781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: unsloth/Qwen3-4B-Base | Mode: Non-CoT\n",
      "INFO 01-05 05:21:59 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-05 05:21:59 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.1: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 69.6%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 52.34 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-05 05:22:05 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.6960016128672332, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n",
      "INFO 01-05 05:22:22 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-05 05:22:22 [__init__.py:1815] Using max model len 4096\n",
      "WARNING 01-05 05:22:22 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n",
      "INFO 01-05 05:22:24 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-05 05:22:24 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 01-05 05:22:26 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n",
      "INFO 01-05 05:22:26 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-05 05:22:26 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-05 05:22:27 [gpu_model_runner.py:2338] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n",
      "INFO 01-05 05:22:27 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 01-05 05:22:27 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "INFO 01-05 05:22:27 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 01-05 05:22:28 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "INFO 01-05 05:22:29 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa1ef73eb1a4c689634d37a4b7d9d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be63d4ebf71540498277abda0c61b810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-05 05:22:30 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-05 05:22:31 [gpu_model_runner.py:2392] Model loading took 3.3825 GiB and 2.671494 seconds\n",
      "INFO 01-05 05:22:47 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/ca83b4ba7b/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-05 05:22:47 [backends.py:550] Dynamo bytecode transform time: 14.62 s\n",
      "INFO 01-05 05:22:53 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.557 s\n",
      "INFO 01-05 05:22:57 [monitor.py:34] torch.compile takes 14.62 s in total\n",
      "INFO 01-05 05:22:59 [gpu_worker.py:298] Available KV cache memory: 50.98 GiB\n",
      "INFO 01-05 05:22:59 [kv_cache_utils.py:864] GPU KV cache size: 371,248 tokens\n",
      "INFO 01-05 05:22:59 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 90.64x\n",
      "INFO 01-05 05:22:59 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:11<00:00,  3.12it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:06<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-05 05:23:17 [gpu_model_runner.py:3118] Graph capturing finished in 17 secs, took 1.21 GiB\n",
      "INFO 01-05 05:23:17 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 17 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-05 05:23:19 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.6960016128672332, 55.21 GiB). Actual usage is 3.38 GiB for weight, 0.82 GiB for peak activation, 0.02 GiB for non-torch memory, and 1.21 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=53281379123` to fit into requested memory, or `--kv-cache-memory=78605918208` to fully utilize gpu memory. Current kv cache memory in use is 54743094067 bytes.\n",
      "INFO 01-05 05:23:19 [core.py:218] init engine (profile, create kv cache, warmup model) took 47.65 seconds\n",
      "INFO 01-05 05:23:20 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-05 05:23:20 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['norm', 'norm1', 'norm2', 'post_attention_layernorm', 'attention_norm', 'ffn_norm', 'layer_norm2', 'q_norm', 'post_layernorm', 'layer_norm1', 'post_feedforward_layernorm', 'input_layernorm', 'pre_feedforward_layernorm', 'k_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/qwen3-4b-base-unsloth-bnb-4bit and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['norm', 'norm1', 'norm2', 'post_attention_layernorm', 'attention_norm', 'ffn_norm', 'layer_norm2', 'cross_attn_post_attention_layernorm', 'q_norm', 'post_layernorm', 'layer_norm1', 'post_feedforward_layernorm', 'cross_attn_input_layernorm', 'input_layernorm', 'pre_feedforward_layernorm', 'k_norm']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d7ee4c7a884cae9e5b9dec5ad10150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d22f4a5daa74f6bbae736b2d5e200e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 164 samples to: data/evaluation/generations_run_2/base_non_cot.jsonl\n"
     ]
    }
   ],
   "source": [
    "# BASE MODEL (Non-CoT)\n",
    "evaluate_model(\n",
    "    model_path=BASE_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=False,\n",
    "    output_jsonl=str(GEN_DIR / \"base_non_cot.jsonl\"),\n",
    "    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n",
    "    temperature=TEMP_NON_COT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "cd58f02176d64b9a85a4eff2c18532ca",
      "56e0459b6ac647b7a94a96800f9a3731",
      "f50603009ca14391a2ad66a9eb30ce14",
      "4de0c93f95dc421eb33dfe5d52e9a78b",
      "73e19a5a5e06410cbf9c9317224953f7",
      "d6c7bf2ef4d94a7e852776fc1e4c998d",
      "1f3dc606fdc446cfbc3e97f8121145e3",
      "7fc6adac53e34b209aa21d335072ace7",
      "7b1522080bb242d082d80298dfc7fb85",
      "6fb92c8304ac41e7999d22b959601cb6",
      "b00bbf363a634c5a98a6b193ee256ebb",
      "d0344ecb8f204a03a4484d5f22929f4a",
      "7d23daed76fa4b8a9a4ebc871b11627f",
      "caedba32f9364f328fc1ae6786e94ac6",
      "59fefc1d0a5d4d658f15fda7ffb32faf",
      "af96d99975e1412481b28854e2cf10cd",
      "5b3e2607032447aa9f0342e8bec5c65d",
      "273f9039fc7c4baaa1a23f23b226189f",
      "943bdbdc8b034a13af31b7bec2c1b779",
      "70b024c8ec994af7a2206bf4f7289a86",
      "9a004982b937461da77ae46e58b5b7e5",
      "70b08b1a857045aa8ebf0c07ab009cc8",
      "9b069a66f50948298a945b1fd36c6e24",
      "f7be467bf1e04cea837d061a3fe175c0",
      "be7cec24929144ddaa0f6cee611a4af9",
      "86b4dd16092a408ca3e851fb7ce7b52f",
      "a53c47eb6d5e4d67bbed7f2db5610a99",
      "3d3b8224e438466eb9e9d2cce0bccdf1",
      "218fdb87602d49ad883a075fb0712249",
      "4a5f2b0ed5214e4fa3256d6bd6a3949e",
      "325b3e9a1528458ea6c8e512ba276e47",
      "068e6ddcef154eeaa931a90d8bf5a107",
      "977b4b35a0304397984b5b01e88d049d",
      "f9b03da61d6c4b728747c17eccda1574",
      "e3f4cb6aec4b499aa31cc56854fa505a",
      "48b65fbf1e344b118a2c23b7e1ed0e7d",
      "45ef58ba8ae84c82ad0740deb318538a",
      "33c9be9a4b1b406fb3ed8b11f01a99e7",
      "eafd4c83b2c347e4827d065d867e2c5b",
      "00ae2df8d21843c6a2bdd32b2946b65d",
      "d9692f55d8ce4af1b91d5a4c39f1a4f1",
      "c67a3c1ce23646fc9ecf32508976a243",
      "481c7d6db0844ec69a1fc106b488a0ff",
      "85b037ac7a934604a16945193b9aec61"
     ]
    },
    "executionInfo": {
     "elapsed": 83157,
     "status": "ok",
     "timestamp": 1768119440070,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "vJifZdlN9rBY",
    "outputId": "0ae5a02b-aa66-494a-d027-2cbbce6959e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: models/qwen3-4b-sft-merged-f32 | Mode: Non-CoT\n",
      " Precision: torch.float16 | 4-Bit Quantization: False\n",
      "INFO 01-11 08:15:57 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-11 08:15:57 [vllm_utils.py:731] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading models/qwen3-4b-sft-merged-f32 with actual GPU utilization = 15.89%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 32.\n",
      "Unsloth: vLLM's KV Cache can use up to 5.54 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-11 08:16:04 [utils.py:328] non-default args: {'dtype': torch.float16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.15889204639455687, 'max_num_batched_tokens': 4096, 'max_num_seqs': 32, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'models/qwen3-4b-sft-merged-f32'}\n",
      "INFO 01-11 08:16:04 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 01-11 08:16:04 [__init__.py:2764] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-11 08:16:04 [__init__.py:1815] Using max model len 4096\n",
      "INFO 01-11 08:16:04 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "WARNING 01-11 08:16:04 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-11 08:16:05 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='models/qwen3-4b-sft-merged-f32', speculative_config=None, tokenizer='models/qwen3-4b-sft-merged-f32', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/qwen3-4b-sft-merged-f32, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "INFO 01-11 08:16:05 [gpu_model_runner.py:2338] Starting to load model models/qwen3-4b-sft-merged-f32...\n",
      "INFO 01-11 08:16:06 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd58f02176d64b9a85a4eff2c18532ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:16:16 [default_loader.py:268] Loading weights took 9.34 seconds\n",
      "INFO 01-11 08:16:17 [gpu_model_runner.py:2392] Model loading took 7.7976 GiB and 9.516278 seconds\n",
      "INFO 01-11 08:16:30 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/343b7bb7ec/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-11 08:16:30 [backends.py:550] Dynamo bytecode transform time: 12.35 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 504.92it/s, triton_poi_fused_view_6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:16:32 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 521.33it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 531.47it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 518.49it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 530.84it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 542.86it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 516.15it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 504.63it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 509.58it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 527.17it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 513.88it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 510.10it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 490.53it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 494.16it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 483.96it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 478.67it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 483.09it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 479.58it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 545.40it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 519.18it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 512.15it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 482.03it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 527.07it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 501.85it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 519.24it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 516.58it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 525.78it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 488.02it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 502.85it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 483.18it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 508.92it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 515.29it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 535.90it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 527.27it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 538.12it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 525.66it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 512.04it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:16:40 [backends.py:215] Compiling a graph for dynamic shape takes 8.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:16:43 [monitor.py:34] torch.compile takes 21.16 s in total\n",
      "INFO 01-11 08:16:45 [gpu_worker.py:298] Available KV cache memory: 4.44 GiB\n",
      "INFO 01-11 08:16:46 [kv_cache_utils.py:864] GPU KV cache size: 32,320 tokens\n",
      "INFO 01-11 08:16:46 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 7.89x\n",
      "INFO 01-11 08:16:46 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "INFO 01-11 08:16:46 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  5.63it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:16:49 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.05 GiB\n",
      "INFO 01-11 08:16:49 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 3 secs.\n",
      "INFO 01-11 08:16:49 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 3 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:16:51 [gpu_worker.py:391] Free memory on device (15.75/79.32 GiB) on startup. Desired GPU memory utilization is (0.15889204639455687, 12.6 GiB). Actual usage is 7.8 GiB for weight, 0.36 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=4555665919` to fit into requested memory, or `--kv-cache-memory=7938765312` to fully utilize gpu memory. Current kv cache memory in use is 4767478271 bytes.\n",
      "INFO 01-11 08:16:51 [core.py:218] init engine (profile, create kv cache, warmup model) took 34.05 seconds\n",
      "INFO 01-11 08:16:52 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-11 08:16:52 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'post_attention_layernorm', 'k_norm', 'layer_norm2', 'q_norm', 'post_layernorm', 'ffn_norm', 'norm2', 'norm1', 'pre_feedforward_layernorm', 'attention_norm', 'norm', 'input_layernorm', 'post_feedforward_layernorm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0344ecb8f204a03a4484d5f22929f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at models/qwen3-4b-sft-merged-f32 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'post_attention_layernorm', 'k_norm', 'layer_norm2', 'cross_attn_post_attention_layernorm', 'q_norm', 'post_layernorm', 'ffn_norm', 'cross_attn_input_layernorm', 'norm2', 'norm1', 'pre_feedforward_layernorm', 'attention_norm', 'norm', 'input_layernorm', 'post_feedforward_layernorm']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b069a66f50948298a945b1fd36c6e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b03da61d6c4b728747c17eccda1574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 164 samples to: data/evaluation/generations_run_2/sft_non_cot.jsonl\n"
     ]
    }
   ],
   "source": [
    "# SFT MODEL (Non-CoT) - - FP32 EVALUATION\n",
    "evaluate_model(\n",
    "    model_path=SFT_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=False,\n",
    "    output_jsonl=str(GEN_DIR / \"sft_non_cot.jsonl\"),\n",
    "    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n",
    "    temperature=TEMP_NON_COT,\n",
    "    load_in_4bit=False,    # Do not compress\n",
    "    dtype=torch.float16,   # Do not round off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "482f9d6ec573443ea4540148fc097c17",
      "84704d45e1c8424682aff747fd48e548",
      "bdc0a364a7fe46f698d54f9c0024d460",
      "a9179465dd94433cb127543d66d218fc",
      "24899c7391964e7a9e13a5629cc3004e",
      "dc5a85d7c9bf46fb84c5d31f67725670",
      "8e9ff1d876804b63a0755ddd975042aa",
      "34bdbe1c2e6e40dcad91bac7e8c4315c",
      "a016b20b9a674b569fa37856618805db",
      "ce276e9ded254220a3e3f19010837869",
      "adabb778794a47a981d161d68c009db7",
      "cb2a4e3dcbe54b67a3a62638c601949d",
      "2f5ef4f7ef524473a5aeae897e3fd30f",
      "05a8a3ac23224b87bc960a209872fa4f",
      "17c5a8f2ba0b46229f47c8c29ce4f26b",
      "080a14bb6bec4815b47a79a9a0313291",
      "22be655218b849b39f06d7ed0d4ace2a",
      "6a82bb6249d047c6a8b1e03785d142d0",
      "d29bc7cc67094511b761e982c7feec16",
      "dfe8ff48991946d88d95b6d73468513b",
      "bf649575d00e4da1bb48da24694f6784",
      "d065ff72902e414a817c8ae049ba6037",
      "9423945b197e431bb3ca91bb6814c415",
      "badd5ca28bbf422cb04284f476b2c494",
      "a0c2b21f6db145958719454035d256b3",
      "0612abc8a0e84fefbf48644a7514c2db",
      "902862a3e592489398695700985fdd9f",
      "533aa01b647846d49c4425a57eff55e0",
      "331fddc1cd2640e5975003d1c7196a50",
      "7744b319d49840659d59c23cc8964a7e",
      "f990ff6ed9844de88ea46fe8b40e8e53",
      "422c28085c5449c6a5e39a6d99c17686",
      "8df57bd3722843fa9df29e283d0320d5",
      "d64fd9d020c64a2b84425cc1733639e8",
      "a7fb525a2c6547119c3d40358eb8bdba",
      "2c3552b24e2841c89ce0eec433a0be6a",
      "b3e84b43b8044b758079e9fb5d5287eb",
      "3caa02474c464c91af1ba3f3f6d335a9",
      "0049eb83a31e43698a5b5b6c90e10819",
      "a3494fc3fb75472581e09f6bf55a643d",
      "6b2b57a154064e2c8953fcc117203dae",
      "76d60c498000424aa552bc4a44a9c1c1",
      "b8029e20f606477898c34b53ba3883e6",
      "34ed5676e49541209ac618a1d6b73421"
     ]
    },
    "executionInfo": {
     "elapsed": 468130,
     "status": "ok",
     "timestamp": 1768119349074,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "DFArBYRR6nvN",
    "outputId": "ec9b9165-b223-4337-fed8-3662056bca1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: models/qwen3-4b-sft-merged-f32 | Mode: CoT\n",
      " Precision: torch.float16 | 4-Bit Quantization: False\n",
      "INFO 01-11 08:08:02 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-11 08:08:02 [vllm_utils.py:731] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading models/qwen3-4b-sft-merged-f32 with actual GPU utilization = 79.54%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 56.03 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-11 08:08:09 [utils.py:328] non-default args: {'dtype': torch.float16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.7954304147054094, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'models/qwen3-4b-sft-merged-f32'}\n",
      "INFO 01-11 08:08:24 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:08:24 [__init__.py:2764] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-11 08:08:24 [__init__.py:1815] Using max model len 4096\n",
      "INFO 01-11 08:08:26 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-11 08:08:26 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-11 08:08:28 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='models/qwen3-4b-sft-merged-f32', speculative_config=None, tokenizer='models/qwen3-4b-sft-merged-f32', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/qwen3-4b-sft-merged-f32, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n",
      "INFO 01-11 08:08:29 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-11 08:08:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-11 08:08:29 [gpu_model_runner.py:2338] Starting to load model models/qwen3-4b-sft-merged-f32...\n",
      "INFO 01-11 08:08:29 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 01-11 08:08:29 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482f9d6ec573443ea4540148fc097c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:14:26 [default_loader.py:268] Loading weights took 356.38 seconds\n",
      "INFO 01-11 08:14:26 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-11 08:14:27 [gpu_model_runner.py:2392] Model loading took 7.8056 GiB and 357.023420 seconds\n",
      "INFO 01-11 08:14:41 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/0ac4129559/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-11 08:14:41 [backends.py:550] Dynamo bytecode transform time: 13.21 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 498.86it/s, triton_poi_fused_view_6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:14:43 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 481.87it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 478.25it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 497.68it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 528.10it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 542.99it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 496.89it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 510.95it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 524.24it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 541.27it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 554.06it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 547.77it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 530.53it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 535.07it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 553.91it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 544.92it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 524.00it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 513.14it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 575.22it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 548.99it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 530.69it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 525.58it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 523.95it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 520.31it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 559.78it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 511.69it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 496.57it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 520.06it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 555.40it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 539.94it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 551.23it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 560.87it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 561.77it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 548.18it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 528.49it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 543.20it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 542.52it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:14:52 [backends.py:215] Compiling a graph for dynamic shape takes 9.15 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:14:55 [monitor.py:34] torch.compile takes 22.36 s in total\n",
      "INFO 01-11 08:14:57 [gpu_worker.py:298] Available KV cache memory: 54.53 GiB\n",
      "INFO 01-11 08:14:57 [kv_cache_utils.py:864] GPU KV cache size: 397,072 tokens\n",
      "INFO 01-11 08:14:57 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 96.94x\n",
      "INFO 01-11 08:14:57 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:05<00:00,  5.92it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:15:07 [gpu_model_runner.py:3118] Graph capturing finished in 9 secs, took 0.65 GiB\n",
      "INFO 01-11 08:15:07 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 9 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:15:08 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.7954304147054094, 63.09 GiB). Actual usage is 7.81 GiB for weight, 0.74 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.65 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=57691607244` to fit into requested memory, or `--kv-cache-memory=74548069376` to fully utilize gpu memory. Current kv cache memory in use is 58551439564 bytes.\n",
      "INFO 01-11 08:15:08 [core.py:218] init engine (profile, create kv cache, warmup model) took 40.83 seconds\n",
      "INFO 01-11 08:15:09 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-11 08:15:09 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'post_attention_layernorm', 'k_norm', 'layer_norm2', 'q_norm', 'post_layernorm', 'ffn_norm', 'norm2', 'norm1', 'pre_feedforward_layernorm', 'attention_norm', 'norm', 'input_layernorm', 'post_feedforward_layernorm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2a4e3dcbe54b67a3a62638c601949d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at models/qwen3-4b-sft-merged-f32 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'post_attention_layernorm', 'k_norm', 'layer_norm2', 'cross_attn_post_attention_layernorm', 'q_norm', 'post_layernorm', 'ffn_norm', 'cross_attn_input_layernorm', 'norm2', 'norm1', 'pre_feedforward_layernorm', 'attention_norm', 'norm', 'input_layernorm', 'post_feedforward_layernorm']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9423945b197e431bb3ca91bb6814c415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64fd9d020c64a2b84425cc1733639e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 164 samples to: data/evaluation/generations_run_2/sft_cot.jsonl\n"
     ]
    }
   ],
   "source": [
    "# SFT MODEL (CoT) - FP32 EVALUATION\n",
    "evaluate_model(\n",
    "    model_path=SFT_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=True,\n",
    "    output_jsonl=str(GEN_DIR / \"sft_cot.jsonl\"),\n",
    "    max_new_tokens=MAX_NEW_TOKENS_COT,\n",
    "    temperature=TEMP_COT,\n",
    "    load_in_4bit=False,    # Do not compress\n",
    "    dtype=torch.float16,   # Do not round off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bd47c59ec165431ea980cff991947042",
      "6ec49f9d467741c7a1b40d2bd9fe3efe",
      "ac196bd87f004da9ad94f8ad2d56fb3a",
      "7145fa589bec45819c20f9b9d1f2638c",
      "71dde7c08324455480b3c93aac7ebbff",
      "4474e1d4d4854f65a7f593ee47ca8236",
      "97d315ecc6aa493f90e35ce14c30d5d1",
      "2d5cd2b38e44498d85e75838ad6fcd47",
      "43c4ad64fbe74916946f76c89e4662e6",
      "e479e2d78ce04725ba8690c0bccfe23b",
      "a225868eef1846268f4f08c20cd3bf97",
      "e10a50f3b5cb45fe851f04d36798cc17",
      "46fa90517f4748a8b0362a7b3fe33b99",
      "9bccf82bb11e4f4f9ce9f4ab61ce94d6",
      "996abf7d00de42af9743553f90182999",
      "ccd43ca0906242b9a696b5e7942387da",
      "47ddf080fab44117b4efe4f05c3e0686",
      "597fdce12a9849678cf49c6bb98eb82c",
      "e7748f5704cc411ab53f4f1725b6dc90",
      "fe20a4efc22344b3be13eb38f56cec60",
      "80e2a334e9524838a33880b6d84fa014",
      "1654ecb50ab64ff7b01accf3b1496875",
      "de0aa07b13674a5aa4903cae994e2945",
      "da3dee72f6d94c28928e8427bb9e9369",
      "74af72d1ed3442f69cfedc50d6da4c2f",
      "9fcbb86583de44b2a93506891912575b",
      "b9dd62f26ec7439a81562c05d8931296",
      "a5aff9cacbbb40d998d40b6d00448e51",
      "172b8df1857d44309dc69f69b2d2b6a3",
      "d5efc08f8a4541cea6370bacd53947ed",
      "95df86004af644ebb5b0bcd6a1f3437d",
      "fee864260ead413a884409f0aa48a97f",
      "b4f9da7d91cf459885493116e5eda816",
      "e10295c70e9448d68232fad7cf25f949",
      "32c5f1e3e2904d468b72fb313c87eb78",
      "b277c38b58e14fa897546b797f34cbc4",
      "6055d83c4cbc4acab47fb8d94861615c",
      "7c4cde7a05da478187f2ce3440286525",
      "2c1b925ce11e4a978792dab65ff83eab",
      "fb9f510556cc4d7a9b3025c42eb6aa80",
      "49a019a0dd09497e80f6253b66064612",
      "07fa05a32c354a1081fc807eb8e11553",
      "6220d351d796477398fdc7947c87da76",
      "f68447a8164c4d258095dffc0e6fd09d"
     ]
    },
    "executionInfo": {
     "elapsed": 136413,
     "status": "ok",
     "timestamp": 1768118802921,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "8DXBvpV_5NOh",
    "outputId": "094a2dbf-8ba5-45c6-f149-155be964cc9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: models/qwen3-4b-grpo-merged-f32-final | Mode: Non-CoT\n",
      " Precision: torch.float16 | 4-Bit Quantization: False\n",
      "INFO 01-11 08:04:26 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-11 08:04:26 [vllm_utils.py:731] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading models/qwen3-4b-grpo-merged-f32-final with actual GPU utilization = 15.89%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 32.\n",
      "Unsloth: vLLM's KV Cache can use up to 5.54 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-11 08:04:33 [utils.py:328] non-default args: {'dtype': torch.float16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.15885264811099853, 'max_num_batched_tokens': 4096, 'max_num_seqs': 32, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'models/qwen3-4b-grpo-merged-f32-final'}\n",
      "INFO 01-11 08:04:33 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 01-11 08:04:33 [__init__.py:2764] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-11 08:04:33 [__init__.py:1815] Using max model len 4096\n",
      "INFO 01-11 08:04:33 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "WARNING 01-11 08:04:33 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-11 08:04:33 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='models/qwen3-4b-grpo-merged-f32-final', speculative_config=None, tokenizer='models/qwen3-4b-grpo-merged-f32-final', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/qwen3-4b-grpo-merged-f32-final, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "INFO 01-11 08:04:34 [gpu_model_runner.py:2338] Starting to load model models/qwen3-4b-grpo-merged-f32-final...\n",
      "INFO 01-11 08:04:35 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd47c59ec165431ea980cff991947042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:04:44 [default_loader.py:268] Loading weights took 9.12 seconds\n",
      "INFO 01-11 08:04:46 [gpu_model_runner.py:2392] Model loading took 7.7976 GiB and 9.291855 seconds\n",
      "INFO 01-11 08:05:00 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/759d6af8cf/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-11 08:05:00 [backends.py:550] Dynamo bytecode transform time: 12.39 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 301.09it/s, triton_poi_fused_view_6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:05:03 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 359.95it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 518.14it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 505.96it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 422.21it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 506.12it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 481.19it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 490.88it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 493.98it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 486.78it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 470.78it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 530.05it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 511.89it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 519.03it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 521.40it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 516.39it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 472.74it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 482.80it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 491.49it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 481.55it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 476.52it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 482.89it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 488.23it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 502.63it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 504.23it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 516.56it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 517.52it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 488.56it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 517.61it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 512.69it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 504.99it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 513.05it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 471.64it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 494.65it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 485.67it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 523.81it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 402.66it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:05:49 [backends.py:215] Compiling a graph for dynamic shape takes 48.86 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:06:04 [monitor.py:34] torch.compile takes 61.24 s in total\n",
      "INFO 01-11 08:06:06 [gpu_worker.py:298] Available KV cache memory: 4.43 GiB\n",
      "INFO 01-11 08:06:07 [kv_cache_utils.py:864] GPU KV cache size: 32,208 tokens\n",
      "INFO 01-11 08:06:07 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 7.86x\n",
      "INFO 01-11 08:06:07 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "INFO 01-11 08:06:07 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  5.93it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:06:10 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.05 GiB\n",
      "INFO 01-11 08:06:10 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 3 secs.\n",
      "INFO 01-11 08:06:10 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 3 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:06:12 [gpu_worker.py:391] Free memory on device (15.75/79.32 GiB) on startup. Desired GPU memory utilization is (0.15885264811099853, 12.6 GiB). Actual usage is 7.8 GiB for weight, 0.37 GiB for peak activation, 0.01 GiB for non-torch memory, and 0.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=4539727564` to fit into requested memory, or `--kv-cache-memory=7921988096` to fully utilize gpu memory. Current kv cache memory in use is 4751539916 bytes.\n",
      "INFO 01-11 08:06:12 [core.py:218] init engine (profile, create kv cache, warmup model) took 86.18 seconds\n",
      "INFO 01-11 08:06:13 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-11 08:06:13 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['post_attention_layernorm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm2', 'k_norm', 'attention_norm', 'q_norm', 'post_layernorm', 'layer_norm2', 'norm', 'norm1', 'ffn_norm', 'layer_norm1', 'input_layernorm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10a50f3b5cb45fe851f04d36798cc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at models/qwen3-4b-grpo-merged-f32-final and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['post_attention_layernorm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm2', 'k_norm', 'attention_norm', 'q_norm', 'post_layernorm', 'layer_norm2', 'norm', 'cross_attn_post_attention_layernorm', 'cross_attn_input_layernorm', 'norm1', 'ffn_norm', 'layer_norm1', 'input_layernorm']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0aa07b13674a5aa4903cae994e2945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10295c70e9448d68232fad7cf25f949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 164 samples to: data/evaluation/generations_run_2/grpo_non_cot.jsonl\n"
     ]
    }
   ],
   "source": [
    "# GRPO MODEL (Non-CoT) - FP32 EVALUATION\n",
    "evaluate_model(\n",
    "    model_path= GRPO_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=False,\n",
    "    output_jsonl=str(GEN_DIR / \"grpo_non_cot.jsonl\"),\n",
    "    max_new_tokens=1024,\n",
    "    temperature=TEMP_NON_COT,\n",
    "    load_in_4bit=False,    # Do not compress\n",
    "    dtype=torch.float16,   # Do not round off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6d34d9f059fd4e7597c8be34e776897e",
      "add402da1eb94b7a9d760891853383c8",
      "9a2c5b37553349e0b6d5b58bfda24520",
      "c737cf2f7c434cad97db05989417249d",
      "a5a954604995409fbd0887e4df92ca9e",
      "f06ccb408bb74c90b73856a845c596b9",
      "5c04b4b61c004a0db3f60968c52ca675",
      "42d3274f0bda4042a4d489cf9920925e",
      "de12c5249305477ea918a0d134ae8c2c",
      "e2dbea9be5ab493aac00e93900b9265f",
      "796b5cf19ff740e18f6b94d74e95e948",
      "b368f3cf33ed48649dbce0c270e11a1b",
      "f4138e4bdea14297b15fd8952968c594",
      "3aa926630d534a33ad5d421901f08a98",
      "4f6b419875414cdbb3ab292d968243ee",
      "3a19c71aa3e445c28093eb32e7b461bd",
      "921204318d994be29bcda678014c96a9",
      "8ecc765160d64040980adeabe89c0e27",
      "b64ca90f8d4649caa5f27484c7a5c315",
      "75f9404217094f91b5839d0bfbbb007a",
      "c2904647bf1245e5a78ac865a139c6f5",
      "d9bd1231aab442cfbccd8220b8178129",
      "c70a41e31e044c0db34e1d5847f9578a",
      "c6b565958ed4477ea73706c6c0c7458e",
      "997f6c2864724e9d818216dfd7e999de",
      "649b04d72cc84e25abba1fe58ed73b10",
      "759f7d85019448b192920e0a45fdddc6",
      "34a6e39be2364b1494b37611eced174e",
      "e3e4a228ab90466e903bc7ecef64c10c",
      "582ae4426e384f239ca5cce5da8ea705",
      "2facede2fced4627a986c9ce59599b1e",
      "0f565479b98f4d3794bc0c68e592bb67",
      "6cee2af307ed407eb137f48a123aa421",
      "5506d87aef2949f9b2d60fba1e2c38bc",
      "58a41529b30a46d490cea6fa51042441",
      "c8f38d8936b3487c9c4a24200e854dd6",
      "5cf41a644b7a485cb5041e67fe698068",
      "173b5355d5444f79b001618a3c6b4fa4",
      "79bdda4e89d24509a6efe64d265397b1",
      "62655868cb554e52bf8689fa0633ba31",
      "9f866cd8a3244f3dbd2ef4b740d2e45b",
      "42c4460145504707bff04d7b97b338c0",
      "615749b6d2b6416880c68821ef8ae6a3",
      "ca2f7fe0cce34c69a329308405b5ca17"
     ]
    },
    "executionInfo": {
     "elapsed": 537719,
     "status": "ok",
     "timestamp": 1768118630673,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "XjMIcrQl0KWf",
    "outputId": "41012833-900d-48b0-a9ff-6a4c86005886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: models/qwen3-4b-grpo-merged-f32-final | Mode: CoT\n",
      " Precision: torch.float16 | 4-Bit Quantization: False\n",
      "INFO 01-11 07:54:54 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-11 07:54:54 [vllm_utils.py:731] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading models/qwen3-4b-grpo-merged-f32-final with actual GPU utilization = 79.54%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 56.03 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-11 07:55:01 [utils.py:328] non-default args: {'dtype': torch.float16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.7954304147054094, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'models/qwen3-4b-grpo-merged-f32-final'}\n",
      "INFO 01-11 07:55:17 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 07:55:17 [__init__.py:2764] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-11 07:55:17 [__init__.py:1815] Using max model len 4096\n",
      "INFO 01-11 07:55:19 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-11 07:55:19 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-11 07:55:22 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='models/qwen3-4b-grpo-merged-f32-final', speculative_config=None, tokenizer='models/qwen3-4b-grpo-merged-f32-final', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/qwen3-4b-grpo-merged-f32-final, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n",
      "INFO 01-11 07:55:23 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-11 07:55:23 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-11 07:55:23 [gpu_model_runner.py:2338] Starting to load model models/qwen3-4b-grpo-merged-f32-final...\n",
      "INFO 01-11 07:55:23 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 01-11 07:55:23 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d34d9f059fd4e7597c8be34e776897e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:01:17 [default_loader.py:268] Loading weights took 352.18 seconds\n",
      "INFO 01-11 08:01:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-11 08:01:18 [gpu_model_runner.py:2392] Model loading took 7.8056 GiB and 353.306098 seconds\n",
      "INFO 01-11 08:01:32 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/902f1ba83e/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-11 08:01:32 [backends.py:550] Dynamo bytecode transform time: 13.43 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 11.26it/s, triton_poi_fused_view_6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:01:38 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 18.21it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 533.82it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 529.05it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 537.13it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 515.26it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 532.63it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 530.89it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 519.78it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 494.29it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 522.90it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 526.18it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 538.35it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 484.90it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 524.38it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 495.50it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 496.59it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 509.34it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 506.40it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 548.17it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 522.49it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 496.77it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 504.47it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 531.13it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 470.20it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 535.24it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 477.81it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 494.01it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 544.26it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 503.43it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 526.25it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 497.23it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 510.40it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 456.49it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 512.51it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 511.78it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 26.37it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:02:23 [backends.py:215] Compiling a graph for dynamic shape takes 49.88 s\n",
      "INFO 01-11 08:02:44 [monitor.py:34] torch.compile takes 63.31 s in total\n",
      "INFO 01-11 08:02:46 [gpu_worker.py:298] Available KV cache memory: 54.53 GiB\n",
      "INFO 01-11 08:02:47 [kv_cache_utils.py:864] GPU KV cache size: 397,040 tokens\n",
      "INFO 01-11 08:02:47 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 96.93x\n",
      "INFO 01-11 08:02:47 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:14<00:00,  2.44it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:03:05 [gpu_model_runner.py:3118] Graph capturing finished in 18 secs, took 0.65 GiB\n",
      "INFO 01-11 08:03:05 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 18 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 08:03:06 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.7954304147054094, 63.09 GiB). Actual usage is 7.81 GiB for weight, 0.74 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.65 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=57689510092` to fit into requested memory, or `--kv-cache-memory=74545972224` to fully utilize gpu memory. Current kv cache memory in use is 58547245260 bytes.\n",
      "INFO 01-11 08:03:06 [core.py:218] init engine (profile, create kv cache, warmup model) took 108.67 seconds\n",
      "INFO 01-11 08:03:07 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-11 08:03:07 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['post_attention_layernorm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm2', 'k_norm', 'attention_norm', 'q_norm', 'post_layernorm', 'layer_norm2', 'norm', 'norm1', 'ffn_norm', 'layer_norm1', 'input_layernorm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b368f3cf33ed48649dbce0c270e11a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at models/qwen3-4b-grpo-merged-f32-final and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['post_attention_layernorm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'norm2', 'k_norm', 'attention_norm', 'q_norm', 'post_layernorm', 'layer_norm2', 'norm', 'cross_attn_post_attention_layernorm', 'cross_attn_input_layernorm', 'norm1', 'ffn_norm', 'layer_norm1', 'input_layernorm']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70a41e31e044c0db34e1d5847f9578a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5506d87aef2949f9b2d60fba1e2c38bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 164 samples to: data/evaluation/generations_run_2/grpo_cot.jsonl\n"
     ]
    }
   ],
   "source": [
    "# GRPO MODEL (CoT) - FP32 EVALUATION\n",
    "evaluate_model(\n",
    "    model_path= GRPO_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=True,\n",
    "    output_jsonl=str(GEN_DIR / \"grpo_cot.jsonl\"),\n",
    "    max_new_tokens=2048,\n",
    "    temperature=TEMP_COT,\n",
    "    load_in_4bit=False,    # Do not compress\n",
    "    dtype=torch.float16,   # Do not round off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1768119553211,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zwJwErJuIz88",
    "outputId": "8ce32059-d9c0-4844-d5f2-ccab5c8ce025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " INSPECTING: data/evaluation/generations_run_2/grpo_cot.jsonl\n",
      "\n",
      " TASK: HumanEval/0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    n = len(numbers)\n",
      "    for i in range(n):\n",
      "        for j in range(i + 1, n):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    paren_string = paren_string.replace(\" \", \"\")\n",
      "    result = []\n",
      "    stack = []\n",
      "    current_group = \"\"\n",
      "\n",
      "    for ch in paren_string:\n",
      "        if ch == '(':\n",
      "            stack.append(ch)\n",
      "            current_group += ch\n",
      "        elif ch == ')':\n",
      "            stack.pop()\n",
      "            current_group += ch\n",
      "            if not stack and current_group:\n",
      "                result.append(current_group)\n",
      "                current_group = \"\"\n",
      "\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/2\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    return number - int(number)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/3\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    balance = 0\n",
      "    for op in operations:\n",
      "        balance += op\n",
      "        if balance < 0:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/4\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not numbers:\n",
      "        return 0.0\n",
      "    mean = sum(numbers) / len(numbers)\n",
      "    return sum(abs(x - mean) for x in numbers) / len(numbers)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/5\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "    if not numbers:\n",
      "        return []\n",
      "    result = []\n",
      "    for i, num in enumerate(numbers):\n",
      "        result.append(num)\n",
      "        if i < len(numbers) - 1:\n",
      "            result.append(delimeter)\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/6\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def max_nesting(group: str) -> int:\n",
      "        depth = 0\n",
      "        max_depth = 0\n",
      "        for char in group:\n",
      "            if char == '(':\n",
      "                depth += 1\n",
      "                if depth > max_depth:\n",
      "                    max_depth = depth\n",
      "            elif char == ')':\n",
      "                if depth > 0:\n",
      "                    depth -= 1\n",
      "        return max_depth\n",
      "\n",
      "    groups = paren_string.split()\n",
      "    return [max_nesting(group) for group in groups]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/7\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "    return [s for s in strings if substring in s]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/8\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    from functools import reduce\n",
      "    from operator import mul\n",
      "\n",
      "    if not numbers:\n",
      "        return 0, 1\n",
      "    return sum(numbers), reduce(mul, numbers)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/9\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not numbers:\n",
      "        return []\n",
      "\n",
      "    result = []\n",
      "    current_max = numbers[0]\n",
      "    for num in numbers:\n",
      "        if num > current_max:\n",
      "            current_max = num\n",
      "        result.append(current_max)\n",
      "\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/10\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not string:\n",
      "        return ''\n",
      "\n",
      "    # Start from the full string length and decrement\n",
      "    for i in range(len(string), 0, -1):\n",
      "        if is_palindrome(string[-i:]):\n",
      "            # Found the longest palindromic suffix\n",
      "            # The prefix to append is string[:len(string)-i]\n",
      "            prefix = string[:len(string)-i]\n",
      "            # Reverse the prefix and concatenate\n",
      "            return string + prefix[::-1]\n",
      "\n",
      "    # If no proper suffix found (only possibility is empty string), return original\n",
      "    return string\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/11\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    # Make both strings same length by padding with zeros\n",
      "    max_len = max(len(a), len(b))\n",
      "    a = a.zfill(max_len)\n",
      "    b = b.zfill(max_len)\n",
      "\n",
      "    # Perform XOR on each bit and join the results\n",
      "    result = ''.join(str(int(x) ^ int(y)) for x, y in zip(a, b))\n",
      "\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/12\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not strings:\n",
      "        return None\n",
      "    return max(strings, key=len)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/13\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    while b != 0:\n",
      "        a, b = b, a % b\n",
      "    return a\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/14\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return [string[:i] for i in range(1, len(string) + 1)]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/15\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return ' '.join(map(str, range(n + 1)))\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/16\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return len(set(string.lower()))\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/17\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "    note_durations = {\n",
      "        'o': 4,\n",
      "        'o|': 2,\n",
      "        '.|': 1\n",
      "    }\n",
      "    return [note_durations[note] for note in music_string.split()]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/18\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not substring:\n",
      "        return 0\n",
      "    count = 0\n",
      "    sub_len = len(substring)\n",
      "    for i in range(len(string) - sub_len + 1):\n",
      "        if string[i:i + sub_len] == substring:\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/19\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "    if not numbers:\n",
      "        return ''\n",
      "\n",
      "    word_to_num = {\n",
      "        'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4,\n",
      "        'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9\n",
      "    }\n",
      "\n",
      "    # Split the input string into words\n",
      "    words = numbers.split()\n",
      "\n",
      "    # Convert words to numbers, sort, then convert back to words\n",
      "    sorted_words = sorted(words, key=lambda word: word_to_num[word])\n",
      "\n",
      "    # Join the sorted words into a single string\n",
      "    return ' '.join(sorted_words)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/20\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "    if len(numbers) < 2:\n",
      "        raise ValueError(\"List must contain at least two numbers\")\n",
      "\n",
      "    closest_pair = (numbers[0], numbers[1])\n",
      "    min_diff = abs(numbers[0] - numbers[1])\n",
      "\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            current_diff = abs(numbers[i] - numbers[j])\n",
      "            if current_diff < min_diff:\n",
      "                min_diff = current_diff\n",
      "                closest_pair = (min(numbers[i], numbers[j]), max(numbers[i], numbers[j]))\n",
      "\n",
      "    return closest_pair\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/21\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not numbers:\n",
      "        return []\n",
      "    mn = min(numbers)\n",
      "    mx = max(numbers)\n",
      "    scale = mx - mn\n",
      "    if scale == 0:\n",
      "        return [0.0] * len(numbers)\n",
      "    return [(x - mn) / scale for x in numbers]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/22\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return [value for value in values if isinstance(value, int)]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/23\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return len(string)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/24\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    if n <= 1:\n",
      "        return 1\n",
      "    for i in range(n // 2, 0, -1):\n",
      "        if n % i == 0:\n",
      "            return i\n",
      "    return 1\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/25\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/26\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    seen = set()\n",
      "    result = []\n",
      "    for num in numbers:\n",
      "        if num not in seen:\n",
      "            result.append(num)\n",
      "            seen.add(num)\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/27\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return ''.join(\n",
      "        char.lower() if char.isupper() else char.upper()\n",
      "        for char in string\n",
      "    )\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/28\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return ''.join(strings)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/29\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "    return [s for s in strings if s.startswith(prefix)]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/30\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return [x for x in l if x > 0]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/31\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n == 2:\n",
      "        return True\n",
      "    if n % 2 == 0:\n",
      "        return False\n",
      "    for i in range(3, math.isqrt(n) + 1, 2):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/32\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if len(xs) < 2:\n",
      "        return None  # Not enough coefficients to form a polynomial of degree >= 1\n",
      "\n",
      "    # Evaluate polynomial at two endpoints\n",
      "    a = -10.0\n",
      "    b = 10.0\n",
      "    fa = poly(xs, a)\n",
      "    fb = poly(xs, b)\n",
      "\n",
      "    # Check for sign change\n",
      "    if fa * fb > 0:\n",
      "        # No sign change in the interval [-10, 10]\n",
      "        # Try a smaller interval or different approach\n",
      "        # For simplicity, we'll return None for this case\n",
      "        return None\n",
      "\n",
      "    # Bisection method\n",
      "    while b - a > 1e-6:\n",
      "        c = (a + b) / 2\n",
      "        fc = poly(xs, c)\n",
      "        if fc == 0:\n",
      "            return round(c, 2)\n",
      "        elif fa * fc < 0:\n",
      "            b = c\n",
      "            fb = fc\n",
      "        else:\n",
      "            a = c\n",
      "            fa = fc\n",
      "\n",
      "    return round((a + b) / 2, 2)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/33\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not l:\n",
      "        return []\n",
      "    # Extract elements at indices divisible by three\n",
      "    third_indices = l[::3]\n",
      "    # Sort the extracted elements\n",
      "    third_indices_sorted = sorted(third_indices)\n",
      "    # Create the result list\n",
      "    result = l[:]  # shallow copy\n",
      "    # Replace elements at indices divisible by three with sorted values\n",
      "    for i, val in enumerate(third_indices_sorted):\n",
      "        result[i * 3] = val\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/34\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    seen = set()\n",
      "    unique_elements = []\n",
      "    for item in l:\n",
      "        if item not in seen:\n",
      "            seen.add(item)\n",
      "            unique_elements.append(item)\n",
      "    return sorted(unique_elements)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/35\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not l:\n",
      "        raise ValueError(\"max_element() arg is an empty sequence\")\n",
      "    return max(l)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/36\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    count = 0\n",
      "    for i in range(1, n):\n",
      "        if i % 11 == 0 or i % 13 == 0:\n",
      "            count += str(i).count('7')\n",
      "    return count\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/37\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    # Extract elements at even indices\n",
      "    evens = sorted(l[::2])\n",
      "    # Extract elements at odd indices\n",
      "    odds = l[1::2]\n",
      "    # Create result list with sorted evens at even positions and original odds at odd positions\n",
      "    result = []\n",
      "    even_idx = 0\n",
      "    odd_idx = 0\n",
      "    for i in range(len(l)):\n",
      "        if i % 2 == 0:\n",
      "            result.append(evens[even_idx])\n",
      "            even_idx += 1\n",
      "        else:\n",
      "            result.append(odds[odd_idx])\n",
      "            odd_idx += 1\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/38\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def encode_cyclic(s: str):\n",
      "        groups = [s[(3 * i):min((3 * i + 3), len(s))] for i in range((len(s) + 2) // 3)]\n",
      "        groups = [(group[1:] + group[0]) if len(group) == 3 else group for group in groups]\n",
      "        return \"\".join(groups)\n",
      "\n",
      "    \"\"\"\n",
      "    takes as input string encoded with encode_cyclic function. Returns decoded string.\n",
      "    \"\"\"\n",
      "    groups = [s[(3 * i):min((3 * i + 3), len(s))] for i in range((len(s) + 2) // 3)]\n",
      "    groups = [(group[-1] + group[:-1]) if len(group) == 3 else group for group in groups]\n",
      "    return \"\".join(groups)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/39\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def is_prime(num: int) -> bool:\n",
      "        if num <= 1:\n",
      "            return False\n",
      "        for i in range(2, int(num**0.5) + 1):\n",
      "            if num % i == 0:\n",
      "                return False\n",
      "        return True\n",
      "\n",
      "    \"\"\"Return the n-th Fibonacci number that is also prime.\"\"\"\n",
      "    if n <= 0:\n",
      "        raise ValueError(\"n must be a positive integer\")\n",
      "\n",
      "    a, b = 0, 1\n",
      "    count = 0\n",
      "    while True:\n",
      "        if is_prime(b):\n",
      "            count += 1\n",
      "            if count == n:\n",
      "                return b\n",
      "        a, b = b, a + b\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/40\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/41\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return n * n\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/42\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return [x + 1 for x in l]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/43\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    seen = set()\n",
      "    for num in l:\n",
      "        if -num in seen:\n",
      "            return True\n",
      "        seen.add(num)\n",
      "    return False\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/44\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if x < 0:\n",
      "        raise ValueError(\"Negative numbers not supported\")\n",
      "    if x == 0:\n",
      "        return '0'\n",
      "    digits = []\n",
      "    while x > 0:\n",
      "        digits.append(str(x % base))\n",
      "        x = x // base\n",
      "    digits.reverse()\n",
      "    return ''.join(digits)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/45\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return (a * h) / 2\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/46\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if n < 0:\n",
      "        raise ValueError(\"n must be a non-negative integer\")\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    if n == 1:\n",
      "        return 0\n",
      "    if n == 2:\n",
      "        return 2\n",
      "    if n == 3:\n",
      "        return 0\n",
      "\n",
      "    # Initialize the base case for fib4(4)\n",
      "    a, b, c, d = 0, 0, 2, 0\n",
      "    for i in range(4, n + 1):\n",
      "        a, b, c, d = b, c, d, a + b + c + d\n",
      "\n",
      "    return d\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/47\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    sorted_l = sorted(l)\n",
      "    n = len(sorted_l)\n",
      "    mid = n // 2\n",
      "    if n % 2 == 1:\n",
      "        return sorted_l[mid]\n",
      "    else:\n",
      "        return (sorted_l[mid - 1] + sorted_l[mid]) / 2\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/48\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import re\n",
      "\n",
      "    # Normalize the string: lowercase and remove non-alphanumeric characters\n",
      "    normalized = re.sub(r'[^a-zA-Z0-9]', '', text).lower()\n",
      "    # Compare with its reverse\n",
      "    return normalized == normalized[::-1]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/49\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if p <= 0:\n",
      "        raise ValueError(\"Modulus p must be positive\")\n",
      "    result = 1\n",
      "    base = 2\n",
      "    while n > 0:\n",
      "        if n % 2 == 1:\n",
      "            result = (result * base) % p\n",
      "        base = (base * base) % p\n",
      "        n = n // 2\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/50\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def encode_shift(s: str):\n",
      "        return \"\".join([chr(((ord(ch) + 5 - ord(\"a\")) % 26) + ord(\"a\")) for ch in s])\n",
      "\n",
      "    \"\"\"\n",
      "    takes as input string encoded with encode_shift function. Returns decoded string.\n",
      "    \"\"\"\n",
      "    return \"\".join([chr(((ord(ch) - 5 - ord(\"a\")) % 26) + ord(\"a\")) for ch in s])\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/51\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import string\n",
      "\n",
      "    vowels = set(string.ascii_lowercase + string.ascii_uppercase)\n",
      "    return text.translate({ord(c): None for c in vowels})\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/52\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return all(x < t for x in l)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/53\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return x + y\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/54\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    from collections import Counter\n",
      "\n",
      "    return Counter(s0) == Counter(s1)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/55\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    a, b = 0, 1\n",
      "    for _ in range(2, n + 1):\n",
      "        a, b = b, a + b\n",
      "    return b\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/56\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    balance = 0\n",
      "    for ch in brackets:\n",
      "        if ch == '<':\n",
      "            balance += 1\n",
      "        elif ch == '>':\n",
      "            balance -= 1\n",
      "        # If balance goes negative, there's a closing bracket without an opening one\n",
      "        if balance < 0:\n",
      "            return False\n",
      "    # If balance is zero, all brackets are correctly matched\n",
      "    return balance == 0\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/57\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not l:\n",
      "        return True\n",
      "    direction = l[1] - l[0]\n",
      "    if direction == 0:\n",
      "        direction = None\n",
      "    for i in range(1, len(l)):\n",
      "        diff = l[i] - l[i-1]\n",
      "        if (direction is not None and diff * direction < 0) or (direction is None and diff != 0):\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/58\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not l1 or not l2:\n",
      "        return []\n",
      "    common_elements = set(l1) & set(l2)\n",
      "    return sorted(common_elements)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/59\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    # Divide by 2 until n is odd\n",
      "    while n % 2 == 0:\n",
      "        max_factor = 2\n",
      "        n //= 2\n",
      "\n",
      "    # Now n is odd, try odd divisors from 3 to sqrt(n)\n",
      "    for i in range(3, int(math.sqrt(n)) + 1, 2):\n",
      "        while n % i == 0:\n",
      "            max_factor = i\n",
      "            n //= i\n",
      "\n",
      "    # If n > 2, then n is the largest prime factor\n",
      "    if n > 2:\n",
      "        max_factor = n\n",
      "\n",
      "    return max_factor\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/60\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return n * (n + 1) // 2\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/61\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    balance = 0\n",
      "    for bracket in brackets:\n",
      "        if bracket == '(':\n",
      "            balance += 1\n",
      "        elif bracket == ')':\n",
      "            balance -= 1\n",
      "        if balance < 0:\n",
      "            return False\n",
      "    return balance == 0\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/62\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if len(xs) <= 1:\n",
      "        return []\n",
      "    return [i * xs[i] for i in range(1, len(xs))]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/63\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if n == 0 or n == 1:\n",
      "        return 0\n",
      "    if n == 2:\n",
      "        return 1\n",
      "    # Initialize the base cases\n",
      "    fibfib_values = [0] * (n + 1)\n",
      "    fibfib_values[2] = 1\n",
      "    # Compute the sequence iteratively\n",
      "    for i in range(3, n + 1):\n",
      "        fibfib_values[i] = fibfib_values[i - 1] + fibfib_values[i - 2] + fibfib_values[i - 3]\n",
      "    return fibfib_values[n]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/64\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    vowels = set('aeiou')\n",
      "    count = 0\n",
      "    for char in s.lower():\n",
      "        if char in vowels:\n",
      "            count += 1\n",
      "    if s.lower() and s[-1].lower() == 'y':\n",
      "        count += 1\n",
      "    return count\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/65\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    num_str = str(abs(x))\n",
      "    n = len(num_str)\n",
      "    if shift < 0:\n",
      "        shift = n + shift % n\n",
      "    shift = shift % n\n",
      "    if shift == 0:\n",
      "        return num_str\n",
      "    if shift >= n:\n",
      "        return num_str[::-1]\n",
      "    return num_str[-shift:] + num_str[:-shift]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/66\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    total = 0\n",
      "    for char in s:\n",
      "        if char.isupper():\n",
      "            total += ord(char)\n",
      "    return total\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/67\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    # Extract the numbers for apples and oranges from the string\n",
      "    # The format is \"X apples and Y oranges\", so we split on \" apples and \" and take the first part, then split on spaces\n",
      "    parts = s.split(\" apples and \")\n",
      "    count_apples = int(parts[0])\n",
      "    count_oranges = int(parts[1].split()[0])\n",
      "\n",
      "    # Calculate the number of mangoes\n",
      "    count_mangoes = n - count_apples - count_oranges\n",
      "\n",
      "    return count_mangoes\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/68\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    min_val = None\n",
      "    min_index = -1\n",
      "    for i, n in enumerate(arr):\n",
      "        if n % 2 == 0:\n",
      "            if min_val is None or n < min_val:\n",
      "                min_val = n\n",
      "                min_index = i\n",
      "    if min_val is None:\n",
      "        return []\n",
      "    return [min_val, min_index]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/69\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    from collections import Counter\n",
      "\n",
      "    if not lst:\n",
      "        return -1\n",
      "\n",
      "    freq = Counter(lst)\n",
      "    max_val = max(lst)\n",
      "    result = None\n",
      "\n",
      "    for i in range(1, max_val + 1):\n",
      "        if freq[i] >= i:\n",
      "            if result is None or i > result:\n",
      "                result = i\n",
      "\n",
      "    return result if result is not None else -1\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/70\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not lst:\n",
      "        return []\n",
      "\n",
      "    result = []\n",
      "    lst_sorted = sorted(lst)\n",
      "    while lst_sorted:\n",
      "        # Add the smallest remaining element\n",
      "        result.append(lst_sorted.pop(0))\n",
      "        if lst_sorted:\n",
      "            # Add the largest remaining element\n",
      "            result.append(lst_sorted.pop())\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/71\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    if (a + b > c) and (a + c > b) and (b + c > a):\n",
      "        s = (a + b + c) / 2\n",
      "        area = math.sqrt(s * (s - a) * (s - b) * (s - c))\n",
      "        return round(area, 2)\n",
      "    else:\n",
      "        return -1\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/72\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if q == q[::-1] and sum(q) <= w:\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/73\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    n = len(arr)\n",
      "    changes = 0\n",
      "    for i in range(n // 2):\n",
      "        if arr[i] != arr[n - i - 1]:\n",
      "            changes += 1\n",
      "            # Make the smaller side match the larger side to minimize changes\n",
      "            if arr[i] < arr[n - i - 1]:\n",
      "                arr[i] = arr[n - i - 1]\n",
      "            else:\n",
      "                arr[n - i - 1] = arr[i]\n",
      "    return changes\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/74\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    total1 = sum(len(s) for s in lst1)\n",
      "    total2 = sum(len(s) for s in lst2)\n",
      "    if total1 <= total2:\n",
      "        return lst1\n",
      "    else:\n",
      "        return lst2\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/75\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if a >= 100:\n",
      "        return False\n",
      "    primes = [2, 3, 5, 7]\n",
      "    product_set = set()\n",
      "    for i in range(len(primes)):\n",
      "        for j in range(i+1, len(primes)):\n",
      "            for k in range(j+1, len(primes)):\n",
      "                product = primes[i] * primes[j] * primes[k]\n",
      "                if product <= 100:\n",
      "                    product_set.add(product)\n",
      "    return a in product_set\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/76\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if x < 1 or n <= 0:\n",
      "        return False\n",
      "    if n == 1:\n",
      "        return x == 1\n",
      "    power = 1\n",
      "    while power < x:\n",
      "        power *= n\n",
      "    return power == x\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/77\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    if a == 0:\n",
      "        return True\n",
      "    x = round(a ** (1/3))\n",
      "    return x ** 3 == a\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/78\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    primes = {2, 3, 5, 7, 0xB, 0xD}\n",
      "    count = 0\n",
      "    for char in num:\n",
      "        digit = int(char, 16)\n",
      "        if digit in primes:\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/79\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if decimal < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer\")\n",
      "    binary_str = bin(decimal)[2:]  # Remove the '0b' prefix\n",
      "    return f\"db{binary_str}db\"\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/80\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    n = len(s)\n",
      "    if n < 3:\n",
      "        return False\n",
      "    for i in range(n - 2):\n",
      "        window = s[i:i + 3]\n",
      "        if len(set(window)) != 3:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/81\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    letter_grades = []\n",
      "    for gpa in grades:\n",
      "        if gpa == 4.0:\n",
      "            letter_grades.append('A+')\n",
      "        elif gpa > 3.7:\n",
      "            letter_grades.append('A')\n",
      "        elif gpa > 3.3:\n",
      "            letter_grades.append('A-')\n",
      "        elif gpa > 3.0:\n",
      "            letter_grades.append('B+')\n",
      "        elif gpa > 2.7:\n",
      "            letter_grades.append('B')\n",
      "        elif gpa > 2.3:\n",
      "            letter_grades.append('B-')\n",
      "        elif gpa > 2.0:\n",
      "            letter_grades.append('C+')\n",
      "        elif gpa > 1.7:\n",
      "            letter_grades.append('C')\n",
      "        elif gpa > 1.3:\n",
      "            letter_grades.append('C-')\n",
      "        elif gpa > 1.0:\n",
      "            letter_grades.append('D+')\n",
      "        elif gpa > 0.7:\n",
      "            letter_grades.append('D')\n",
      "        elif gpa > 0.0:\n",
      "            letter_grades.append('D-')\n",
      "        else:\n",
      "            letter_grades.append('E')\n",
      "    return letter_grades\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/82\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def is_prime(n):\n",
      "        if n <= 1:\n",
      "            return False\n",
      "        if n <= 3:\n",
      "            return True\n",
      "        if n % 2 == 0 or n % 3 == 0:\n",
      "            return False\n",
      "        i = 5\n",
      "        while i * i <= n:\n",
      "            if n % i == 0 or n % (i + 2) == 0:\n",
      "                return False\n",
      "            i += 6\n",
      "        return True\n",
      "\n",
      "    \"\"\"Return True if the string length is a prime number, False otherwise.\"\"\"\n",
      "    length = len(string)\n",
      "    return is_prime(length)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/83\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    return 18 * (10 ** (n - 2))\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/84\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if N == 0:\n",
      "        return \"0\"\n",
      "    digit_sum = sum(int(digit) for digit in str(N))\n",
      "    return bin(digit_sum)[2:]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/85\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return sum(x for i, x in enumerate(lst) if i % 2 == 1 and x % 2 == 0)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/86\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def sort_word(word):\n",
      "        \"\"\"Return the word with characters sorted by ASCII value.\"\"\"\n",
      "        return ''.join(sorted(word))\n",
      "\n",
      "    # Split the string into words and spaces (keeping order)\n",
      "    parts = []\n",
      "    word_buffer = []\n",
      "    for ch in s:\n",
      "        if ch.isspace():\n",
      "            if word_buffer:\n",
      "                parts.append(''.join(word_buffer))\n",
      "                word_buffer = []\n",
      "            parts.append(ch)\n",
      "        else:\n",
      "            word_buffer.append(ch)\n",
      "    if word_buffer:\n",
      "        parts.append(''.join(word_buffer))\n",
      "\n",
      "    # Sort each word, keep spaces and order\n",
      "    result = []\n",
      "    for part in parts:\n",
      "        if part.strip():\n",
      "            result.append(sort_word(part))\n",
      "        else:\n",
      "            result.append(part)\n",
      "\n",
      "    return ''.join(result)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/87\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not lst:\n",
      "        return []\n",
      "\n",
      "    # Collect all (row, col) pairs where lst[row][col] == x\n",
      "    coords = []\n",
      "    for row_idx, row in enumerate(lst):\n",
      "        for col_idx, val in enumerate(row):\n",
      "            if val == x:\n",
      "                coords.append((row_idx, col_idx))\n",
      "\n",
      "    # Sort by row ascending, then by column descending\n",
      "    coords.sort(key=lambda coord: (coord[0], -coord[1]))\n",
      "\n",
      "    return coords\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/88\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not array:\n",
      "        return []\n",
      "    first = array[0]\n",
      "    last = array[-1]\n",
      "    if (first + last) % 2 == 0:\n",
      "        return sorted(array, reverse=True)\n",
      "    return sorted(array)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/89\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import string\n",
      "\n",
      "    # Create a translation table for lowercase letters\n",
      "    lower_shift = string.ascii_lowercase\n",
      "    lower_rotated = lower_shift[4:] + lower_shift[:4]\n",
      "    lower_table = str.maketrans(lower_shift, lower_rotated)\n",
      "\n",
      "    # Create a translation table for uppercase letters\n",
      "    upper_shift = string.ascii_uppercase\n",
      "    upper_rotated = upper_shift[4:] + upper_shift[:4]\n",
      "    upper_table = str.maketrans(upper_shift, upper_rotated)\n",
      "\n",
      "    # Translate the string using both tables\n",
      "    encrypted = s.translate(lower_table).translate(upper_table)\n",
      "    return encrypted\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/90\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if len(lst) < 2:\n",
      "        return None\n",
      "    sorted_unique = sorted(set(lst))\n",
      "    if len(sorted_unique) < 2:\n",
      "        return None\n",
      "    return sorted_unique[1]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/91\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    # Split the string into sentences using regex pattern for delimiters\n",
      "    import re\n",
      "    sentences = re.split('[.?!]', S)\n",
      "\n",
      "    # Count sentences that start with \"I\" (case-insensitive)\n",
      "    count = 0\n",
      "    for sentence in sentences:\n",
      "        stripped = sentence.strip()\n",
      "        if stripped and stripped.startswith(\"I\"):\n",
      "            count += 1\n",
      "\n",
      "    return count\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/92\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not (isinstance(x, int) and isinstance(y, int) and isinstance(z, int)):\n",
      "        return False\n",
      "    return (x == y + z) or (y == x + z) or (z == x + y)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/93\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    vowels = 'aeiou'\n",
      "    result = []\n",
      "    for ch in message:\n",
      "        if ch.lower() in vowels:\n",
      "            # Find position in vowels, add 2, wrap around if necessary\n",
      "            pos = vowels.index(ch.lower())\n",
      "            new_char = vowels[(pos + 2) % 5]\n",
      "            # Preserve original case\n",
      "            if ch.islower():\n",
      "                result.append(new_char)\n",
      "            else:\n",
      "                result.append(new_char.upper())\n",
      "        else:\n",
      "            # Swap case\n",
      "            if ch.islower():\n",
      "                result.append(ch.upper())\n",
      "            else:\n",
      "                result.append(ch.lower())\n",
      "    return ''.join(result)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/94\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def is_prime(n):\n",
      "        if n <= 1:\n",
      "            return False\n",
      "        if n == 2:\n",
      "            return True\n",
      "        if n % 2 == 0:\n",
      "            return False\n",
      "        for i in range(3, int(n**0.5) + 1, 2):\n",
      "            if n % i == 0:\n",
      "                return False\n",
      "        return True\n",
      "\n",
      "    def sum_of_largest_prime_digits(lst):\n",
      "        \"\"\"Return the sum of digits of the largest prime number in the list.\"\"\"\n",
      "        primes = [x for x in lst if is_prime(x)]\n",
      "        if not primes:\n",
      "            return 0\n",
      "        largest_prime = max(primes)\n",
      "        return sum(int(digit) for digit in str(largest_prime))\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/95\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not dict:\n",
      "        return False\n",
      "    keys = dict.keys()\n",
      "    first_key = next(iter(keys))\n",
      "    first_case = first_key.islower()\n",
      "    for key in keys:\n",
      "        if (key.islower() != first_case) and (key.isupper() != first_case):\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/96\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if n <= 1:\n",
      "        return []\n",
      "\n",
      "    def is_prime(num):\n",
      "        if num <= 1:\n",
      "            return False\n",
      "        if num <= 3:\n",
      "            return True\n",
      "        if num % 2 == 0 or num % 3 == 0:\n",
      "            return False\n",
      "        i = 5\n",
      "        while i * i <= num:\n",
      "            if num % i == 0 or num % (i + 2) == 0:\n",
      "                return False\n",
      "            i += 6\n",
      "        return True\n",
      "\n",
      "    primes = []\n",
      "    num = 2\n",
      "    while len(primes) < n:\n",
      "        if is_prime(num) and num < n:\n",
      "            primes.append(num)\n",
      "        num += 1\n",
      "    return primes\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/97\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    unit_a = abs(a) % 10\n",
      "    unit_b = abs(b) % 10\n",
      "    return unit_a * unit_b\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/98\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    count = 0\n",
      "    for i, char in enumerate(s):\n",
      "        if i % 2 == 0 and char in 'AEIOU':\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    num = float(value)\n",
      "    if num == int(num):\n",
      "        return int(num)\n",
      "    fractional = num - int(num)\n",
      "    if fractional == 0.5:\n",
      "        if num > 0:\n",
      "            return int(num) + 1\n",
      "        else:\n",
      "            return int(num) - 1\n",
      "    elif fractional == -0.5:\n",
      "        if num > 0:\n",
      "            return int(num)\n",
      "        else:\n",
      "            return int(num) + 1\n",
      "    else:\n",
      "        return round(num)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/100\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if n <= 0:\n",
      "        return []\n",
      "\n",
      "    pile = [n]\n",
      "    current = n\n",
      "    for i in range(1, n):\n",
      "        if n % 2 == 1:  # n is odd\n",
      "            if current % 2 == 1:\n",
      "                current += 2\n",
      "            else:\n",
      "                current += 1\n",
      "        else:  # n is even\n",
      "            if current % 2 == 0:\n",
      "                current += 2\n",
      "            else:\n",
      "                current += 1\n",
      "        pile.append(current)\n",
      "\n",
      "    return pile\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/101\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import re\n",
      "\n",
      "    # Split on one or more spaces or commas and filter out empty strings\n",
      "    return re.split(r'[ ,]+', s)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/102\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if y < x:\n",
      "        return -1\n",
      "    for num in range(y, x - 1, -1):\n",
      "        if num % 2 == 0:\n",
      "            return num\n",
      "    return -1\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/103\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if n > m:\n",
      "        return -1\n",
      "    sum_n_to_m = n + (m - n) * (n + m) // 2\n",
      "    count = m - n + 1\n",
      "    avg = sum_n_to_m // count\n",
      "    if (sum_n_to_m - avg * count) >= count // 2:\n",
      "        avg += 1\n",
      "    return bin(avg)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/104\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def is_odd_digit(n):\n",
      "        while n > 0:\n",
      "            digit = n % 10\n",
      "            if digit % 2 == 0:\n",
      "                return False\n",
      "            n = n // 10\n",
      "        return True\n",
      "\n",
      "    \"\"\"Return sorted list of integers from x that have no even digits.\"\"\"\n",
      "    return sorted([num for num in x if is_odd_digit(num)])\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/105\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    # Filter integers 1-9\n",
      "    filtered = [x for x in arr if isinstance(x, int) and 1 <= x <= 9]\n",
      "    if not filtered:\n",
      "        return []\n",
      "\n",
      "    # Sort and reverse\n",
      "    sorted_reversed = sorted(filtered, reverse=True)\n",
      "\n",
      "    # Map to word names\n",
      "    word_map = {\n",
      "        1: \"One\", 2: \"Two\", 3: \"Three\", 4: \"Four\",\n",
      "        5: \"Five\", 6: \"Six\", 7: \"Seven\", 8: \"Eight\", 9: \"Nine\"\n",
      "    }\n",
      "    return [word_map[num] for num in sorted_reversed]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/106\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def factorial(i):\n",
      "        if i == 0:\n",
      "            return 1\n",
      "        result = 1\n",
      "        for j in range(1, i + 1):\n",
      "            result *= j\n",
      "        return result\n",
      "\n",
      "    \"\"\"Return a list of length n where the value at index i (1-based) is factorial(i) if i is even, else sum(1..i).\"\"\"\n",
      "    return [factorial(i) if i % 2 == 0 else i * (i + 1) // 2 for i in range(1, n + 1)]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/107\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def is_palindrome(x):\n",
      "        s = str(x)\n",
      "        return s == s[::-1]\n",
      "\n",
      "    \"\"\"\n",
      "    Return a tuple (count_even, count_odd) where count_even is the number of even\n",
      "    palindromic integers and count_odd is the number of odd palindromic integers\n",
      "    in the range 1 to n (inclusive).\n",
      "    \"\"\"\n",
      "    count_even = 0\n",
      "    count_odd = 0\n",
      "    for i in range(1, n + 1):\n",
      "        if is_palindrome(i):\n",
      "            if i % 2 == 0:\n",
      "                count_even += 1\n",
      "            else:\n",
      "                count_odd += 1\n",
      "    return (count_even, count_odd)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/108\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def digit_sum(n):\n",
      "        if n == 0:\n",
      "            return 0\n",
      "        neg = n < 0\n",
      "        n = abs(n)\n",
      "        total = 0\n",
      "        while n > 0:\n",
      "            total += (n % 10) * (-1 if neg else 1)\n",
      "            n //= 10\n",
      "            neg = False  # Only the first digit can be negative\n",
      "        return total\n",
      "\n",
      "    count = 0\n",
      "    for num in arr:\n",
      "        if digit_sum(num) > 0:\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/109\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    n = len(arr)\n",
      "    if n <= 1:\n",
      "        return True\n",
      "\n",
      "    sorted_arr = sorted(arr)\n",
      "    if sorted_arr != arr:\n",
      "        # Check if sorted array is a cyclic shift of original array\n",
      "        # by comparing concatenated versions\n",
      "        return ''.join(map(str, arr)) == ''.join(map(str, sorted_arr))\n",
      "    return True\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/110\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    odd_count = sum(1 for x in lst1 if x % 2 != 0)\n",
      "    even_count = sum(1 for x in lst2 if x % 2 == 0)\n",
      "    return \"YES\" if odd_count <= even_count else \"NO\"\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/111\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not test:\n",
      "        return {}\n",
      "\n",
      "    # Count frequency of each letter\n",
      "    counts = {}\n",
      "    for letter in test.split():\n",
      "        counts[letter] = counts.get(letter, 0) + 1\n",
      "\n",
      "    # Find the maximum count\n",
      "    max_count = max(counts.values(), default=0)\n",
      "\n",
      "    # Build the result dictionary with letters having the maximum count\n",
      "    result = {letter: count for letter, count in counts.items() if count == max_count}\n",
      "\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/112\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    # Build string with characters from s not in c\n",
      "    result = ''.join(ch for ch in s if ch not in c)\n",
      "    # Check if the result is a palindrome\n",
      "    is_palindrome = result == result[::-1]\n",
      "    return result, is_palindrome\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/113\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    result = []\n",
      "    for s in lst:\n",
      "        odd_count = sum(1 for c in s if int(c) % 2 == 1)\n",
      "        count_str = str(odd_count)\n",
      "        result.append(f\"the number of odd elements {count_str}n the str{count_str}ng {count_str} of the {count_str}nput.\")\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/114\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not nums:\n",
      "        return None  # or raise an exception, but problem specifies non-empty array\n",
      "    current_min = overall_min = nums[0]\n",
      "    for num in nums[1:]:\n",
      "        current_min = min(num, current_min + num)\n",
      "        overall_min = min(overall_min, current_min)\n",
      "    return overall_min\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/115\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    return sum(math.ceil(sum(row) / capacity) for row in grid)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/116\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def key(n):\n",
      "        # Count binary ones (set bits) using bin() and count '1'\n",
      "        # Use absolute value for sorting, but original sign is not part of sorting\n",
      "        return (bin(abs(n)).count('1'), abs(n))\n",
      "\n",
      "    # Sort using the custom key function\n",
      "    return sorted(arr, key=key)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/117\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import re\n",
      "    from collections import Counter\n",
      "\n",
      "    def count_consonants(word):\n",
      "        consonants = set('bcdfghjklmnpqrstvwxyz')\n",
      "        return sum(1 for char in word.lower() if char in consonants)\n",
      "\n",
      "    \"\"\"Return list of words from string s that contain exactly n consonants.\"\"\"\n",
      "    if not s:\n",
      "        return []\n",
      "\n",
      "    words = s.split()\n",
      "    return [word for word in words if count_consonants(word) == n]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/118\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    vowels = set(\"aeiouAEIOU\")\n",
      "    n = len(word)\n",
      "    for i in range(n-2, -1, -1):\n",
      "        if word[i] in vowels:\n",
      "            # Check if the adjacent characters are consonants\n",
      "            if word[i-1] not in vowels and word[i+1] not in vowels:\n",
      "                return word[i]\n",
      "    return \"\"\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/119\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    s1, s2 = lst\n",
      "    # Check concatenation in order s1 + s2\n",
      "    count = 0\n",
      "    for ch in s1 + s2:\n",
      "        if ch == '(':\n",
      "            count += 1\n",
      "        else:  # ch == ')'\n",
      "            count -= 1\n",
      "        if count < 0:\n",
      "            break\n",
      "    if count == 0:\n",
      "        return 'Yes'\n",
      "\n",
      "    # Check concatenation in order s2 + s1\n",
      "    count = 0\n",
      "    for ch in s2 + s1:\n",
      "        if ch == '(':\n",
      "            count += 1\n",
      "        else:  # ch == ')'\n",
      "            count -= 1\n",
      "        if count < 0:\n",
      "            break\n",
      "    if count == 0:\n",
      "        return 'Yes'\n",
      "\n",
      "    return 'No'\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/120\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import heapq\n",
      "\n",
      "    if k <= 0 or not arr:\n",
      "        return []\n",
      "\n",
      "    # Invert the signs to use heapq as a max heap\n",
      "    inverted = [-x for x in arr]\n",
      "\n",
      "    # Use heapq.nlargest to find the k largest numbers (smallest in inverted heap)\n",
      "    largest_k = heapq.nlargest(k, inverted)\n",
      "\n",
      "    # Invert the signs back and sort in ascending order\n",
      "    result = [-x for x in largest_k]\n",
      "    result.sort()\n",
      "\n",
      "    return result\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/121\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    total = 0\n",
      "    for i, num in enumerate(lst):\n",
      "        if num % 2 == 1 and i % 2 == 0:\n",
      "            total += num\n",
      "    return total\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/122\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return sum(x for x in arr[:k] if -99 <= x <= 99)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/123\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    odd_numbers = []\n",
      "    while n != 1:\n",
      "        if n % 2 == 1:\n",
      "            odd_numbers.append(n)\n",
      "        n = n // 2 if n % 2 == 0 else 3 * n + 1\n",
      "    if 1 not in odd_numbers:\n",
      "        odd_numbers.append(1)\n",
      "    return sorted(odd_numbers)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/124\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not date:\n",
      "        return False\n",
      "\n",
      "    parts = date.split('-')\n",
      "    if len(parts) != 3:\n",
      "        return False\n",
      "\n",
      "    month, day, year = parts\n",
      "    try:\n",
      "        month = int(month)\n",
      "        day = int(day)\n",
      "        year = int(year)\n",
      "    except ValueError:\n",
      "        return False\n",
      "\n",
      "    if month < 1 or month > 12:\n",
      "        return False\n",
      "\n",
      "    if month in (1, 3, 5, 7, 8, 10, 12):\n",
      "        max_day = 31\n",
      "    elif month in (4, 6, 9, 11):\n",
      "        max_day = 30\n",
      "    else:  # month == 2\n",
      "        max_day = 29\n",
      "\n",
      "    if day < 1 or day > max_day:\n",
      "        return False\n",
      "\n",
      "    return True\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/125\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    words = txt.split()\n",
      "    if not words:\n",
      "        words = txt.split(',')\n",
      "        if not words:\n",
      "            count = sum(1 for c in txt if c.islower() and (ord(c) - ord('a')) % 2 == 1)\n",
      "            return count\n",
      "    return words\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/126\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    for i in range(len(lst) - 1):\n",
      "        if lst[i] > lst[i + 1]:\n",
      "            return False\n",
      "        if lst[i] == lst[i + 1] and i + 2 < len(lst) and lst[i + 1] == lst[i + 2]:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/127\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def is_prime(n):\n",
      "        if n <= 1:\n",
      "            return False\n",
      "        for i in range(2, int(n**0.5) + 1):\n",
      "            if n % i == 0:\n",
      "                return False\n",
      "        return True\n",
      "\n",
      "    start1, end1 = interval1\n",
      "    start2, end2 = interval2\n",
      "    inter_start = max(start1, start2)\n",
      "    inter_end = min(end1, end2)\n",
      "    if inter_end < inter_start:\n",
      "        return \"NO\"\n",
      "    length = inter_end - inter_start\n",
      "    if length <= 0:\n",
      "        return \"NO\"\n",
      "    return \"YES\" if is_prime(length) else \"NO\"\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/128\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not arr:\n",
      "        return None\n",
      "\n",
      "    sign_product = 1\n",
      "    magnitude_sum = 0\n",
      "\n",
      "    for num in arr:\n",
      "        if num > 0:\n",
      "            sign_product *= 1\n",
      "        elif num < 0:\n",
      "            sign_product *= -1\n",
      "        else:\n",
      "            sign_product *= 0\n",
      "            break  # Early exit if there's a zero\n",
      "        magnitude_sum += abs(num)\n",
      "\n",
      "    if sign_product == 0:\n",
      "        return 0\n",
      "    return magnitude_sum * sign_product\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/129\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import heapq\n",
      "\n",
      "    N = len(grid)\n",
      "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
      "    best_path = None\n",
      "\n",
      "    for i in range(N):\n",
      "        for j in range(N):\n",
      "            heap = []\n",
      "            heapq.heappush(heap, (tuple(), (i, j), 0))\n",
      "            visited = set()\n",
      "            while heap:\n",
      "                path, (x, y), step = heapq.heappop(heap)\n",
      "                if step == k:\n",
      "                    if best_path is None or path < best_path:\n",
      "                        best_path = path\n",
      "                    continue\n",
      "                if (x, y) in visited:\n",
      "                    continue\n",
      "                visited.add((x, y))\n",
      "                for dx, dy in directions:\n",
      "                    nx, ny = x + dx, y + dy\n",
      "                    if 0 <= nx < N and 0 <= ny < N:\n",
      "                        new_path = path + (grid[nx][ny],)\n",
      "                        heapq.heappush(heap, (new_path, (nx, ny), step + 1))\n",
      "\n",
      "    return list(best_path)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/130\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if n < 0:\n",
      "        return []\n",
      "    # Initialize the sequence with the known values\n",
      "    if n == 0:\n",
      "        return [1]\n",
      "    sequence = [1, 3]\n",
      "    # Compute the remaining terms\n",
      "    for i in range(2, n + 1):\n",
      "        if i % 2 == 0:\n",
      "            sequence.append(1 + i // 2)\n",
      "        else:\n",
      "            # For odd i, we need tri(i-1), tri(i-2), and tri(i+1)\n",
      "            # But we only have up to tri(i) so far, so we need to compute tri(i+1) first\n",
      "            # We can compute tri(i+1) using the even formula: tri(i+1) = 1 + (i+1)//2\n",
      "            next_val = 1 + (i + 1) // 2\n",
      "            sequence.append(sequence[-1] + sequence[-2] + next_val)\n",
      "    return sequence\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/131\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    product = 1\n",
      "    has_odd = False\n",
      "    for digit in str(n):\n",
      "        d = int(digit)\n",
      "        if d % 2 == 1:\n",
      "            if not has_odd:\n",
      "                product = d\n",
      "                has_odd = True\n",
      "            else:\n",
      "                product *= d\n",
      "    return product if has_odd else 0\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/132\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    count = 0\n",
      "    for ch in string:\n",
      "        if ch == '[':\n",
      "            count += 1\n",
      "        elif ch == ']':\n",
      "            count -= 1\n",
      "        # If count is positive, there must be a nested sequence\n",
      "        if count > 0:\n",
      "            return True\n",
      "        # If count goes negative, it means there's a closing bracket without a matching opening\n",
      "        if count < 0:\n",
      "            return False\n",
      "    # If count is zero and we've gone through the string, no nesting\n",
      "    return False\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/133\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    total = 0\n",
      "    for num in lst:\n",
      "        rounded = math.ceil(num)\n",
      "        squared = rounded ** 2\n",
      "        total += squared\n",
      "    return total\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/134\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import re\n",
      "\n",
      "    if not txt:\n",
      "        return False\n",
      "    last_char = txt[-1]\n",
      "    if not last_char.isalpha():\n",
      "        return False\n",
      "    # Check if the last character is either the only character or followed by whitespace\n",
      "    if len(txt) == 1 or re.search(r'[\\s\\t\\n\\r]', txt[-2]) is not None:\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/135\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    n = len(arr)\n",
      "    for i in range(1, n):\n",
      "        if arr[i] < arr[i - 1]:\n",
      "            return i\n",
      "    return -1\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/136\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    a = None\n",
      "    b = None\n",
      "    for num in lst:\n",
      "        if num < 0:\n",
      "            if a is None or num > a:\n",
      "                a = num\n",
      "        elif num > 0:\n",
      "            if b is None or num < b:\n",
      "                b = num\n",
      "    return (a, b)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/137\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    # Convert both to strings for consistent decimal handling\n",
      "    a_str = str(a)\n",
      "    b_str = str(b)\n",
      "\n",
      "    # Replace comma with dot in strings if present\n",
      "    if ',' in a_str:\n",
      "        a_str = a_str.replace(',', '.')\n",
      "    if ',' in b_str:\n",
      "        b_str = b_str.replace(',', '.')\n",
      "\n",
      "    # Convert to floats for comparison\n",
      "    a_val = float(a_str)\n",
      "    b_val = float(b_str)\n",
      "\n",
      "    # Compare values\n",
      "    if a_val == b_val:\n",
      "        return None\n",
      "    elif a_val > b_val:\n",
      "        # Return original type of a\n",
      "        if isinstance(a, str):\n",
      "            return a\n",
      "        else:\n",
      "            return a\n",
      "    else:\n",
      "        # Return original type of b\n",
      "        if isinstance(b, str):\n",
      "            return b\n",
      "        else:\n",
      "            return b\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/138\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return n >= 8 and n % 2 == 0\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/139\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def factorial(k):\n",
      "        if k == 0:\n",
      "            return 1\n",
      "        result = 1\n",
      "        for i in range(1, k + 1):\n",
      "            result *= i\n",
      "        return result\n",
      "\n",
      "    \"\"\"Compute the special factorial: n! * (n-1)! * ... * 1!.\"\"\"\n",
      "    product = 1\n",
      "    for i in range(n, 0, -1):\n",
      "        product *= factorial(i)\n",
      "    return product\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/140\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    result = []\n",
      "    space_count = 0\n",
      "    for ch in text:\n",
      "        if ch == ' ':\n",
      "            space_count += 1\n",
      "            if space_count <= 2:\n",
      "                result.append('_')\n",
      "            else:\n",
      "                result.append('-')\n",
      "        else:\n",
      "            result.append(ch)\n",
      "            space_count = 0\n",
      "    return ''.join(result)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/141\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import re\n",
      "\n",
      "    # Split by first dot\n",
      "    parts = file_name.split('.', 1)\n",
      "    if len(parts) != 2:\n",
      "        return 'No'\n",
      "\n",
      "    prefix, suffix = parts\n",
      "\n",
      "    # Check prefix\n",
      "    if not prefix or not prefix[0].isalpha():\n",
      "        return 'No'\n",
      "\n",
      "    # Check suffix\n",
      "    if suffix not in ['txt', 'exe', 'dll']:\n",
      "        return 'No'\n",
      "\n",
      "    # Count digits in prefix\n",
      "    if sum(c.isdigit() for c in prefix) > 3:\n",
      "        return 'No'\n",
      "\n",
      "    return 'Yes'\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/142\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    total = 0\n",
      "    for i, num in enumerate(lst):\n",
      "        if i % 3 == 0:\n",
      "            total += num ** 2\n",
      "        elif i % 4 == 0:\n",
      "            total += num ** 3\n",
      "    return total\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/143\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def is_prime(n):\n",
      "        if n < 2:\n",
      "            return False\n",
      "        for i in range(2, int(n ** 0.5) + 1):\n",
      "            if n % i == 0:\n",
      "                return False\n",
      "        return True\n",
      "\n",
      "    \"\"\"\n",
      "    Return words from the sentence whose lengths are prime numbers.\n",
      "    \"\"\"\n",
      "    words = sentence.split()\n",
      "    prime_words = [word for word in words if is_prime(len(word))]\n",
      "    return ' '.join(prime_words)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/144\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    x_num, x_den = map(int, x.split('/'))\n",
      "    n_num, n_den = map(int, n.split('/'))\n",
      "    product_num = x_num * n_num\n",
      "    product_den = x_den * n_den\n",
      "    gcd = math.gcd(product_num, product_den)\n",
      "    simplified_num = product_num // gcd\n",
      "    simplified_den = product_den // gcd\n",
      "    return simplified_den == 1\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/145\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    def sum_of_digits(n):\n",
      "        \"\"\"Returns the sum of digits of an integer n.\"\"\"\n",
      "        n = abs(n)\n",
      "        return sum(int(digit) for digit in str(n))\n",
      "\n",
      "    # Use sorted with a custom key function to sort by digit sum\n",
      "    return sorted(nums, key=sum_of_digits)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/146\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    count = 0\n",
      "    for num in nums:\n",
      "        if num > 10:\n",
      "            num_str = str(num)\n",
      "            if num_str[0] in '13579' and num_str[-1] in '13579':\n",
      "                count += 1\n",
      "    return count\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/147\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if n < 3:\n",
      "        return 0\n",
      "\n",
      "    # Compute array a\n",
      "    a = [i * i - i + 1 for i in range(1, n + 1)]\n",
      "\n",
      "    # Count residues modulo 3\n",
      "    count0 = count1 = count2 = 0\n",
      "    for num in a:\n",
      "        if num % 3 == 0:\n",
      "            count0 += 1\n",
      "        elif num % 3 == 1:\n",
      "            count1 += 1\n",
      "        else:\n",
      "            count2 += 1\n",
      "\n",
      "    # Count valid triples\n",
      "    total = 0\n",
      "    if count0 >= 3:\n",
      "        total += count0 * (count0 - 1) * (count0 - 2) // 6\n",
      "    if count1 >= 3:\n",
      "        total += count1 * (count1 - 1) * (count1 - 2) // 6\n",
      "    if count2 >= 3:\n",
      "        total += count2 * (count2 - 1) * (count2 - 2) // 6\n",
      "    if count0 >= 1 and count1 >= 1 and count2 >= 1:\n",
      "        total += count0 * count1 * count2\n",
      "\n",
      "    return total\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/148\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    planets = [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"]\n",
      "    idx1 = planets.index(planet1.lower())\n",
      "    idx2 = planets.index(planet2.lower())\n",
      "    if idx1 < 0 or idx2 < 0:\n",
      "        return ()\n",
      "    if idx1 > idx2:\n",
      "        idx1, idx2 = idx2, idx1\n",
      "    return tuple(planets[idx1+1:idx2])\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/149\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    # Filter out strings with odd lengths\n",
      "    even_length_strings = [s for s in lst if len(s) % 2 == 0]\n",
      "\n",
      "    # Sort by length then alphabetically\n",
      "    sorted_strings = sorted(even_length_strings, key=lambda x: (len(x), x))\n",
      "\n",
      "    return sorted_strings\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/150\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import math\n",
      "\n",
      "    def is_prime(n):\n",
      "        if n <= 1:\n",
      "            return False\n",
      "        for i in range(2, int(math.sqrt(n)) + 1):\n",
      "            if n % i == 0:\n",
      "                return False\n",
      "        return True\n",
      "\n",
      "    \"\"\"Return x if n is prime, otherwise return y.\"\"\"\n",
      "    return x if is_prime(n) else y\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/151\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return sum(num ** 2 for num in lst if isinstance(num, int) and num >= 0 and num % 2 == 1)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/152\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    return [abs(s - g) for s, g in zip(game, guess)]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/153\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    strongest = \"\"\n",
      "    max_strength = float('-inf')\n",
      "    for ext in extensions:\n",
      "        cap = sum(1 for c in ext if c.isupper())\n",
      "        sm = sum(1 for c in ext if c.islower())\n",
      "        strength = cap - sm\n",
      "        if strength > max_strength:\n",
      "            max_strength = strength\n",
      "            strongest = ext\n",
      "    return f\"{class_name}.{strongest}\"\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/154\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if len(b) > len(a):\n",
      "        return False\n",
      "    for i in range(len(b)):\n",
      "        rotated = b[i:] + b[:i]\n",
      "        if rotated in a:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/155\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    num = abs(num)\n",
      "    even_count = 0\n",
      "    odd_count = 0\n",
      "    for digit in str(num):\n",
      "        if int(digit) % 2 == 0:\n",
      "            even_count += 1\n",
      "        else:\n",
      "            odd_count += 1\n",
      "    return (even_count, odd_count)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/156\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    val = [\n",
      "        1000, 900, 500, 400,\n",
      "        100, 90, 50, 40,\n",
      "        10, 9, 5, 4,\n",
      "        1\n",
      "    ]\n",
      "    syms = [\n",
      "        \"m\", \"cm\", \"d\", \"cd\",\n",
      "        \"c\", \"xc\", \"l\", \"xl\",\n",
      "        \"x\", \"ix\", \"v\", \"iv\",\n",
      "        \"i\"\n",
      "    ]\n",
      "    roman = ''\n",
      "    i = 0\n",
      "    while number > 0:\n",
      "        for _ in range(number // val[i]):\n",
      "            roman += syms[i]\n",
      "            number -= val[i]\n",
      "        i += 1\n",
      "    return roman\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/157\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    sides = sorted([a, b, c])\n",
      "    return sides[2]**2 == sides[0]**2 + sides[1]**2\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/158\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if not words:\n",
      "        return \"\"\n",
      "    max_word = words[0]\n",
      "    max_unique = len(set(max_word))\n",
      "    for word in words:\n",
      "        unique_count = len(set(word))\n",
      "        if unique_count > max_unique or (unique_count == max_unique and word < max_word):\n",
      "            max_word = word\n",
      "            max_unique = unique_count\n",
      "    return max_word\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/159\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    if need <= remaining:\n",
      "        eaten = need\n",
      "    else:\n",
      "        eaten = remaining\n",
      "    total_eaten = number + eaten\n",
      "    remaining_after = max(remaining - eaten, 0)\n",
      "    return [total_eaten, remaining_after]\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/160\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import operator\n",
      "\n",
      "    if not operator or not operand or len(operator) != len(operand) - 1:\n",
      "        raise ValueError(\"Invalid input lists\")\n",
      "\n",
      "    # Construct the expression string\n",
      "    expr = str(operand[0])\n",
      "    for op, num in zip(operator, operand[1:-1]):\n",
      "        expr += f\" {op} {num}\"\n",
      "    expr += f\" {operand[-1]}\"\n",
      "\n",
      "    # Evaluate the expression\n",
      "    return eval(expr)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/161\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    has_letters = any(c.isalpha() for c in s)\n",
      "    if not has_letters:\n",
      "        return s[::-1]\n",
      "    result = []\n",
      "    for c in s:\n",
      "        if c.isalpha():\n",
      "            if c.islower():\n",
      "                result.append(c.upper())\n",
      "            else:\n",
      "                result.append(c.lower())\n",
      "        else:\n",
      "            result.append(c)\n",
      "    return ''.join(result)\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/162\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    import hashlib\n",
      "\n",
      "    if not text:\n",
      "        return None\n",
      "    return hashlib.md5(text.encode()).hexdigest()\n",
      "\n",
      "================================================================================\n",
      "\n",
      " TASK: HumanEval/163\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    even_digits = set()\n",
      "    start = min(a, b)\n",
      "    end = max(a, b)\n",
      "    for num in range(start, end + 1):\n",
      "        for digit in str(num):\n",
      "            if int(digit) % 2 == 0:\n",
      "                even_digits.add(int(digit))\n",
      "    return sorted(even_digits)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# REPLACE THIS with the path to the file you want to inspect\n",
    "# Example: \"generation_results/grpo_cot_2048.jsonl\"\n",
    "FILE_PATH = str(GEN_DIR / \"grpo_cot.jsonl\")\n",
    "\n",
    "print(f\" INSPECTING: {FILE_PATH}\\n\")\n",
    "\n",
    "with open(FILE_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        task_id = data.get(\"task_id\", \"Unknown Task\")\n",
    "        completion = data.get(\"completion\", \"\")\n",
    "\n",
    "        print(f\" TASK: {task_id}\")\n",
    "        print(\"-\" * 80)\n",
    "        # Printing completion directly renders newlines correctly\n",
    "        print(completion)\n",
    "        print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAkf4YxDndTV"
   },
   "source": [
    "# Step 10: Calculating Pass@1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1768119574414,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "L7CMTdIhnmzW"
   },
   "outputs": [],
   "source": [
    "def run_humaneval(\n",
    "    samples_file: str,\n",
    "    k: int = 1,\n",
    "    timeout: float = 3.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs HumanEval pass@k on a JSONL file of completions.\n",
    "    \"\"\"\n",
    "    results = evaluate_functional_correctness(\n",
    "        sample_file=samples_file,\n",
    "        k=[k],\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1768119575064,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "RtrrDF79nrgb"
   },
   "outputs": [],
   "source": [
    "RESULTS = []\n",
    "\n",
    "EVAL_RUNS = [\n",
    "    (\"Base\",     \"Non-CoT\", GEN_DIR / \"base_non_cot.jsonl\"),\n",
    "    (\"SFT\",      \"Non-CoT\", GEN_DIR / \"sft_non_cot.jsonl\"),\n",
    "    (\"SFT\",      \"CoT\",     GEN_DIR / \"sft_cot.jsonl\"),\n",
    "    (\"GRPO\",     \"Non-CoT\", GEN_DIR / \"grpo_non_cot.jsonl\"),\n",
    "    (\"GRPO\",     \"CoT\",     GEN_DIR / \"grpo_cot.jsonl\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "executionInfo": {
     "elapsed": 22580,
     "status": "ok",
     "timestamp": 1768119598518,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "6JZYqMdcn-Aw",
    "outputId": "b528231a-dbd4-496c-c0ee-cf2f39630d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating: Base | Non-CoT\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:00, 3146.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:03<00:00, 44.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2/base_non_cot.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 9471.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating: SFT | Non-CoT\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:00, 385.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 164/164 [00:03<00:00, 43.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2/sft_non_cot.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 11727.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating: SFT | CoT\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:00, 357.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:04<00:00, 36.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2/sft_cot.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 10071.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating: GRPO | Non-CoT\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:00, 358.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:03<00:00, 44.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2/grpo_non_cot.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 12650.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating: GRPO | CoT\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:00, 3666.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:05<00:00, 30.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2/grpo_cot.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 8831.36it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Base\",\n          \"SFT\",\n          \"GRPO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mode\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"CoT\",\n          \"Non-CoT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pass@1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11093539877231862,\n        \"min\": 0.524390243902439,\n        \"max\": 0.7987804878048781,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.75,\n          0.7987804878048781\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-7510f834-9d80-4c8b-a883-6345159c98ae\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mode</th>\n",
       "      <th>pass@1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>Non-CoT</td>\n",
       "      <td>0.52439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SFT</td>\n",
       "      <td>Non-CoT</td>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SFT</td>\n",
       "      <td>CoT</td>\n",
       "      <td>0.77439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRPO</td>\n",
       "      <td>Non-CoT</td>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRPO</td>\n",
       "      <td>CoT</td>\n",
       "      <td>0.79878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7510f834-9d80-4c8b-a883-6345159c98ae')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-7510f834-9d80-4c8b-a883-6345159c98ae button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-7510f834-9d80-4c8b-a883-6345159c98ae');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-c7e17b34-6e4e-4c71-8a82-eff19ef4594e\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c7e17b34-6e4e-4c71-8a82-eff19ef4594e')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-c7e17b34-6e4e-4c71-8a82-eff19ef4594e button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_b105161a-bd3c-4e7e-aa01-6dd242673762\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_b105161a-bd3c-4e7e-aa01-6dd242673762 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_results');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "  Model     Mode   pass@1\n",
       "0  Base  Non-CoT  0.52439\n",
       "1   SFT  Non-CoT  0.75000\n",
       "2   SFT      CoT  0.77439\n",
       "3  GRPO  Non-CoT  0.75000\n",
       "4  GRPO      CoT  0.79878"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_name, mode, path in EVAL_RUNS:\n",
    "    print(f\"\\n Evaluating: {model_name} | {mode}\")\n",
    "    res = run_humaneval(str(path), k=1)\n",
    "    RESULTS.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Mode\": mode,\n",
    "        \"pass@1\": res[\"pass@1\"],\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(RESULTS)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1768120008819,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "j3-7lFiRxIGj",
    "outputId": "0d1d337a-8480-4331-c872-69aa38c67ebb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = df_results.copy()\n",
    "df_results[\"Label_ml\"] = df_results[\"Model\"] + \" (\" + df_results[\"Mode\"] + \")\"\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(df_results[\"Label_ml\"], df_results[\"pass@1\"])\n",
    "plt.ylabel(\"pass@1\")\n",
    "plt.title(\"HumanEval pass@1 by Model and Mode\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1768120013421,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "6d20Mujl1hsZ",
    "outputId": "2f9678d7-ac7d-4dc9-eaae-8e8400715449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT_CoT_minus_SFT_NonCoT: 0.0244\n",
      "GRPO_CoT_minus_SFT_CoT: 0.0244\n",
      "GRPO_CoT_minus_Base_NonCoT: 0.2744\n"
     ]
    }
   ],
   "source": [
    "def get_score(model: str, mode: str) -> float:\n",
    "    s = df_results.loc[\n",
    "        (df_results[\"Model\"] == model) & (df_results[\"Mode\"] == mode),\n",
    "        \"pass@1\"\n",
    "    ]\n",
    "    if len(s) != 1:\n",
    "        raise ValueError(f\"Expected exactly 1 row for ({model}, {mode}), got {len(s)}.\")\n",
    "    return float(s.iloc[0])\n",
    "\n",
    "deltas = {\n",
    "    \"SFT_CoT_minus_SFT_NonCoT\":\n",
    "        get_score(\"SFT\", \"CoT\") - get_score(\"SFT\", \"Non-CoT\"),\n",
    "\n",
    "    \"GRPO_CoT_minus_SFT_CoT\":\n",
    "        get_score(\"GRPO\", \"CoT\") - get_score(\"SFT\", \"CoT\"),\n",
    "\n",
    "    \"GRPO_CoT_minus_Base_NonCoT\":\n",
    "        get_score(\"GRPO\", \"CoT\") - get_score(\"Base\", \"Non-CoT\"),\n",
    "}\n",
    "\n",
    "for k, v in deltas.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aaSM2OK4ayg"
   },
   "source": [
    "# Step 11: Fixing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1354,
     "status": "ok",
     "timestamp": 1768120574231,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "QQ_G21MH4cnb"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12520,
     "status": "ok",
     "timestamp": 1768120586753,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zDLgosHr4e3w",
    "outputId": "c0a4e427-defa-4aeb-cdfc-0bd92efb5e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLF_hG2-4gga"
   },
   "outputs": [],
   "source": [
    "# List the notebook directory to confirm the file exists\n",
    "os.listdir(\"/content/drive/MyDrive/grpo-verified-reasoner/notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpdji2u14jnS"
   },
   "outputs": [],
   "source": [
    "notebook_path = \"/content/drive/MyDrive/grpo-verified-reasoner/notebooks/06_evaluation_2.ipynb\"\n",
    "\n",
    "with open(notebook_path, \"r\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "with open(notebook_path, \"w\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Notebook fixed and saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOouY+iu3iBDbMzDEPM0Fv0",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
