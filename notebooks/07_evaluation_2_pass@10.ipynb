{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwCYejay3dmd"
   },
   "source": [
    "# Notebook: HumanEval Evaluation (pass@10)\n",
    "\n",
    "This notebook performs the **final evaluation phase** of the GRPO verifiable-reward coding project using the **HumanEval benchmark**, reporting **pass@10** results.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Evaluate and compare the functional correctness of multiple models trained under different regimes:\n",
    "\n",
    "- **Base model** (no fine-tuning)\n",
    "- **SFT model** (supervised fine-tuning warm-up)\n",
    "- **GRPO model** (verifiable-reward reinforcement learning)\n",
    "\n",
    "Each model is evaluated in both:\n",
    "- **Non-CoT mode** (direct code completion)\n",
    "- **CoT mode** (reasoning + solution, schema-constrained)\n",
    "\n",
    "The goal is to measure **pass@10 performance**, capturing each model’s ability to produce *at least one correct solution* within **10 stochastic generations**, under a strict execution harness.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Protocol\n",
    "\n",
    "1. **Prompt Construction**\n",
    "   - Non-CoT: model completes the function body directly.\n",
    "   - CoT: model is instructed to emit reasoning followed by a `<SOLUTION>` block.\n",
    "\n",
    "2. **Multi-Sample Generation**\n",
    "   - For each HumanEval task, **10 independent samples** are generated (`n=10`).\n",
    "   - Sampling uses a non-zero temperature to encourage solution diversity.\n",
    "   - vLLM batch inference is used for speed and reproducibility.\n",
    "\n",
    "3. **Output Sanitization**\n",
    "   A custom `cleaner` function enforces:\n",
    "   - Strict `<SOLUTION>` extraction (CoT only)\n",
    "   - Markdown fence removal (```python … ```)\n",
    "   - Redundant `def` removal\n",
    "   - Docstring stripping\n",
    "   - Context-aware indentation repair\n",
    "   - Final global indentation to satisfy HumanEval’s completion semantics\n",
    "\n",
    "4. **Execution-Based Scoring**\n",
    "   - All generated completions are executed against the official HumanEval tests.\n",
    "   - A problem is marked **correct** if *any* of the 10 samples passes all tests.\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics\n",
    "\n",
    "- **pass@10**: fraction of problems for which at least one of 10 generated samples is correct\n",
    "- Results are reported per model and per generation mode (CoT / non-CoT)\n",
    "\n",
    "The HumanEval harness automatically aggregates multiple samples per task; no manual grouping logic is required.\n",
    "\n",
    "---\n",
    "\n",
    "## Output Artifacts\n",
    "\n",
    "- `*_non_cot_pass10.jsonl` — non-CoT completions (10 samples per task)\n",
    "- `*_cot_pass10.jsonl` — CoT completions (10 samples per task)\n",
    "- `df_results` — consolidated evaluation table with pass@10 scores\n",
    "\n",
    "These artifacts provide a **distribution-level view** of model capability beyond single-sample accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation Notes\n",
    "\n",
    "- **pass@10 is a saturation-prone metric** on HumanEval, especially under CoT prompting.\n",
    "- Improvements observed in **pass@1** may not translate to higher pass@10 once near-ceiling performance is reached.\n",
    "- pass@10 primarily reflects *coverage*, while pass@1 reflects *policy sharpness*.\n",
    "\n",
    "Both metrics are therefore complementary and should be interpreted together.\n",
    "\n",
    "---\n",
    "\n",
    "## Result Summary\n",
    "\n",
    "- Cleaner bugs affecting CoT indentation and redundant function headers were resolved\n",
    "- All final evaluations used correctly merged, full-precision model weights\n",
    "- GRPO improves single-sample correctness (pass@1) while matching SFT at pass@10 due to benchmark saturation\n",
    "- Non-CoT pass@10 shows modest but consistent gains under GRPO\n",
    "\n",
    "These results confirm that GRPO reshapes the solution distribution without inflating benchmark scores artificially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMEo3HOH-slR"
   },
   "source": [
    "# Step 1: Mounting Google Drive and Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20862,
     "status": "ok",
     "timestamp": 1768150574051,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "yrrwKNmg-lNR",
    "outputId": "e3370d50-349f-4a77-9e6e-a2dbfcd610f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/grpo-verified-reasoner\n",
      "data\t\t\t      LICENSE\t outputs    unsloth_compiled_cache\n",
      "grpo_trainer_lora_model       models\t README.md  _unsloth_sentencepiece_temp\n",
      "huggingface_tokenizers_cache  notebooks  src\t    wandb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd /content/drive/MyDrive/grpo-verified-reasoner\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0q0FVqc67uRU"
   },
   "outputs": [],
   "source": [
    "# Install UV (Faster pip)\n",
    "!pip install --upgrade -qqq uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKwjGl3V7zI1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imojZdcn7ztn"
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y5vd4i8712_"
   },
   "outputs": [],
   "source": [
    "# os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAQLD8-a9blY"
   },
   "outputs": [],
   "source": [
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install -q unsloth vllm human-eval tqdm\n",
    "else:\n",
    "    # Version matching for Colab GPUs\n",
    "    try:\n",
    "        import numpy, PIL\n",
    "        get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "        get_pil   = f\"pillow=={PIL.__version__}\"\n",
    "    except Exception:\n",
    "        get_numpy, get_pil = \"numpy\", \"pillow\"\n",
    "\n",
    "    try:\n",
    "        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "    except Exception:\n",
    "        is_t4 = False\n",
    "\n",
    "    # A100/H100: vllm 0.10.2, T4: vllm 0.9.2 + pinned triton\n",
    "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
    "\n",
    "    !uv pip install -qqq --upgrade \\\n",
    "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers tqdm human-eval\n",
    "    !uv pip install -qqq {get_triton}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4hqdCI59tMU"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "import textwrap\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from vllm import SamplingParams\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from human_eval.data import read_problems, write_jsonl\n",
    "from human_eval.evaluation import evaluate_functional_correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1768148474914,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "3_b5DzuU-S6z",
    "outputId": "7e6851e2-9dc2-41ea-a4c1-b711b031cb60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-8c736d8a-3461-77ee-9364-fae9f27b5f54)\n"
     ]
    }
   ],
   "source": [
    "SEED = 3407\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWuA9pSe-0M8"
   },
   "source": [
    "# Step 2: Verifying GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1768148477771,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "KmlB9Uie-xLj",
    "outputId": "02652eb5-b0f8-4eb1-f92d-a3e2752f93aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA7FVu96-anB"
   },
   "source": [
    "# Step 3: Setting Up the Main Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjIWuUg0-aC4"
   },
   "outputs": [],
   "source": [
    "# HumanEval evaluation settings\n",
    "N_SAMPLES_PER_PROBLEM = 1          # pass@1 by default; raise to 5 or 10 if you later want pass@k\n",
    "MAX_NEW_TOKENS_NON_COT = 512       # Non-CoT completions are usually short (function body)\n",
    "MAX_NEW_TOKENS_COT = 2048           # CoT can be longer due to tags + full function\n",
    "\n",
    "TEMP_NON_COT = 0.8                # low sampling noise; stable for benchmarking\n",
    "TEMP_COT = 0.8                     # encourages exploration under schema (optional)\n",
    "\n",
    "TOP_P = 0.95\n",
    "MIN_P = 0.10\n",
    "\n",
    "# Stop guards (prevent rambling without clipping typical solutions)\n",
    "STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SN2W2UUc-XgA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Output paths (keep all artifacts under one folder)\n",
    "EVAL_DIR = \"data/evaluation\"\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZp2YS7SABnS"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = \"unsloth/Qwen3-4B-Base\"\n",
    "SFT_MODEL_PATH  = \"models/qwen3-4b-sft-merged-f32\"\n",
    "GRPO_MODEL_PATH = \"models/qwen3-4b-grpo-merged-f32-final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ezJrKFY8lGR"
   },
   "outputs": [],
   "source": [
    "COT_SYSTEM_PROMPT_HUMANEVAL = \"\"\"You are a code-generation engine.\n",
    "You must output your response in the following exact format:\n",
    "<START_WORKING_OUT>\n",
    "Concise reasoning steps required to solve the problem.\n",
    "</END_WORKING_OUT>\n",
    "<SOLUTION>\n",
    "Valid Python code only.\n",
    "</SOLUTION>\n",
    "Do not output anything outside these tags.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFeCRur7Aa7k"
   },
   "source": [
    "# Step 4: Loading HumanEval Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1768150756235,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "5BicgVzbAVcf",
    "outputId": "bdbb4408-b99f-4955-b941-28cf09a4c175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HumanEval problems: 164\n",
      "Example task_id: HumanEval/0\n",
      "\n",
      "--- Prompt Preview ---\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load HumanEval problems: mapping task_id -> {\"prompt\", \"test\", \"entry_point\", ...}\n",
    "problems = read_problems()  # dict: {task_id: {\"prompt\":..., \"test\":..., \"entry_point\":...}}\n",
    "# Extract task IDs as a stable list (preserves dict order for reproducible runs)\n",
    "task_ids = list(problems.keys())\n",
    "\n",
    "print(f\"Loaded HumanEval problems: {len(task_ids)}\")\n",
    "print(\"Example task_id:\", task_ids[0])\n",
    "print(\"\\n--- Prompt Preview ---\")\n",
    "print(problems[task_ids[0]][\"prompt\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0hbt-vjEf-y"
   },
   "source": [
    "# Step 5: Prompt Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6ZmWRrRAjGh"
   },
   "outputs": [],
   "source": [
    "def build_non_cot_prompt(problem: dict) -> str:\n",
    "    \"\"\"\n",
    "    Non-CoT: HumanEval-style continuation.\n",
    "    We provide ONLY the HumanEval prompt.\n",
    "    The harness will prepend this prompt again during execution.\n",
    "    \"\"\"\n",
    "    return problem[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "na1taHI7Emmv"
   },
   "outputs": [],
   "source": [
    "def build_cot_prompt(problem: dict, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    CoT: Uses the same chat template distribution as SFT/GRPO training.\n",
    "    Returns a fully formatted ChatML prompt string.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": COT_SYSTEM_PROMPT_HUMANEVAL},\n",
    "        {\"role\": \"user\", \"content\": problem[\"prompt\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_FSvWfHE24y"
   },
   "source": [
    "# Step 6: Output Post-processing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHbWyt3kH0PO"
   },
   "outputs": [],
   "source": [
    "# Stop strings (for safety against rambling)\n",
    "# These are conservative: they stop the model from starting a NEW definition / class / main block.\n",
    "STOP_STRINGS = [\"\\nclass \", \"\\ndef \", \"\\nif __name__\"]\n",
    "\n",
    "SOLUTION_RE = re.compile(r\"<SOLUTION>(.*?)</SOLUTION>\", re.DOTALL | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VN1M8SizH0Ne"
   },
   "outputs": [],
   "source": [
    "def extract_cot_solution(completion_only_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract code from <SOLUTION>...</SOLUTION>.\n",
    "    Returns \"\" on failure (schema violation → harness will fail, which is correct behavior).\n",
    "    \"\"\"\n",
    "    m = SOLUTION_RE.search(completion_only_text)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    return m.group(1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS7QqhbiH0LJ"
   },
   "outputs": [],
   "source": [
    "def truncate_on_stop_strings(text: str, stop_strings: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Stops the completion if it begins a new unrelated block (def/class/main).\n",
    "    This reduces harness crashes from rambling continuations.\n",
    "    \"\"\"\n",
    "    cut = len(text)\n",
    "    for s in stop_strings:\n",
    "        idx = text.find(s)\n",
    "        if idx != -1:\n",
    "            cut = min(cut, idx)\n",
    "    return text[:cut].rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgTiLITgN81J"
   },
   "outputs": [],
   "source": [
    "def cleaner(code: str, entry_point: str, use_cot: bool) -> str:\n",
    "    \"\"\"\n",
    "    Clean model completions for HumanEval.\n",
    "    - If use_cot: extract code inside <SOLUTION>...</SOLUTION>.\n",
    "    - Strip markdown fences and the first docstring.\n",
    "    - Remove typing imports.\n",
    "    - If the entry_point is defined, drop its signature and dedent its body.\n",
    "    - Otherwise dedent whole block relative to first real line.\n",
    "    - Finally indent by 4 spaces for harness compatibility.\n",
    "    \"\"\"\n",
    "    # A. CoT extraction: if requested, take only the content between <SOLUTION> tags.\n",
    "    if use_cot:\n",
    "        if \"<SOLUTION>\" in code:\n",
    "            code = code.split(\"<SOLUTION>\")[-1]\n",
    "            if \"</SOLUTION>\" in code:\n",
    "                code = code.split(\"</SOLUTION>\")[0]\n",
    "        else:\n",
    "            # CoT requested but missing → indicate failure.\n",
    "            return \"\"\n",
    "\n",
    "    # B. Remove markdown fences and the first docstring to avoid indentation noise.\n",
    "    code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    # Remove the first triple-quoted string (likely a module docstring or explanation).\n",
    "    code = re.sub(r'(\\s*(\"\"\"|\"\\\"\\\")[[\\s\\S]]*?\\2)', '', code, count=1)\n",
    "\n",
    "    lines = code.split('\\n')\n",
    "    filtered_lines = []\n",
    "\n",
    "    # C. Pre-filter: drop 'from typing' imports as they are irrelevant for execution.\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"from typing\"):\n",
    "            continue\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "    lines = filtered_lines\n",
    "\n",
    "    # D. Surgical dedent: if entry_point function is present, remove its header and\n",
    "    #    dedent its body relative to the first non-blank body line.\n",
    "    def_pattern = re.compile(rf\"^\\s*def\\s+{re.escape(entry_point)}(\\s*\\(|\\s*:)\")\n",
    "\n",
    "    header_index = -1\n",
    "    for i, line in enumerate(lines):\n",
    "        if def_pattern.match(line):\n",
    "            header_index = i\n",
    "            break\n",
    "\n",
    "    final_lines = []\n",
    "\n",
    "    if header_index != -1:\n",
    "        # CASE 1: Found redundant function header (GRPO-style).\n",
    "        # Keep helpers before the header unchanged.\n",
    "        pre_header_lines = lines[:header_index]\n",
    "\n",
    "        # Drop the header line; work with the body that follows.\n",
    "        post_header_lines = lines[header_index+1:]\n",
    "\n",
    "        # Determine body indentation from the first non-empty body line.\n",
    "        body_indent = 0\n",
    "        for line in post_header_lines:\n",
    "            if line.strip():\n",
    "                body_indent = len(line) - len(line.lstrip())\n",
    "                break\n",
    "\n",
    "        # Dedent each body line by body_indent (but never negative).\n",
    "        normalized_body = []\n",
    "        for line in post_header_lines:\n",
    "            if not line.strip():\n",
    "                normalized_body.append(\"\")\n",
    "                continue\n",
    "            current = len(line) - len(line.lstrip())\n",
    "            new_indent = max(0, current - body_indent)\n",
    "            normalized_body.append(\" \" * new_indent + line.lstrip())\n",
    "\n",
    "        final_lines = pre_header_lines + normalized_body\n",
    "\n",
    "    else:\n",
    "        # CASE 2: No explicit header found (SFT-style or nested definitions).\n",
    "        # Dedent entire block relative to the first substantive line.\n",
    "        global_indent = 0\n",
    "        for line in lines:\n",
    "            if line.strip() and not line.strip().startswith((\"import \", \"from \")):\n",
    "                global_indent = len(line) - len(line.lstrip())\n",
    "                break\n",
    "\n",
    "        for line in lines:\n",
    "            if not line.strip():\n",
    "                final_lines.append(\"\")\n",
    "                continue\n",
    "            current = len(line) - len(line.lstrip())\n",
    "            new_indent = max(0, current - global_indent)\n",
    "            final_lines.append(\" \" * new_indent + line.lstrip())\n",
    "\n",
    "    # E. Final: join and uniformly indent by 4 spaces for the evaluation harness.\n",
    "    result = \"\\n\".join(final_lines)\n",
    "    return textwrap.indent(result, '    ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO7zEeCjK8le"
   },
   "source": [
    "# Step 7: Defining Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZH_cf_6OCyP"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model_path: str,\n",
    "    problems: dict,\n",
    "    task_ids: list[str],\n",
    "    *,\n",
    "    use_cot: bool,\n",
    "    output_jsonl: str,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float,\n",
    "    top_p: float = 0.95,\n",
    "    min_p: float = 0.10,\n",
    "    load_in_4bit: bool = False, # Controls 4-bit quantization (Must be False for correct results)\n",
    "    dtype = torch.float16,      # Specifies precision (Must be float16 or float32)\n",
    "    gpu_memory_utilization: float = 0.8,\n",
    "    n: int = 10,                # NEW PARAMETER: Number of samples per prompt\n",
    "):\n",
    "    # Log current model, mode, and n for clarity\n",
    "    print(f\"\\n Loading: {model_path} | Mode: {'CoT' if use_cot else 'Non-CoT'} | n={n}\")\n",
    "    print(f\" Precision: {dtype} | 4-Bit Quantization: {load_in_4bit}\")\n",
    "\n",
    "    # Clear GPU memory before loading a new model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load the language model with specified precision and vLLM for inference\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_path,\n",
    "        max_seq_length=4096,\n",
    "        load_in_4bit=load_in_4bit, # Use 4-bit quantization if specified\n",
    "        dtype=dtype,               # Load model with the desired data type\n",
    "        fast_inference=True,       # Enable vLLM for faster inference\n",
    "        gpu_memory_utilization=gpu_memory_utilization,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Prepare model for inference mode\n",
    "\n",
    "    prompts = []\n",
    "    print(f\" Preparing {len(task_ids)} prompts...\")\n",
    "    # Generate prompts for each task based on CoT or Non-CoT mode\n",
    "    for task_id in task_ids:\n",
    "        problem = problems[task_id]\n",
    "        if use_cot:\n",
    "            prompt_text = build_cot_prompt(problem, tokenizer)\n",
    "        else:\n",
    "            prompt_text = build_non_cot_prompt(problem)\n",
    "        prompts.append(prompt_text)\n",
    "\n",
    "    # Define stop tokens based on generation mode (CoT or Non-CoT)\n",
    "    if use_cot:\n",
    "        stop_tokens = [\"</SOLUTION>\"]\n",
    "    else:\n",
    "        stop_tokens = [\"\\nclass\", \"\\nif __name__\", \"\\nprint\", \"\\ndef \"]\n",
    "\n",
    "    # Configure sampling parameters for vLLM generation\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        min_p=min_p,\n",
    "        max_tokens=max_new_tokens,\n",
    "        stop=stop_tokens,\n",
    "        n=n, # CRITICAL: Generate n samples per prompt\n",
    "    )\n",
    "\n",
    "    print(f\" Running vLLM Batch Generation (n={n})...\")\n",
    "    # Perform batch generation using the loaded model and sampling parameters\n",
    "    outputs = model.fast_generate(prompts, sampling_params=sampling_params)\n",
    "\n",
    "    samples = []\n",
    "    # Process each generated output\n",
    "    for i, task_id in enumerate(task_ids):\n",
    "        problem = problems[task_id]\n",
    "\n",
    "        # vLLM returns a list of 'n' outputs for each prompt\n",
    "        # We must iterate through all n completions for this specific prompt\n",
    "        for sample_idx in range(n):\n",
    "            completion_only = outputs[i].outputs[sample_idx].text\n",
    "\n",
    "            # Clean and format the completion using the 'cleaner' function\n",
    "            completion = cleaner(completion_only, problem[\"entry_point\"], use_cot)\n",
    "\n",
    "            # Store task_id, original prompt, and cleaned completion\n",
    "            samples.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"prompt\": problem[\"prompt\"],\n",
    "                \"completion\": completion\n",
    "            })\n",
    "\n",
    "    # Write all processed samples to a JSONL file\n",
    "    write_jsonl(output_jsonl, samples)\n",
    "    print(f\" Saved {len(samples)} samples to: {output_jsonl}\")\n",
    "\n",
    "    # Clean up model and tokenizer from GPU memory\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsDfO98zwS48"
   },
   "source": [
    "# Step 8: Generating JSONL Files for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czY25ERRwcWH"
   },
   "outputs": [],
   "source": [
    "GEN_DIR = Path(EVAL_DIR) / \"generations_run_2_pass@10\"\n",
    "GEN_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "491afe75b0fc49aab23e4808d2d8e9e2",
      "7f161b8db595416bab92b679db056d83",
      "b50603e2dd8d4309bf1273675a7c5e3f",
      "68ea618b5d0b4106b2b184723a636476",
      "fbcc96d849ce4403b0880864402a4da7",
      "bc797040b239464486930ee7a78baf08",
      "995377b6c8eb404b949689eb2b10438c",
      "c9615b59cffa4c71bce8db32bcb3f0e3",
      "32c23a26a8c84bbfa61e27a9cebd30ac",
      "b36503d624e24a9094b87816439dbb9c",
      "192e9f2a741a4005b651f3f33d4d7daf",
      "e6a0d9003e874dbd90b07057791f3c33",
      "1c5d50cbd6304e9ea2a47a68f3fcd923",
      "69d98974f45c4975b692b873153a5746",
      "6444c45a662646ac8e575b70cdaf63aa",
      "c012f8030b6546d08e5bb04969894d9a",
      "5dc93ede81ba425dad60198393cd2155",
      "37455381fe674c1084a5a9bf3416c24e",
      "454f0ee955c84a5f973056ebf2d3da5d",
      "f0fb2547446b4d7a8cce93892d0a603a",
      "280745e0670d4ba5bc76a96c22e24455",
      "47e7320ed9a54f7785caa41096ba373c",
      "195a5b6aea7a43d78efd59999751b793",
      "4dc9b497194749359537c39bd4b9ba1d",
      "b7c403d4414c4f3f908fa2136960537f",
      "6c5fcc500d454a35bb8e4c2c5ffa20bf",
      "1cd8bd1d9c3a429d8a441e7e1c68e667",
      "a092721fda5847efbd69917c691386db",
      "41631b65f4664c33a92e3e34b2f8b1ff",
      "6c4e0388b065444b85f736ed7a12e8bb",
      "6922e82edc304b9e85be2deb1a6fcc5e",
      "640cbe11c3da419aa749683a93538315",
      "dae5957e8df54637b7593f5258148fd9",
      "63f8db854b2048b2afe32cd170555cbb",
      "72a6f59a5783417494b440514df40ec1",
      "cf9a1e7557384120b4b04a5236843061",
      "ee06b7c2136c4067879e1873eadac348",
      "90ce28f85cae40fd86f6f16f8ac548d0",
      "df3273c83f76403c89d9a52c931d697a",
      "4211d3a64a7d447b957075f24bdbc0cc",
      "a430e462bf0443e889411cba5dfd27d5",
      "b1cea812c92440c9ae69b8ca24067194",
      "8b8c42bf2c1c4b7986eb68b30c643fb8",
      "f267fec500d64b0bb7ecee512131cdba",
      "6aa5d53ed6c141909c02b165ae5849ae",
      "bf0d7583165e4d7b9714329bde9e12b0",
      "5f7c5aadba974277bc1c18b42b562561",
      "599907cff2174787a81cdb842133b8a1",
      "f56c9f9e93114264b1f29fb4ea1f4258",
      "2f34a469d4784092aeecee867db33b7b",
      "351a5b131c114d229d62e4524c74f2d0",
      "973f9a80641d4e45b4923ca166c4fbea",
      "958c5d08a8e84c79a8ef065edb39a8b5",
      "eeab882bc59a4dd2afd18974da69d198",
      "fffd76cf122c479a808a865336edfbe7",
      "c1a19fd59b89458e898b5155a34ce270",
      "40b5c439dabe47d8aa79888db4306860",
      "9eb7c57cc9e74194af092a49f9c6674c",
      "0e2b0047e2144f15b70ebec9e4743195",
      "5f3316b9965242269bfbb89e44d55eed",
      "182d634031f64385ad2312a495b4bd83",
      "8a57448d10334d82840607c432222712",
      "490c3a086dc24ea998e9931e01a2a528",
      "ff81b90683194fc48f825052d93a4c52",
      "0d90ea9d4d90466ab601ad3661d81f97",
      "3e651cebfb594692a11b92f26f13f164",
      "07204d0cfa2f42d695e01250ae49e617",
      "b88440556e284314a0b30b2d223bee3c",
      "68e8abfa009f48a9b239f1e933ac97c2",
      "93dced07bde14411a52b30cf56e9b50d",
      "9370b4efd5ab4b9a914dfa5a51a9425f",
      "7f1df252ea684bf380f37a52f8adad78",
      "e8f8360fc6744d2f931a8b5dd86ad099",
      "8e1b7fcb9ece4740bfb7d515977ef3f8",
      "c74a7f7dafab46acb9e05cd554d7a783",
      "a45b060521da41d9b404a140fe5be456",
      "d8fbdf3af8404e15ad4ed990aead0190",
      "be4976ea4faf446784e73983044701fb",
      "1b68b0ab603143f8800cb21d73b8524b",
      "eaf895d4252d4022b0d0cf9f49d255da",
      "130e9a1add3a491884316eba58878eab",
      "3908fb5c88ea46dea3ebf0e5a0f43585",
      "fa93c561be0c46119f191e0ccb2a01e4",
      "efeec25d8dab43a8bf5e49808a6cde03",
      "4b743e0a33cd421b8d12aa5a9c824b91",
      "90f468db7c6a4f708ddefcb5ac1b8190",
      "108fcf0c89884128a976a194526ff7a2",
      "77d3fc37007841beb9ac8feb97a636ff",
      "0327731ba39f4be2be8380836491cb6b",
      "4af750f9d4bb4ab09b83f3757805b97e",
      "66460e926e6f49c79cbcadb595e8a4e7",
      "921f72a916e442f4b8b1813253336aa4",
      "b3af6ed43af7471fbcff15fdfab400ec",
      "fe1db610995641668d54775526a72035",
      "f02e9d7fab424366a57dd2f03f8c8457",
      "6f48cc2b9fb84c429912100f19f76d92",
      "2835a3a1e30d4a5ca074f4c7cfedeba9",
      "3b02acdb18ca4a0fb21b3edaa91faa02",
      "0f1d7fdde7374a07a182e26a4120fde1",
      "d5be057b91d442508537f6e41ac6b2dd",
      "46b792d6ea484a1a850d8201a060af2e",
      "4d76cee9d9e64e179832e7381d8c8115",
      "3623c8ec65c84bf282920b8e143814d2",
      "0586af31962f4533ae3bd03c735b220d",
      "7e0194a53d1146368b020f9bc5987ede",
      "807546c534f5419b974fd0d2a79d6475",
      "6bef7961c4024a52aad853281a82a821",
      "c41a602a8f0e48eab8823274ba4cc568",
      "2c5831439c1541b4bc863d72d19878e7",
      "351b52d62950436d97680d36dd4fd1cf",
      "c636521bfee04a589b48dd63775127d3",
      "c5998dfe4006431c9e1cd10a2127663f",
      "6b2689838bd14edbb246de603fc507ba",
      "ebf01a92805640c4b3c6a38f58beba73",
      "6735369b9d08449d8c5589871dceee91",
      "0c966dde498f41a68b0bf717d68b9c53",
      "84898cf44c5a4fa0bd49a7c5c37f019a",
      "d683fce49b754d2282c44f2323799be5",
      "e51f4257e160452aa6463279027c5afc",
      "364ec6eb0c24442393caa4e23026b622",
      "b3939888454a4bc4a0e0441be25d5faa",
      "c0bb0e095ded4fe7b1012e3a3ab9c50f",
      "5f192eafc50849cfb98cc4f3f37dd08d",
      "d877eff69afb49b58686b7696246e364",
      "5536b0856b8e4f22ad0d88899006c204",
      "929ddac6bfc14f6ca59531772f9f26da",
      "a0733ceff689413ba21791a10f49179d",
      "b7df782a24f043ae8fc7895976eda74a",
      "fbf5b5c97c6d433faf65477e3a442705",
      "d673408df38343ff85f89b99e1b9e00a",
      "9a88f162935a4beebf22729f676d5c2a",
      "cc4b68b5dfc641ad85c0321757f8b01f",
      "47adfdcc045d49d985c8e6e468698bad",
      "546470486265413fab1295cd7f446f12",
      "dd2a5b3bdb404d368512852f6e979e2a",
      "a4d3a0fad69e49309fe45aef4529531c",
      "d2f5b2391b5b4ca68dd2d009731582a8",
      "6b8491715e7b4024a8adb4464b8ce798",
      "d8ec08b818d44f24ad5b21dab5176f17",
      "37a01573ec514a85b3df43afb0022e9f",
      "0afe1b6e89b74278a714944944d17903",
      "4c03e7b0308b425984a77e9810891147",
      "9983821ae65b4c188fd6d4cfd4a4bcda",
      "fa423ece4c7e4b58a72d87fd8fdf6891",
      "f24e3b7a4c714cb383b28947cf5ded47",
      "8a18fedc02bc4ce5b82a78db7e816ebb",
      "2649156f981341e68fc15a8029fb1f3d",
      "a0cab70f8e1a418d97b6a287004596c9",
      "502f7ae1168d4c89b7bed553352c1807",
      "3acc0d8d6a354616851e229333a59001",
      "4627b46d0df74191bce56891197d01ff",
      "ff6e88ac189f4ca1ae200db6492b51c8",
      "e12825a57e884e86aa3e0beeb5675e6c",
      "d51639d52350478381856221974a19f2"
     ]
    },
    "executionInfo": {
     "elapsed": 173550,
     "status": "ok",
     "timestamp": 1768150195400,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "s2O5o6iww1uQ",
    "outputId": "5c8335c9-bbf1-4c69-d35e-3aa650ec6312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: unsloth/Qwen3-4B-Base | Mode: Non-CoT | n=10\n",
      " Precision: torch.float16 | 4-Bit Quantization: False\n",
      "INFO 01-11 16:47:03 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-11 16:47:03 [vllm_utils.py:731] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Qwen3-4B-Base with actual GPU utilization = 79.54%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 56.03 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-11 16:47:17 [utils.py:328] non-default args: {'dtype': torch.float16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.7954304147054094, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/Qwen3-4B-Base'}\n",
      "INFO 01-11 16:47:35 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-11 16:47:35 [__init__.py:2767] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 01-11 16:47:35 [__init__.py:1815] Using max model len 4096\n",
      "INFO 01-11 16:47:37 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-11 16:47:37 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491afe75b0fc49aab23e4808d2d8e9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a0d9003e874dbd90b07057791f3c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195a5b6aea7a43d78efd59999751b793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f8db854b2048b2afe32cd170555cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa5d53ed6c141909c02b165ae5849ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a19fd59b89458e898b5155a34ce270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07204d0cfa2f42d695e01250ae49e617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:47:42 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/Qwen3-4B-Base', speculative_config=None, tokenizer='unsloth/Qwen3-4B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen3-4B-Base, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n",
      "INFO 01-11 16:47:42 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-11 16:47:42 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-11 16:47:43 [gpu_model_runner.py:2338] Starting to load model unsloth/Qwen3-4B-Base...\n",
      "INFO 01-11 16:47:43 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 01-11 16:47:43 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "INFO 01-11 16:47:44 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4976ea4faf446784e73983044701fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0327731ba39f4be2be8380836491cb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:48:03 [weight_utils.py:369] Time spent downloading weights for unsloth/Qwen3-4B-Base: 19.492317 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5be057b91d442508537f6e41ac6b2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c636521bfee04a589b48dd63775127d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:48:08 [default_loader.py:268] Loading weights took 3.98 seconds\n",
      "INFO 01-11 16:48:08 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-11 16:48:09 [gpu_model_runner.py:2392] Model loading took 7.8056 GiB and 24.560947 seconds\n",
      "INFO 01-11 16:48:24 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/6bdcbe920b/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-11 16:48:24 [backends.py:550] Dynamo bytecode transform time: 13.64 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 510.48it/s, triton_poi_fused_view_6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:48:25 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 504.51it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 517.04it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 493.39it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 477.86it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 482.06it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 481.57it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 502.89it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 454.35it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 470.41it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 498.02it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 525.32it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 526.90it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 525.21it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 514.62it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 511.99it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 471.63it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 535.56it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 543.89it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 532.80it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 540.90it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 493.64it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 516.15it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 535.24it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 508.48it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 535.15it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 524.74it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 509.73it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 505.28it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 482.13it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 479.85it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 490.97it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 505.50it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 540.27it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 525.13it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 511.38it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 524.50it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:48:34 [backends.py:215] Compiling a graph for dynamic shape takes 9.56 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:48:38 [monitor.py:34] torch.compile takes 23.19 s in total\n",
      "INFO 01-11 16:48:39 [gpu_worker.py:298] Available KV cache memory: 54.53 GiB\n",
      "INFO 01-11 16:48:40 [kv_cache_utils.py:864] GPU KV cache size: 397,072 tokens\n",
      "INFO 01-11 16:48:40 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 96.94x\n",
      "INFO 01-11 16:48:40 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:06<00:00,  5.71it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:48:50 [gpu_model_runner.py:3118] Graph capturing finished in 10 secs, took 0.65 GiB\n",
      "INFO 01-11 16:48:50 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 10 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:48:52 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.7954304147054094, 63.09 GiB). Actual usage is 7.81 GiB for weight, 0.74 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.65 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=57691607244` to fit into requested memory, or `--kv-cache-memory=74548069376` to fully utilize gpu memory. Current kv cache memory in use is 58551439564 bytes.\n",
      "INFO 01-11 16:48:52 [core.py:218] init engine (profile, create kv cache, warmup model) took 42.78 seconds\n",
      "INFO 01-11 16:48:54 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-11 16:48:54 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'input_layernorm', 'norm', 'post_feedforward_layernorm', 'post_attention_layernorm', 'ffn_norm', 'layer_norm2', 'q_norm', 'k_norm', 'attention_norm', 'norm1', 'post_layernorm', 'pre_feedforward_layernorm', 'norm2']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bb0e095ded4fe7b1012e3a3ab9c50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at unsloth/Qwen3-4B-Base and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'cross_attn_input_layernorm', 'input_layernorm', 'norm', 'post_feedforward_layernorm', 'post_attention_layernorm', 'ffn_norm', 'layer_norm2', 'q_norm', 'k_norm', 'attention_norm', 'norm1', 'post_layernorm', 'pre_feedforward_layernorm', 'cross_attn_post_attention_layernorm', 'norm2']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation (n=10)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47adfdcc045d49d985c8e6e468698bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa423ece4c7e4b58a72d87fd8fdf6891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1640 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3552671664.py:21: FutureWarning: Possible nested set at position 16\n",
      "  code = re.sub(r'(\\s*(\"\"\"|\"\\\"\\\")[[\\s\\S]]*?\\2)', '', code, count=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1640 samples to: data/evaluation/generations_run_2_pass@10/base_non_cot_pass@10.jsonl\n"
     ]
    }
   ],
   "source": [
    "# BASE MODEL (Non-CoT)\n",
    "evaluate_model(\n",
    "    model_path=BASE_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=False,\n",
    "    output_jsonl=str(GEN_DIR / \"base_non_cot_pass@10.jsonl\"),\n",
    "    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n",
    "    temperature=TEMP_NON_COT,\n",
    "    n = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b4949e9447f346539ae66a4975568ca9",
      "e488a5d63f87475da06e53ebd9d79cd8",
      "105dcfd46d5c4ce1880fb04295e83093",
      "ed2094595ff44c948b2ab0517a5409cc",
      "576a0458dd01416883a4ed8fba02e793",
      "697417819d9f497886212874fd8093fb",
      "1c3c7ed823284a7ba8b50100f801a640",
      "f2e61ea4665f48149741550bf4710333",
      "094fa0935b9545ab962db04d5eedeebc",
      "480d183f9cbe46bfa15aff9dbb9a6bd3",
      "234cb8e2660a49158d683d3989a643aa",
      "95fe496085f1475589916b19b4bf0dbd",
      "d8bbda7d92cb47969b40a93d0293180e",
      "feedfdd0e1464246adb3bf74527f7b02",
      "ad9b99b18616488b99c557563133aeaa",
      "f385d73506f5426497a32548dbd3f50d",
      "3edf398531ed424ba0b063b28dd3fa34",
      "07ad871857a24087b199b83f5fedaeda",
      "146485b577fa4721a68d024747e92d5a",
      "294786be5ee7481c806d44bedc4e4dca",
      "447acef18f894a27ab1345e66646d775",
      "aa7e6f391ba7455ba671711fb3fc4e75",
      "3d0326fff04f4601afcca1f427f60b5e",
      "6f011db6dd384c2fb600924ad05f9372",
      "becc2fb4c10e49ff9bf5e8be12a53493",
      "8daffcee15b34733b4a0478a8ec61e0c",
      "05aa3f65b2ec4b398d1948bc4715424f",
      "12771d297c6a42989a8261e0e1a98b78",
      "ec42db80b1f74f23b85e1bdfd1b6fce6",
      "52fda7d188f44bfa93e234cdf18ea6fa",
      "ba6533a7fe5a4a0f9d8436d8cf58ef8b",
      "1a92097edbfe458c8c98442130c8e5e6",
      "433424440f2341c381935817659916e0",
      "403ada4a9f3e4bf3b28d55d2db832de5",
      "a16e0e23964f4aec8f72dc8441422ea5",
      "37d05a9758ba44d9b00e3fed1ebb9cdc",
      "6ab0cb31592a4e08bc91d5b58eaeac4c",
      "fb1110fdbbd0499292391213c32350de",
      "86d774ac415b4650b63b7e83a5b5198a",
      "12abefc9c39141ae82060c1620b22c97",
      "6dd25193bd514eab8b7e7ee45e68e603",
      "3fee223b6251407c93984ad89839aaca",
      "da6425b1058f4c08aee285f12c75c0a7",
      "6cbddecc616e4595801d258fe6551141"
     ]
    },
    "executionInfo": {
     "elapsed": 142758,
     "status": "ok",
     "timestamp": 1768149956893,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "yeEE_WbAxYOX",
    "outputId": "309a033a-812d-4851-d4d2-864b6d2028d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: models/qwen3-4b-sft-merged-f32 | Mode: Non-CoT | n=10\n",
      " Precision: torch.float16 | 4-Bit Quantization: False\n",
      "INFO 01-11 16:43:34 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-11 16:43:34 [vllm_utils.py:731] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading models/qwen3-4b-sft-merged-f32 with actual GPU utilization = 15.89%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 32.\n",
      "Unsloth: vLLM's KV Cache can use up to 5.54 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-11 16:43:46 [utils.py:328] non-default args: {'dtype': torch.float16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.15889204639455687, 'max_num_batched_tokens': 4096, 'max_num_seqs': 32, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'models/qwen3-4b-sft-merged-f32'}\n",
      "INFO 01-11 16:43:46 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 01-11 16:43:46 [__init__.py:2764] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-11 16:43:46 [__init__.py:1815] Using max model len 4096\n",
      "INFO 01-11 16:43:46 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "WARNING 01-11 16:43:46 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-11 16:43:47 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='models/qwen3-4b-sft-merged-f32', speculative_config=None, tokenizer='models/qwen3-4b-sft-merged-f32', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/qwen3-4b-sft-merged-f32, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "INFO 01-11 16:43:48 [gpu_model_runner.py:2338] Starting to load model models/qwen3-4b-sft-merged-f32...\n",
      "INFO 01-11 16:43:49 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4949e9447f346539ae66a4975568ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:43:59 [default_loader.py:268] Loading weights took 9.76 seconds\n",
      "INFO 01-11 16:44:00 [gpu_model_runner.py:2392] Model loading took 7.7976 GiB and 9.940193 seconds\n",
      "INFO 01-11 16:44:14 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/343b7bb7ec/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-11 16:44:14 [backends.py:550] Dynamo bytecode transform time: 12.66 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 476.46it/s, triton_poi_fused_view_6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:44:15 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 506.50it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 488.35it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 488.10it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 468.60it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 464.86it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 493.50it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 478.45it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 466.71it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 441.26it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 489.71it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 502.14it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 468.16it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 461.17it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 508.85it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 451.31it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 469.33it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 466.02it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 448.75it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 458.71it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 457.62it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 492.87it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 496.27it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 500.26it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 515.33it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 531.22it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 517.87it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 499.40it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 496.71it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 512.05it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 532.35it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 468.11it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 462.45it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 505.13it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 510.02it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 495.92it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 449.69it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:44:24 [backends.py:215] Compiling a graph for dynamic shape takes 9.08 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:44:27 [monitor.py:34] torch.compile takes 21.74 s in total\n",
      "INFO 01-11 16:44:29 [gpu_worker.py:298] Available KV cache memory: 4.44 GiB\n",
      "INFO 01-11 16:44:30 [kv_cache_utils.py:864] GPU KV cache size: 32,320 tokens\n",
      "INFO 01-11 16:44:30 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 7.89x\n",
      "INFO 01-11 16:44:30 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "INFO 01-11 16:44:30 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  5.61it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:44:33 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.05 GiB\n",
      "INFO 01-11 16:44:33 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 3 secs.\n",
      "INFO 01-11 16:44:33 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 3 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:44:35 [gpu_worker.py:391] Free memory on device (15.75/79.32 GiB) on startup. Desired GPU memory utilization is (0.15889204639455687, 12.6 GiB). Actual usage is 7.8 GiB for weight, 0.36 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=4555665919` to fit into requested memory, or `--kv-cache-memory=7938765312` to fully utilize gpu memory. Current kv cache memory in use is 4767478271 bytes.\n",
      "INFO 01-11 16:44:36 [core.py:218] init engine (profile, create kv cache, warmup model) took 35.27 seconds\n",
      "INFO 01-11 16:44:36 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-11 16:44:36 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm', 'k_norm', 'norm2', 'layer_norm1', 'attention_norm', 'norm', 'post_attention_layernorm', 'post_layernorm', 'norm1', 'q_norm', 'layer_norm2', 'ffn_norm', 'input_layernorm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fe496085f1475589916b19b4bf0dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at models/qwen3-4b-sft-merged-f32 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm', 'k_norm', 'norm2', 'cross_attn_post_attention_layernorm', 'cross_attn_input_layernorm', 'layer_norm1', 'attention_norm', 'norm', 'post_attention_layernorm', 'post_layernorm', 'norm1', 'q_norm', 'layer_norm2', 'ffn_norm', 'input_layernorm']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation (n=10)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0326fff04f4601afcca1f427f60b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403ada4a9f3e4bf3b28d55d2db832de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1640 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1640 samples to: data/evaluation/generations_run_2_pass@10/sft_non_cot_pass@10.jsonl\n"
     ]
    }
   ],
   "source": [
    "# SFT MODEL (Non-CoT)\n",
    "evaluate_model(\n",
    "    model_path=SFT_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=False,\n",
    "    output_jsonl=str(GEN_DIR / \"sft_non_cot_pass@10.jsonl\"),\n",
    "    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n",
    "    temperature=TEMP_NON_COT,\n",
    "    load_in_4bit=False,    # Do not compress\n",
    "    dtype=torch.float16,   # Do not round off\n",
    "    n = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6a37de687b934ec9ab45a0b9762956e5",
      "337181da71ed4ae090acd7f6816137f2",
      "f15c797f7af94e35b6e93dad2eefc3f0",
      "50662433c13f4531b9c8ca384a7e6c0e",
      "c27f9720c272430aa6aef106d7ae78d9",
      "1daa6a7188c641cbb3321e5fe6e7431b",
      "66d9a9563962438c82d9d0d3cddb9ad9",
      "3ec6a8dca2384912b0ddf61150cb3126",
      "13d7b9385e82425db858f8c03254437c",
      "1606a95f1fe342bd96022e836bbd13ad",
      "78379c21cfa840229e0e631031adcd29",
      "f3735678a78840fc9af73d3df9ccffc3",
      "473d16b7fd5d41ab9da88b71f7bc04c8",
      "e8392f05abac45d6afa85af90af37053",
      "5590f79c3f5548939e845c858a05c12e",
      "40c0efc681c542d09b6de590550b7a78",
      "bdebb372f58843ce841893e8f2d54833",
      "f1dbb8f9e3e44eaba6b1a448d8c811bb",
      "9f4c032f85a6418393531b0277f2c0a9",
      "c25f0b13766d45e6881778491aafa700",
      "013b524837734b97a9c063bd2201a1e9",
      "a9a78873f58b4407829e7af6298b80aa",
      "00fe5646aba2433caa0bd0e869566b21",
      "7739958f015743448b7a34b15906dd54",
      "6185a532e84e4882833fd80613bf5420",
      "8712ef2a1f584c2eaba7f7ced0e00a81",
      "b517fea1434f44abb24cd317c2236e55",
      "2fde4701fd96415eb48dfaef619ea90a",
      "02b92e48441c4b27a9346e004095bf3f",
      "5ac7e4c43f4340808f5f53bd19f41099",
      "184c3f88147e4901b40a2af1ec5add6e",
      "3177075433c64a7190f4dd0c2168edbb",
      "bb88b685b6d0422385037a518368b528",
      "1aab388627e54bd68c46ada451229633",
      "9bf699196a0e4331b1cca23df48db587",
      "8c8838ff6c94433a885d29b4102f5626",
      "f3f29378837f4c7c96c8e98b547162c3",
      "8968d5c902334fb19fb512c43289c706",
      "cd9fd058c8ad483a917e99f95eb46d26",
      "4869b5f6c4be4820ab0ddedc7a6ba04e",
      "64ca11b0c2824f27a3ecb33f33ee747b",
      "03f9ab80a9144f9a92d4c3bb675881f0",
      "515f8b6693dc4549b848205ffa7ee52a",
      "1e7a793f50f5464b914b47d4f2ca8c85"
     ]
    },
    "executionInfo": {
     "elapsed": 369401,
     "status": "ok",
     "timestamp": 1768149785737,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "LwL8oA_SMkdv",
    "outputId": "2413cd8a-1d69-4efc-fa1d-882ff5e4d2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: models/qwen3-4b-sft-merged-f32 | Mode: CoT | n=10\n",
      " Precision: torch.float16 | 4-Bit Quantization: False\n",
      "INFO 01-11 16:36:58 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-11 16:36:58 [vllm_utils.py:731] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading models/qwen3-4b-sft-merged-f32 with actual GPU utilization = 79.54%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 56.03 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-11 16:37:11 [utils.py:328] non-default args: {'dtype': torch.float16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.7954304147054094, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'models/qwen3-4b-sft-merged-f32'}\n",
      "INFO 01-11 16:37:27 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:37:27 [__init__.py:2764] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-11 16:37:27 [__init__.py:1815] Using max model len 4096\n",
      "INFO 01-11 16:37:28 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-11 16:37:28 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-11 16:37:32 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='models/qwen3-4b-sft-merged-f32', speculative_config=None, tokenizer='models/qwen3-4b-sft-merged-f32', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/qwen3-4b-sft-merged-f32, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n",
      "INFO 01-11 16:37:32 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-11 16:37:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-11 16:37:32 [gpu_model_runner.py:2338] Starting to load model models/qwen3-4b-sft-merged-f32...\n",
      "INFO 01-11 16:37:33 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 01-11 16:37:33 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a37de687b934ec9ab45a0b9762956e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:40:30 [default_loader.py:268] Loading weights took 175.81 seconds\n",
      "INFO 01-11 16:40:30 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-11 16:40:31 [gpu_model_runner.py:2392] Model loading took 7.8056 GiB and 176.670305 seconds\n",
      "INFO 01-11 16:40:45 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/0ac4129559/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-11 16:40:45 [backends.py:550] Dynamo bytecode transform time: 13.51 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 480.92it/s, triton_poi_fused_view_6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:40:47 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 470.09it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 513.88it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 513.41it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 518.96it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 514.03it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 519.95it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 515.81it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 489.91it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 507.76it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 525.00it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 528.82it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 538.74it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 547.34it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 514.43it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 516.77it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 518.79it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 556.39it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 556.79it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 537.50it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 494.10it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 508.76it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 532.39it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 525.77it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 534.68it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 525.60it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 507.80it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 510.57it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 520.94it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 520.86it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 549.80it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 502.83it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 523.68it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 523.16it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 517.29it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 542.21it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 545.44it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:40:56 [backends.py:215] Compiling a graph for dynamic shape takes 9.44 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:40:59 [monitor.py:34] torch.compile takes 22.94 s in total\n",
      "INFO 01-11 16:41:01 [gpu_worker.py:298] Available KV cache memory: 54.53 GiB\n",
      "INFO 01-11 16:41:02 [kv_cache_utils.py:864] GPU KV cache size: 397,072 tokens\n",
      "INFO 01-11 16:41:02 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 96.94x\n",
      "INFO 01-11 16:41:02 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:06<00:00,  5.80it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:41:11 [gpu_model_runner.py:3118] Graph capturing finished in 9 secs, took 0.65 GiB\n",
      "INFO 01-11 16:41:11 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 9 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:41:13 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.7954304147054094, 63.09 GiB). Actual usage is 7.81 GiB for weight, 0.74 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.65 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=57691607244` to fit into requested memory, or `--kv-cache-memory=74548069376` to fully utilize gpu memory. Current kv cache memory in use is 58551439564 bytes.\n",
      "INFO 01-11 16:41:13 [core.py:218] init engine (profile, create kv cache, warmup model) took 42.02 seconds\n",
      "INFO 01-11 16:41:14 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-11 16:41:14 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm', 'k_norm', 'norm2', 'layer_norm1', 'attention_norm', 'norm', 'post_attention_layernorm', 'post_layernorm', 'norm1', 'q_norm', 'layer_norm2', 'ffn_norm', 'input_layernorm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3735678a78840fc9af73d3df9ccffc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at models/qwen3-4b-sft-merged-f32 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm', 'k_norm', 'norm2', 'cross_attn_post_attention_layernorm', 'cross_attn_input_layernorm', 'layer_norm1', 'attention_norm', 'norm', 'post_attention_layernorm', 'post_layernorm', 'norm1', 'q_norm', 'layer_norm2', 'ffn_norm', 'input_layernorm']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation (n=10)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fe5646aba2433caa0bd0e869566b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aab388627e54bd68c46ada451229633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1640 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3552671664.py:21: FutureWarning: Possible nested set at position 16\n",
      "  code = re.sub(r'(\\s*(\"\"\"|\"\\\"\\\")[[\\s\\S]]*?\\2)', '', code, count=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1640 samples to: data/evaluation/generations_run_2_pass@10/sft_cot_pass@10.jsonl\n"
     ]
    }
   ],
   "source": [
    "# SFT MODEL (CoT)\n",
    "evaluate_model(\n",
    "    model_path=SFT_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=True,\n",
    "    output_jsonl=str(GEN_DIR / \"sft_cot_pass@10.jsonl\"),\n",
    "    max_new_tokens=MAX_NEW_TOKENS_COT,\n",
    "    temperature=TEMP_COT,\n",
    "    load_in_4bit=False,    # Do not compress\n",
    "    dtype=torch.float16,   # Do not round off\n",
    "    n = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9b91128b6ebe44adab81abcac7f786ca",
      "64cc0f08999a443fbeb5dddf246e39c9",
      "d6be224261fc4874ad9eff7ed680ae46",
      "25cfe09960444569bdd7d6eb0272ba6d",
      "7a5b3490db7147ee836a2324a61a86a5",
      "414ab32d45b14f4bbb6fc4dc587ed1c4",
      "ff4fae4158c042caa91ff63d4e67cfc8",
      "f38aa90438a24bfa8dfb7bf7aeee8e3b",
      "9025ee5e6d3e4cd699f1d2c7f051c36c",
      "0dd7a98132594f019e28a8052ef22ccf",
      "1ed5eb1c28e143f4a263c0c33b21f929",
      "905354fe523a458d8c9da2e93d1dc829",
      "de8dd0814475421dbe07c4005563e7e2",
      "3127a9c4d2a14ce3834f507fbcbf4daf",
      "6abf2784cfb84512b685d2b3d9975578",
      "7baa97e9b565468aa3fa8ec33485c2aa",
      "9345d88e59d84078a9adad84f193222f",
      "572e2ec66b2649298422e355e07b1610",
      "bee0e93de550452e986a7a257a650e64",
      "bf1ebaa8336a48719c5c13404c640c8f",
      "7e425b14ad974e63becf4db7ddd30c31",
      "af63fbff1e8f45fe8a0c5b3436241d4f",
      "699f1a3b63e64648b01c6f79c03dba86",
      "79b4f56c42284c4c84aa79df3dcc48b4",
      "fe25c33915e846ef847af47cb131174a",
      "0dee251fa25647be87dc53a57903c7d5",
      "5f2d44d440ab4f5296cd403265a0eda8",
      "cdf677f9f6c0420291a74847b2a6d7cb",
      "20748f6c0a974f4f9ac770cdbe772804",
      "c2252e3a3c4e462cba39a19270b14859",
      "c986dabd04614964a94efd8161d9fe8d",
      "4acbb5cd4acb4025b26f12abedd75596",
      "19eaba2d46124714b13fb8e8ff84b3a2",
      "1ede286d134f4a5cb7e606bf3818cc8c",
      "e35ff2908fa141f885c0ab9681ba61ff",
      "203af8c602c643afac8df8f451979666",
      "2ac5b8e9b2c744f0a558a112b1d218f8",
      "9802156434ad4232b62942a50972857f",
      "b7de3ba09b124e60b9864b9dc4aeb6a4",
      "3e4ab4f5b2c445f8b5bd02726e224860",
      "3e01be3230ee44a1ad804a77d81a71d5",
      "fb2285a31e254ad99ecdd3bbe1ee40c3",
      "214432f326064f898335af7d7f19a117",
      "2fa6399a980e4cffa086c95b9ce3c803"
     ]
    },
    "executionInfo": {
     "elapsed": 204794,
     "status": "ok",
     "timestamp": 1768149337645,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "8KMQzj0nVx6C",
    "outputId": "314b07e9-7523-4e4d-a27a-a97b56b523e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: models/qwen3-4b-grpo-merged-f32-final | Mode: Non-CoT | n=10\n",
      " Precision: torch.float16 | 4-Bit Quantization: False\n",
      "INFO 01-11 16:32:13 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-11 16:32:13 [vllm_utils.py:731] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading models/qwen3-4b-grpo-merged-f32-final with actual GPU utilization = 15.89%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 32.\n",
      "Unsloth: vLLM's KV Cache can use up to 5.54 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-11 16:32:26 [utils.py:328] non-default args: {'dtype': torch.float16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.15885264811099853, 'max_num_batched_tokens': 4096, 'max_num_seqs': 32, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'models/qwen3-4b-grpo-merged-f32-final'}\n",
      "INFO 01-11 16:32:26 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 01-11 16:32:26 [__init__.py:2764] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-11 16:32:26 [__init__.py:1815] Using max model len 4096\n",
      "INFO 01-11 16:32:26 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "WARNING 01-11 16:32:26 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-11 16:32:27 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='models/qwen3-4b-grpo-merged-f32-final', speculative_config=None, tokenizer='models/qwen3-4b-grpo-merged-f32-final', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/qwen3-4b-grpo-merged-f32-final, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "INFO 01-11 16:32:28 [gpu_model_runner.py:2338] Starting to load model models/qwen3-4b-grpo-merged-f32-final...\n",
      "INFO 01-11 16:32:29 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b91128b6ebe44adab81abcac7f786ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:32:39 [default_loader.py:268] Loading weights took 10.13 seconds\n",
      "INFO 01-11 16:32:41 [gpu_model_runner.py:2392] Model loading took 7.7976 GiB and 10.302335 seconds\n",
      "INFO 01-11 16:32:55 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/759d6af8cf/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-11 16:32:55 [backends.py:550] Dynamo bytecode transform time: 12.65 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 314.94it/s, triton_poi_fused_view_6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:32:58 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 365.46it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 482.18it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 481.37it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 487.19it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 502.54it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 484.97it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 490.06it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 500.05it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 484.07it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 499.08it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 495.53it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 508.46it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 500.79it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 504.12it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 502.30it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 476.89it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 477.20it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 481.10it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 497.99it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 477.00it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 488.65it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 504.00it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 483.61it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 472.09it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 481.00it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 465.93it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 467.11it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 473.15it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 528.15it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 512.49it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 519.30it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 484.77it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 455.46it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 459.29it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 460.78it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 374.53it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:33:46 [backends.py:215] Compiling a graph for dynamic shape takes 50.06 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:34:00 [monitor.py:34] torch.compile takes 62.71 s in total\n",
      "INFO 01-11 16:34:03 [gpu_worker.py:298] Available KV cache memory: 4.43 GiB\n",
      "INFO 01-11 16:34:04 [kv_cache_utils.py:864] GPU KV cache size: 32,208 tokens\n",
      "INFO 01-11 16:34:04 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 7.86x\n",
      "INFO 01-11 16:34:04 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "INFO 01-11 16:34:04 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  5.60it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:34:07 [gpu_model_runner.py:3118] Graph capturing finished in 3 secs, took 0.05 GiB\n",
      "INFO 01-11 16:34:07 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 3 secs.\n",
      "INFO 01-11 16:34:07 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 3 secs.\n",
      "INFO 01-11 16:34:10 [gpu_worker.py:391] Free memory on device (15.75/79.32 GiB) on startup. Desired GPU memory utilization is (0.15885264811099853, 12.6 GiB). Actual usage is 7.8 GiB for weight, 0.37 GiB for peak activation, 0.01 GiB for non-torch memory, and 0.05 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=4539727564` to fit into requested memory, or `--kv-cache-memory=7921988096` to fully utilize gpu memory. Current kv cache memory in use is 4751539916 bytes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:34:10 [core.py:218] init engine (profile, create kv cache, warmup model) took 88.78 seconds\n",
      "INFO 01-11 16:34:11 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-11 16:34:11 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_attention_layernorm', 'post_feedforward_layernorm', 'input_layernorm', 'norm1', 'norm', 'q_norm', 'norm2', 'layer_norm2', 'layer_norm1', 'k_norm', 'ffn_norm', 'post_layernorm', 'attention_norm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905354fe523a458d8c9da2e93d1dc829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at models/qwen3-4b-grpo-merged-f32-final and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['cross_attn_input_layernorm', 'cross_attn_post_attention_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'post_feedforward_layernorm', 'input_layernorm', 'norm1', 'norm', 'q_norm', 'norm2', 'layer_norm2', 'layer_norm1', 'k_norm', 'ffn_norm', 'post_layernorm', 'attention_norm']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation (n=10)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699f1a3b63e64648b01c6f79c03dba86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ede286d134f4a5cb7e606bf3818cc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1640 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1640 samples to: data/evaluation/generations_run_2_pass@10/grpo_non_cot_pass@10.jsonl\n"
     ]
    }
   ],
   "source": [
    "# GRPO MODEL (Non-CoT)\n",
    "evaluate_model(\n",
    "    model_path=GRPO_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=False,\n",
    "    output_jsonl=str(GEN_DIR / \"grpo_non_cot_pass@10.jsonl\"),\n",
    "    max_new_tokens=MAX_NEW_TOKENS_NON_COT,\n",
    "    temperature=TEMP_NON_COT,\n",
    "    load_in_4bit=False,    # Do not compress\n",
    "    dtype=torch.float16,   # Do not round off\n",
    "    n = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "967b9debbde445c5b545f91bd08f856d",
      "ffc761ba9fc2448e9c1b4a753fce746b",
      "cbc2f336ed0047e485579f2386ad71a7",
      "eed98e42a5fb4c3a8ce767dbcbfe8370",
      "6823e2b470e641d3ae72f507a7a4c825",
      "459953903d2340f98fa68f55e77d421d",
      "4568921de63f4e599580024faef03a2b",
      "95eecc08c86144daadcb6cb434e6e474",
      "af033f87407844ccb2c221e3ff52b283",
      "2a7c9f3de63b41259efd43b7d34749e9",
      "3112f80a0241441bb75c684fd98c69d8",
      "643d5ac7e97a415fb75950ae5f2cdc05",
      "8ce89756085544c5bacf0e81ea4cf730",
      "6ee9f483489942b2a57bc5a41f13cde2",
      "6760fbccdf74432f88d58398d5a1b94a",
      "2b27c8d7e3e64e019e6734fedb190059",
      "ceba540659ac4b4ba419b3d285471782",
      "9407ea8dea76497586cdb2b61045f00f",
      "310a54fab3284344a73a426e4031a2d6",
      "09fb952a62594dc7909ac11da7a32cb1",
      "0a717dc36b704032aee214e2bafea820",
      "4f64e71088824e3d98bc56725b598e54",
      "1ddac990870f4cc5b07007670549ab3d",
      "2d18c051ef564467bc2e84f7fda7b332",
      "005a0b75940b40d6b3094c6557512aea",
      "34c1ea3144e44201ac1bc7b47f3f19cc",
      "2b8bb8250efa4fa08f96fec35e508883",
      "ba14bd69b0704ebab5490773362354bd",
      "bd133c89becd4fd8a1320a91fd5509c9",
      "582d39192a27428e99f567e276134a1a",
      "1099fa46639941d798cb3381f82572fa",
      "712e005f021949799890b5ba74683716",
      "f13eaf47abb34828b46bbbe55e2bff24",
      "079fe524c0cb4673b69f41c51c36e1c1",
      "91be8efc0bc04c5a9073328a6f9dc89b",
      "94c8617dbfdc4d97b398ea3c543e9b50",
      "eb84887c37a94e8597877d6a9a2f2be3",
      "5474cadcc5ce46f5a613098f4347863e",
      "8f102ed451d3447da7daa5bab554bb76",
      "d9a02b85f3a14c60a6d3dd7a5f30dbfc",
      "db8738c2acf1409fb0b3ab8b0f712b9c",
      "c2d6943c0bf94c83a0a5b590581d9229",
      "b415b4978c7a4164b60b0ac1bda7b0bb",
      "697e6542ac784ee29836ba239d7e7e80"
     ]
    },
    "executionInfo": {
     "elapsed": 487314,
     "status": "ok",
     "timestamp": 1768148992223,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "YNRyAc4MAAcP",
    "outputId": "0bf72e49-8751-468e-caf5-ca4fbc0d4d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading: models/qwen3-4b-grpo-merged-f32-final | Mode: CoT | n=10\n",
      " Precision: torch.float16 | 4-Bit Quantization: False\n",
      "INFO 01-11 16:21:47 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 01-11 16:21:47 [vllm_utils.py:731] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading models/qwen3-4b-grpo-merged-f32-final with actual GPU utilization = 79.54%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 79.32 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 56.03 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 01-11 16:21:59 [utils.py:328] non-default args: {'dtype': torch.float16, 'seed': 0, 'max_model_len': 4096, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.7954304147054094, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'models/qwen3-4b-grpo-merged-f32-final'}\n",
      "INFO 01-11 16:22:16 [__init__.py:742] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:22:16 [__init__.py:2764] Downcasting torch.float32 to torch.float16.\n",
      "INFO 01-11 16:22:16 [__init__.py:1815] Using max model len 4096\n",
      "INFO 01-11 16:22:18 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-11 16:22:18 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 01-11 16:22:22 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='models/qwen3-4b-grpo-merged-f32-final', speculative_config=None, tokenizer='models/qwen3-4b-grpo-merged-f32-final', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=models/qwen3-4b-grpo-merged-f32-final, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":12,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":256,\"local_cache_dir\":null}\n",
      "INFO 01-11 16:22:22 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-11 16:22:22 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-11 16:22:22 [gpu_model_runner.py:2338] Starting to load model models/qwen3-4b-grpo-merged-f32-final...\n",
      "INFO 01-11 16:22:23 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 01-11 16:22:23 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967b9debbde445c5b545f91bd08f856d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:25:22 [default_loader.py:268] Loading weights took 178.62 seconds\n",
      "INFO 01-11 16:25:25 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-11 16:25:26 [gpu_model_runner.py:2392] Model loading took 7.8056 GiB and 181.620079 seconds\n",
      "INFO 01-11 16:25:40 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/902f1ba83e/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-11 16:25:40 [backends.py:550] Dynamo bytecode transform time: 13.47 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|██████████| 7/7 [00:00<00:00, 10.39it/s, triton_poi_fused_view_6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:25:46 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 19.55it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 479.35it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 490.69it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 491.93it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 467.94it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 516.63it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 511.56it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 513.44it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 511.31it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 500.30it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 547.42it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 524.49it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 489.03it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 487.52it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 471.14it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 497.54it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 478.56it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 462.46it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 510.95it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 508.86it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 487.78it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 499.41it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 485.31it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 501.55it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 467.13it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 443.24it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 499.14it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 544.17it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 517.25it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 516.86it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 479.59it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 456.89it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 488.72it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 471.11it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 11/11 [00:00<00:00, 490.37it/s, triton_poi_fused_view_10]\n",
      "Unsloth: Compiling kernels: 100%|██████████| 5/5 [00:00<00:00, 26.16it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:26:32 [backends.py:215] Compiling a graph for dynamic shape takes 50.84 s\n",
      "INFO 01-11 16:26:53 [monitor.py:34] torch.compile takes 64.31 s in total\n",
      "INFO 01-11 16:26:56 [gpu_worker.py:298] Available KV cache memory: 54.53 GiB\n",
      "INFO 01-11 16:26:57 [kv_cache_utils.py:864] GPU KV cache size: 397,040 tokens\n",
      "INFO 01-11 16:26:57 [kv_cache_utils.py:868] Maximum concurrency for 4,096 tokens per request: 96.93x\n",
      "INFO 01-11 16:26:57 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:14<00:00,  2.40it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:27:14 [gpu_model_runner.py:3118] Graph capturing finished in 18 secs, took 0.65 GiB\n",
      "INFO 01-11 16:27:14 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 18 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 16:27:16 [gpu_worker.py:391] Free memory on device (78.79/79.32 GiB) on startup. Desired GPU memory utilization is (0.7954304147054094, 63.09 GiB). Actual usage is 7.81 GiB for weight, 0.74 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.65 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=57689510092` to fit into requested memory, or `--kv-cache-memory=74545972224` to fully utilize gpu memory. Current kv cache memory in use is 58547245260 bytes.\n",
      "INFO 01-11 16:27:16 [core.py:218] init engine (profile, create kv cache, warmup model) took 110.57 seconds\n",
      "INFO 01-11 16:27:17 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 01-11 16:27:17 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_attention_layernorm', 'post_feedforward_layernorm', 'input_layernorm', 'norm1', 'norm', 'q_norm', 'norm2', 'layer_norm2', 'layer_norm1', 'k_norm', 'ffn_norm', 'post_layernorm', 'attention_norm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643d5ac7e97a415fb75950ae5f2cdc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForCausalLM were not initialized from the model checkpoint at models/qwen3-4b-grpo-merged-f32-final and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['cross_attn_input_layernorm', 'cross_attn_post_attention_layernorm', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'post_feedforward_layernorm', 'input_layernorm', 'norm1', 'norm', 'q_norm', 'norm2', 'layer_norm2', 'layer_norm1', 'k_norm', 'ffn_norm', 'post_layernorm', 'attention_norm']\n",
      " Preparing 164 prompts...\n",
      " Running vLLM Batch Generation (n=10)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddac990870f4cc5b07007670549ab3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079fe524c0cb4673b69f41c51c36e1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1640 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3552671664.py:21: FutureWarning: Possible nested set at position 16\n",
      "  code = re.sub(r'(\\s*(\"\"\"|\"\\\"\\\")[[\\s\\S]]*?\\2)', '', code, count=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 1640 samples to: data/evaluation/generations_run_2_pass@10/grpo_cot_pass@10.jsonl\n"
     ]
    }
   ],
   "source": [
    "# GRPO MODEL (CoT)\n",
    "evaluate_model(\n",
    "    model_path=GRPO_MODEL_PATH,\n",
    "    problems=problems,\n",
    "    task_ids=task_ids,\n",
    "    use_cot=True,\n",
    "    output_jsonl=str(GEN_DIR / \"grpo_cot_pass@10.jsonl\"),\n",
    "    max_new_tokens=MAX_NEW_TOKENS_COT, # Increased for safety, vLLM handles the speed\n",
    "    temperature=TEMP_COT,\n",
    "    load_in_4bit=False,    # Do not compress\n",
    "    dtype=torch.float16,   # Do not round off\n",
    "    n = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAkf4YxDndTV"
   },
   "source": [
    "# Step 9: Calculating Pass@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7CMTdIhnmzW"
   },
   "outputs": [],
   "source": [
    "def run_humaneval(\n",
    "    samples_file: str,\n",
    "    k: int = 1,\n",
    "    timeout: float = 3.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs HumanEval pass@k on a JSONL file of completions.\n",
    "    \"\"\"\n",
    "    # samples_file: path to JSONL produced by write_jsonl; each record must include\n",
    "    #               \"task_id\" and \"completion\". Multiple completions per task allowed.\n",
    "    # k: integer pass@k to compute (function wraps evaluate_functional_correctness which\n",
    "    #    expects a list of ks).\n",
    "    # timeout: per-test execution timeout (seconds).\n",
    "    results = evaluate_functional_correctness(\n",
    "        sample_file=samples_file,\n",
    "        k=[k],\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    # returns a dict with keys like \"pass@k\" and detailed per-task status\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtrrDF79nrgb"
   },
   "outputs": [],
   "source": [
    "RESULTS = []\n",
    "EVAL_RUNS = [\n",
    "    (\"Base\", \"Non-CoT\", GEN_DIR / \"base_non_cot_pass@10.jsonl\"),\n",
    "    (\"SFT\",  \"Non-CoT\", GEN_DIR / \"sft_non_cot_pass@10.jsonl\"),\n",
    "    (\"GRPO\", \"Non-CoT\", GEN_DIR / \"grpo_non_cot_pass@10.jsonl\"),\n",
    "    (\"SFT\",  \"CoT\",     GEN_DIR / \"sft_cot_pass@10.jsonl\"),\n",
    "    (\"GRPO\", \"CoT\",     GEN_DIR / \"grpo_cot_pass@10.jsonl\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "executionInfo": {
     "elapsed": 437050,
     "status": "ok",
     "timestamp": 1768151218706,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "6JZYqMdcn-Aw",
    "outputId": "a74c6997-7eb8-43c7-f60f-7f843809372d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: Base | Non-CoT | pass@10\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1640it [00:01, 1212.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [01:26<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2_pass@10/base_non_cot_pass@10.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:00<00:00, 36089.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: SFT | Non-CoT | pass@10\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1640it [00:00, 2563.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [01:26<00:00, 18.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2_pass@10/sft_non_cot_pass@10.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:00<00:00, 27567.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: GRPO | Non-CoT | pass@10\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1640it [00:00, 2433.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [01:24<00:00, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2_pass@10/grpo_non_cot_pass@10.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:00<00:00, 31743.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: SFT | CoT | pass@10\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1640it [00:00, 2352.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [01:30<00:00, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2_pass@10/sft_cot_pass@10.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:00<00:00, 30217.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: GRPO | CoT | pass@10\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1640it [00:00, 2290.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [01:24<00:00, 19.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to data/evaluation/generations_run_2_pass@10/grpo_cot_pass@10.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:00<00:00, 26530.97it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Base\",\n          \"SFT\",\n          \"GRPO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mode\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"CoT\",\n          \"Non-CoT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pass@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.036686851113393454,\n        \"min\": 0.8597560975609756,\n        \"max\": 0.9451219512195121,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.8902439024390244,\n          0.9451219512195121\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-70ad61b4-7bb4-4580-804a-700a25f5ba5c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mode</th>\n",
       "      <th>pass@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>Non-CoT</td>\n",
       "      <td>0.859756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SFT</td>\n",
       "      <td>Non-CoT</td>\n",
       "      <td>0.890244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRPO</td>\n",
       "      <td>Non-CoT</td>\n",
       "      <td>0.908537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SFT</td>\n",
       "      <td>CoT</td>\n",
       "      <td>0.945122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRPO</td>\n",
       "      <td>CoT</td>\n",
       "      <td>0.945122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70ad61b4-7bb4-4580-804a-700a25f5ba5c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-70ad61b4-7bb4-4580-804a-700a25f5ba5c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-70ad61b4-7bb4-4580-804a-700a25f5ba5c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-d85d78f3-56fa-4e9f-acab-433dc0ebcc2c\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d85d78f3-56fa-4e9f-acab-433dc0ebcc2c')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-d85d78f3-56fa-4e9f-acab-433dc0ebcc2c button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_a08868c2-1402-4e8c-ad15-ea652cc2feb2\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_a08868c2-1402-4e8c-ad15-ea652cc2feb2 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df_results');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "  Model     Mode   pass@10\n",
       "0  Base  Non-CoT  0.859756\n",
       "1   SFT  Non-CoT  0.890244\n",
       "2  GRPO  Non-CoT  0.908537\n",
       "3   SFT      CoT  0.945122\n",
       "4  GRPO      CoT  0.945122"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_name, mode, path in EVAL_RUNS:\n",
    "    print(f\"\\nEvaluating: {model_name} | {mode} | pass@10\")\n",
    "    res = run_humaneval(str(path), k=10)\n",
    "    RESULTS.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Mode\": mode,\n",
    "        \"pass@10\": res[\"pass@10\"],\n",
    "    })\n",
    "df_results = pd.DataFrame(RESULTS)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1768152806539,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "j3-7lFiRxIGj",
    "outputId": "90c3221c-27c3-4d14-b498-c51662339baf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results = df_results.copy()\n",
    "df_results[\"Label_ml\"] = df_results[\"Model\"] + \" (\" + df_results[\"Mode\"] + \")\"\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(df_results[\"Label_ml\"], df_results[\"pass@10\"])\n",
    "plt.ylabel(\"pass@10\")\n",
    "plt.title(\"HumanEval pass@10 by Model and Mode\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aaSM2OK4ayg"
   },
   "source": [
    "# Step 10: Fixing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ_G21MH4cnb"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDLgosHr4e3w"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLF_hG2-4gga"
   },
   "outputs": [],
   "source": [
    "# List the notebook directory to confirm the file exists\n",
    "os.listdir(\"/content/drive/MyDrive/grpo-verified-reasoner/notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpdji2u14jnS"
   },
   "outputs": [],
   "source": [
    "notebook_path = \"/content/drive/MyDrive/grpo-verified-reasoner/notebooks/04_evaluation.ipynb\"\n",
    "\n",
    "with open(notebook_path, \"r\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "with open(notebook_path, \"w\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Notebook fixed and saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO3Wa2QCu287bpddBOMDstZ",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
