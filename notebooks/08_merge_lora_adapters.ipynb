{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNVGT/nqhYHFPN/t4HZz9O4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Step 1: Mounting Google Drive and Importing Libraries\n"],"metadata":{"id":"JYGkPqOQyjCb"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XezTJj3VyV3I","executionInfo":{"status":"ok","timestamp":1767935083598,"user_tz":360,"elapsed":18961,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"3772bd3e-abcf-4bfa-9419-ca6da004a1a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/grpo-verified-reasoner\n","data\t\t\t      LICENSE\t outputs    unsloth_compiled_cache\n","grpo_trainer_lora_model       models\t README.md  _unsloth_sentencepiece_temp\n","huggingface_tokenizers_cache  notebooks  src\t    wandb\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd /content/drive/MyDrive/grpo-verified-reasoner\n","!ls"]},{"cell_type":"code","source":["# Install UV (Faster pip)\n","!pip install --upgrade -qqq uv"],"metadata":{"id":"1nzcSLWdyoL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q unsloth"],"metadata":{"id":"7zEhja-B9duL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import gc\n","import torch\n","import safetensors.torch\n","from safetensors import safe_open\n","#from unsloth import FastLanguageModel"],"metadata":{"id":"axrWYx3W9hc-","executionInfo":{"status":"ok","timestamp":1767935120032,"user_tz":360,"elapsed":3451,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Step 2: Loading the Base Model and the GRPO LoRA Adapter"],"metadata":{"id":"w3UklbRj9rLb"}},{"cell_type":"code","source":["BASE_MODEL_PATH = \"unsloth/Qwen3-4B-Base\"\n","SFT_MODEL_PATH  = \"models/qwen3-4b-sft\"\n","CHECKPOINT_PATH = \"outputs/checkpoint-188\"\n","MERGED_PATH = \"models/qwen3-4b-grpo-final-2-merged\"\n","GRPO_MODEL_PATH = \"models/qwen3-4b-grpo-final-2\""],"metadata":{"id":"ZLj3K84i9wev","executionInfo":{"status":"ok","timestamp":1767934742360,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = CHECKPOINT_PATH,\n","    max_seq_length = 3072,\n","    load_in_4bit = False,    # Must be False for merging\n","    dtype = torch.float16,   # Standard 16-bit precision\n",")"],"metadata":{"id":"j9RotUbF95JX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This physically modifies the weights: W_new = W_base + (A * B)\n","model.merge_and_unload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wsr9yIBJ-LVa","executionInfo":{"status":"ok","timestamp":1767785385537,"user_tz":360,"elapsed":149,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"61c00103-e019-4a2d-a29d-2b9988a6ba4e"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Qwen3ForCausalLM(\n","  (model): Qwen3Model(\n","    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n","    (layers): ModuleList(\n","      (0-35): 36 x Qwen3DecoderLayer(\n","        (self_attn): Qwen3Attention(\n","          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n","          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen3MLP(\n","          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",")"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Saves as a standard model (no adapters folder, just model.safetensors)\n","model.save_pretrained_merged(\n","    MERGED_PATH,\n","    tokenizer,\n","    save_method = \"merged_16bit\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDIF8Sic-OH2","executionInfo":{"status":"ok","timestamp":1767785490187,"user_tz":360,"elapsed":72512,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"82523488-b9c3-4cdd-a212-c1da8f5626b6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n","Checking cache directory for required files...\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Copying 2 files from cache to `models/qwen3-4b-grpo-final-2-merged`: 100%|██████████| 2/2 [00:22<00:00, 11.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully copied all 2 files from cache to `models/qwen3-4b-grpo-final-2-merged`\n","Checking cache directory for required files...\n","Cache check failed: tokenizer.model not found in local cache.\n","Not all required files found in cache. Will proceed with downloading.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 20410.24it/s]\n","Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:46<00:00, 23.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-final-2-merged`\n"]}]},{"cell_type":"code","source":["sft = safetensors.torch.load_file(\"outputs/checkpoint-90/adapter_model.safetensors\")\n","grpo = safetensors.torch.load_file(\"outputs/checkpoint-188/adapter_model.safetensors\")\n","\n","# Pick any key\n","k = list(sft.keys())[0]\n","torch.norm(sft[k] - grpo[k])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CXSnbFawOMq2","executionInfo":{"status":"ok","timestamp":1767846758896,"user_tz":360,"elapsed":29710,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"26dadc04-7be5-40b7-d978-c1c5a2b32b3d"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.0062)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["sum(torch.norm(sft[k] - grpo[k]) for k in sft.keys()) / sum(torch.norm(sft[k]) for k in sft.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09bjZwBjPm3t","executionInfo":{"status":"ok","timestamp":1767846767624,"user_tz":360,"elapsed":280,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"69286aa0-356e-4df7-8d85-2b4f9135af7d"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.0023)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# Step 3: Merging at 32-Bit Precision (GRPO Model)"],"metadata":{"id":"yiFXlUhAWcJE"}},{"cell_type":"code","source":["SAVE_PATH = \"models/qwen3-4b-grpo-merged-f32-final\"\n","SAVE_PATH_2 = \"models/qwen3-4b-grpo-merged-f16-final\""],"metadata":{"id":"PUe_Dr819SWZ","executionInfo":{"status":"ok","timestamp":1767932940866,"user_tz":360,"elapsed":4,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = CHECKPOINT_PATH,\n","    max_seq_length = 3072,\n","    load_in_4bit = False,    # Must be False for merging\n","    dtype = torch.float32,   # Standard 16-bit precision\n",")"],"metadata":{"id":"WAk3Xuh0WfoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This physically modifies the weights: W_new = W_base + (A * B)\n","# The addition now happens in 32-bit. The 0.002 signal is preserved.\n","model.merge_and_unload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfE3B9kKWfji","executionInfo":{"status":"ok","timestamp":1767933087918,"user_tz":360,"elapsed":217,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"f3bf314f-d557-4d73-9dfb-3d95ede6ed30"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Qwen3ForCausalLM(\n","  (model): Qwen3Model(\n","    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n","    (layers): ModuleList(\n","      (0-35): 36 x Qwen3DecoderLayer(\n","        (self_attn): Qwen3Attention(\n","          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n","          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen3MLP(\n","          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# We DO NOT use Unsloth's save method. We use the raw HuggingFace save.\n","# This prevents Unsloth from secretly downcasting to 16-bit behind your back.\n","model.model.save_pretrained(SAVE_PATH, safe_serialization=True)\n","tokenizer.save_pretrained(SAVE_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agvN8ggd78HI","executionInfo":{"status":"ok","timestamp":1767933165554,"user_tz":360,"elapsed":64858,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"d422a0e1-bfaf-4b90-f13a-410a8e68cb4e"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('models/qwen3-4b-grpo-merged-f32-final/tokenizer_config.json',\n"," 'models/qwen3-4b-grpo-merged-f32-final/special_tokens_map.json',\n"," 'models/qwen3-4b-grpo-merged-f32-final/chat_template.jinja',\n"," 'models/qwen3-4b-grpo-merged-f32-final/vocab.json',\n"," 'models/qwen3-4b-grpo-merged-f32-final/merges.txt',\n"," 'models/qwen3-4b-grpo-merged-f32-final/added_tokens.json',\n"," 'models/qwen3-4b-grpo-merged-f32-final/tokenizer.json')"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Saves as a standard model (no adapters folder, just model.safetensors)\n","model.save_pretrained_merged(\n","    SAVE_PATH_2,\n","    tokenizer,\n","    save_method = \"merged_16bit\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQEkoQ8q9dOq","executionInfo":{"status":"ok","timestamp":1767933420330,"user_tz":360,"elapsed":91335,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"be9fcc6e-b281-4fe0-efec-94e0c006690c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n","Checking cache directory for required files...\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Copying 2 files from cache to `models/qwen3-4b-grpo-merged-f16-final`: 100%|██████████| 2/2 [00:45<00:00, 22.60s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully copied all 2 files from cache to `models/qwen3-4b-grpo-merged-f16-final`\n","Checking cache directory for required files...\n","Cache check failed: tokenizer.model not found in local cache.\n","Not all required files found in cache. Will proceed with downloading.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 17403.75it/s]\n","Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:43<00:00, 21.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-merged-f16-final`\n"]}]},{"cell_type":"code","source":["del model\n","del tokenizer\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84l0EqWIDS06","executionInfo":{"status":"ok","timestamp":1767933574215,"user_tz":360,"elapsed":43,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"94764f5b-9dae-4948-c8ff-21c619855edb"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["78"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["# Step 4: Merging at BF16 Precision (GRPO Model)"],"metadata":{"id":"Nw5pbwUiF5Hm"}},{"cell_type":"code","source":["SAVE_PATH = \"models/qwen3-4b-grpo-merged-bf16-final\""],"metadata":{"id":"tDu6fGSPF52Y","executionInfo":{"status":"ok","timestamp":1767934748032,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = GRPO_MODEL_PATH,\n","    max_seq_length = 3072,\n","    load_in_4bit = False,    # Must be False for merging\n","    dtype = torch.bfloat16,\n",")"],"metadata":{"id":"vV_FnhJFF5me"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This physically modifies the weights: W_new = W_base + (A * B)\n","# The addition now happens in 32-bit. The 0.002 signal is preserved.\n","model.merge_and_unload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yam1yK2GGJsI","executionInfo":{"status":"ok","timestamp":1767934807206,"user_tz":360,"elapsed":418,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"8335cd7d-276d-424a-8b27-7017c993791e"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Qwen3ForCausalLM(\n","  (model): Qwen3Model(\n","    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n","    (layers): ModuleList(\n","      (0-35): 36 x Qwen3DecoderLayer(\n","        (self_attn): Qwen3Attention(\n","          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n","          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen3MLP(\n","          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# We DO NOT use Unsloth's save method. We use the raw HuggingFace save.\n","# This prevents Unsloth from secretly downcasting to 16-bit behind your back.\n","model.model.save_pretrained(SAVE_PATH, safe_serialization=True)\n","tokenizer.save_pretrained(SAVE_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnSzx9JJGRMX","executionInfo":{"status":"ok","timestamp":1767934842616,"user_tz":360,"elapsed":33253,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"471d39c6-fc66-4e58-dba5-6f1129d52ab5"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('models/qwen3-4b-grpo-merged-bf16-final/tokenizer_config.json',\n"," 'models/qwen3-4b-grpo-merged-bf16-final/special_tokens_map.json',\n"," 'models/qwen3-4b-grpo-merged-bf16-final/chat_template.jinja',\n"," 'models/qwen3-4b-grpo-merged-bf16-final/vocab.json',\n"," 'models/qwen3-4b-grpo-merged-bf16-final/merges.txt',\n"," 'models/qwen3-4b-grpo-merged-bf16-final/added_tokens.json',\n"," 'models/qwen3-4b-grpo-merged-bf16-final/tokenizer.json')"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Step 5: Merging SFT Model"],"metadata":{"id":"UepT38P60vxg"}},{"cell_type":"code","source":["MERGED_PATH = \"models/qwen3-4b-sft-merged\""],"metadata":{"id":"ARF0DVCK1Fz4","executionInfo":{"status":"ok","timestamp":1767829208464,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = SFT_MODEL_PATH,\n","    max_seq_length = 3072,\n","    load_in_4bit = False,    # Must be False for merging\n","    dtype = torch.float16,   # Standard 16-bit precision\n",")"],"metadata":{"id":"R6UJTSch0vW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This physically modifies the weights: W_new = W_base + (A * B)\n","model.merge_and_unload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fdGpZcjl05ur","executionInfo":{"status":"ok","timestamp":1767829343612,"user_tz":360,"elapsed":147,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"3a78a87d-f125-4a72-f6ef-74f93a1d0c78"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Qwen3ForCausalLM(\n","  (model): Qwen3Model(\n","    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n","    (layers): ModuleList(\n","      (0-35): 36 x Qwen3DecoderLayer(\n","        (self_attn): Qwen3Attention(\n","          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n","          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen3MLP(\n","          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Saves as a standard model (no adapters folder, just model.safetensors)\n","model.save_pretrained_merged(\n","    MERGED_PATH,\n","    tokenizer,\n","    save_method = \"merged_16bit\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_ncifke0_v5","executionInfo":{"status":"ok","timestamp":1767829426457,"user_tz":360,"elapsed":75989,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"e2391590-d2d2-4b75-ef8d-52da039796c8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n","Checking cache directory for required files...\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Copying 2 files from cache to `models/qwen3-4b-sft-merged`: 100%|██████████| 2/2 [00:22<00:00, 11.06s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully copied all 2 files from cache to `models/qwen3-4b-sft-merged`\n","Checking cache directory for required files...\n","Cache check failed: tokenizer.model not found in local cache.\n","Not all required files found in cache. Will proceed with downloading.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 16980.99it/s]\n","Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:50<00:00, 25.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-sft-merged`\n"]}]},{"cell_type":"markdown","source":["# Step 6: Merging at 32-Bit Precision (SFT Model)"],"metadata":{"id":"eEF60vh8DbD7"}},{"cell_type":"code","source":["SAVE_PATH = \"models/qwen3-4b-sft-merged-f32\"\n","SAVE_PATH_2 = \"models/qwen3-4b-sft-merged-f16\""],"metadata":{"id":"slm57gZiDayh","executionInfo":{"status":"ok","timestamp":1767933803088,"user_tz":360,"elapsed":40,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = SFT_MODEL_PATH,\n","    max_seq_length = 3072,\n","    load_in_4bit = False,    # Must be False for merging\n","    dtype = torch.float32,   # Standard 16-bit precision\n",")"],"metadata":{"id":"wTPJGSFbDrpr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This physically modifies the weights: W_new = W_base + (A * B)\n","model.merge_and_unload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNq_NybQDyuj","executionInfo":{"status":"ok","timestamp":1767934012797,"user_tz":360,"elapsed":4522,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"e281670a-4662-40f5-daf5-e630437b4a04"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Qwen3ForCausalLM(\n","  (model): Qwen3Model(\n","    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n","    (layers): ModuleList(\n","      (0-35): 36 x Qwen3DecoderLayer(\n","        (self_attn): Qwen3Attention(\n","          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n","          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen3MLP(\n","          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# We DO NOT use Unsloth's save method. We use the raw HuggingFace save.\n","# This prevents Unsloth from secretly downcasting to 16-bit behind your back.\n","model.model.save_pretrained(SAVE_PATH, safe_serialization=True)\n","tokenizer.save_pretrained(SAVE_PATH)"],"metadata":{"id":"5ghTVkdeEA_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saves as a standard model (no adapters folder, just model.safetensors)\n","model.save_pretrained_merged(\n","    SAVE_PATH_2,\n","    tokenizer,\n","    save_method = \"merged_16bit\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rS0op5KKECQY","executionInfo":{"status":"ok","timestamp":1767934175528,"user_tz":360,"elapsed":91120,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"8b5d6e4f-8f4d-4459-c10c-6030270b418c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n","Checking cache directory for required files...\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Copying 2 files from cache to `models/qwen3-4b-sft-merged-f16`: 100%|██████████| 2/2 [00:31<00:00, 15.60s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully copied all 2 files from cache to `models/qwen3-4b-sft-merged-f16`\n","Checking cache directory for required files...\n","Cache check failed: tokenizer.model not found in local cache.\n","Not all required files found in cache. Will proceed with downloading.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 13530.01it/s]\n","Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:56<00:00, 28.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-sft-merged-f16`\n"]}]},{"cell_type":"code","source":["del model\n","del tokenizer\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Po7GL-hNFzeC","executionInfo":{"status":"ok","timestamp":1767934214442,"user_tz":360,"elapsed":7,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"6c731cad-3ca6-4ca0-d6b3-3fbdf909b952"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["104"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["# Step 3: Merging Model at Checkpoint 90"],"metadata":{"id":"BRfYgDu72iCM"}},{"cell_type":"code","source":["CHECKPOINT_PATH = \"outputs/checkpoint-90\"\n","MERGED_PATH = \"models/qwen3-4b-grpo-checkpoint90-merged\""],"metadata":{"id":"zbZP2sPz1yc4","executionInfo":{"status":"ok","timestamp":1767829728164,"user_tz":360,"elapsed":2,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = CHECKPOINT_PATH,\n","    max_seq_length = 3072,\n","    load_in_4bit = False,    # Must be False for merging\n","    dtype = torch.float16,   # Standard 16-bit precision\n",")"],"metadata":{"id":"Y5boh7xu2zFl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This physically modifies the weights: W_new = W_base + (A * B)\n","model.merge_and_unload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YProXQh224yx","executionInfo":{"status":"ok","timestamp":1767829751279,"user_tz":360,"elapsed":142,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"e37fe693-e4e4-4425-e540-5bf038af400d"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Qwen3ForCausalLM(\n","  (model): Qwen3Model(\n","    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n","    (layers): ModuleList(\n","      (0-35): 36 x Qwen3DecoderLayer(\n","        (self_attn): Qwen3Attention(\n","          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n","          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen3MLP(\n","          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n","          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Saves as a standard model (no adapters folder, just model.safetensors)\n","model.save_pretrained_merged(\n","    MERGED_PATH,\n","    tokenizer,\n","    save_method = \"merged_16bit\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ym085f2n2-Bh","executionInfo":{"status":"ok","timestamp":1767829920858,"user_tz":360,"elapsed":131346,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"503865d1-99a0-4d63-c5f4-91742a1d1cc9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n","Checking cache directory for required files...\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Copying 2 files from cache to `models/qwen3-4b-grpo-checkpoint90-merged`: 100%|██████████| 2/2 [00:23<00:00, 11.81s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully copied all 2 files from cache to `models/qwen3-4b-grpo-checkpoint90-merged`\n","Checking cache directory for required files...\n","Cache check failed: tokenizer.model not found in local cache.\n","Not all required files found in cache. Will proceed with downloading.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 3818.21it/s]\n","Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [01:44<00:00, 52.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-checkpoint90-merged`\n"]}]},{"cell_type":"markdown","source":["# Step 4: Checking Precision"],"metadata":{"id":"VGKFxOEuSoRk"}},{"cell_type":"code","source":["# DEFINE YOUR PATHS HERE\n","paths_to_check = {\n","    \"SFT Adapter (Manual Save)\": \"models/qwen3-4b-sft/adapter_model.safetensors\",\n","    \"SFT Adapter (Merged, fp16)\": \"models/qwen3-4b-sft-merged-f16/model-00001-of-00002.safetensors\",\n","    \"SFT Adapter (Merged, fp32)\": \"models/qwen3-4b-sft-merged-f32/model-00001-of-00004.safetensors\",\n","    \"GRPO Checkpoint (Adapter, Auto Save)\": \"outputs/checkpoint-188/adapter_model.safetensors\",\n","    \"GRPO Checkpoint (Adapter, save_lora method)\": \"models/qwen3-4b-grpo-final-2/adapter_model.safetensors\",\n","    \"GRPO Checkpoint (Merged, bf16)\": \"models/qwen3-4b-grpo-merged-bf16-final/model-00001-of-00002.safetensors\",\n","    \"GRPO Checkpoint (Merged, fp16)\": \"models/qwen3-4b-grpo-merged-f16-final/model-00001-of-00002.safetensors\",\n","    \"GRPO Checkpoint (Merfed, fp32)\": \"models/qwen3-4b-grpo-merged-f32-final/model-00001-of-00004.safetensors\",\n","    # Add any other paths you want\n","}"],"metadata":{"id":"5RC3-PGFTMqk","executionInfo":{"status":"ok","timestamp":1767935673359,"user_tz":360,"elapsed":5,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(f\"{'FILE PATH':<50} | {'SIZE (MB)':<10} | {'PRECISION':<10}\")\n","print(\"-\" * 75)\n","\n","for label, path in paths_to_check.items():\n","    if not os.path.exists(path):\n","        print(f\"{label:<50} | FILE NOT FOUND\")\n","        continue\n","\n","    # 1. Check File Size\n","    size_mb = os.path.getsize(path) / (1024 * 1024)\n","\n","    # 2. Check Precision (Dtype)\n","    # We open the file metadata WITHOUT loading the tensors into RAM\n","    dtype_str = \"Unknown\"\n","    try:\n","        with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n","            # We just check the first tensor we find\n","            first_key = list(f.keys())[0]\n","            tensor_slice = f.get_slice(first_key)\n","            dtype_str = str(tensor_slice.get_dtype())\n","    except Exception as e:\n","        dtype_str = \"Error reading\"\n","\n","    print(f\"{label:<50} | {size_mb:<10.1f} | {dtype_str:<10}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4_IQKAJTUEs","executionInfo":{"status":"ok","timestamp":1767935680799,"user_tz":360,"elapsed":6620,"user":{"displayName":"Samyak Shrestha","userId":"13083503381857072620"}},"outputId":"3c3211e5-9132-449c-8d22-a6000b9e5712"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["FILE PATH                                          | SIZE (MB)  | PRECISION \n","---------------------------------------------------------------------------\n","SFT Adapter (Manual Save)                          | 252.1      | F32       \n","SFT Adapter (Merged, fp16)                         | 4737.1     | BF16      \n","SFT Adapter (Merged, fp32)                         | 4758.9     | F32       \n","GRPO Checkpoint (Adapter, Auto Save)               | 252.1      | F32       \n","GRPO Checkpoint (Adapter, save_lora method)        | 126.1      | BF16      \n","GRPO Checkpoint (Merged, bf16)                     | 4737.1     | BF16      \n","GRPO Checkpoint (Merged, fp16)                     | 4737.1     | BF16      \n","GRPO Checkpoint (Merfed, fp32)                     | 4758.9     | F32       \n"]}]}]}