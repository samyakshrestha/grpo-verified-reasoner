{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYGkPqOQyjCb"
   },
   "source": [
    "# Step 1: Mounting Google Drive and Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18961,
     "status": "ok",
     "timestamp": 1767935083598,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "XezTJj3VyV3I",
    "outputId": "3772bd3e-abcf-4bfa-9419-ca6da004a1a1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd /content/drive/MyDrive/grpo-verified-reasoner\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nzcSLWdyoL6"
   },
   "outputs": [],
   "source": [
    "# Install UV (Faster pip)\n",
    "!pip install --upgrade -qqq uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zEhja-B9duL"
   },
   "outputs": [],
   "source": [
    "!pip install -q unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3451,
     "status": "ok",
     "timestamp": 1767935120032,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "axrWYx3W9hc-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import safetensors.torch\n",
    "from safetensors import safe_open\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3UklbRj9rLb"
   },
   "source": [
    "# Step 2: Loading the Base Model and the GRPO LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767934742360,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "ZLj3K84i9wev"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = \"unsloth/Qwen3-4B-Base\"\n",
    "SFT_MODEL_PATH  = \"models/qwen3-4b-sft\"\n",
    "CHECKPOINT_PATH = \"outputs/checkpoint-188\"\n",
    "MERGED_PATH = \"models/qwen3-4b-grpo-final-2-merged\"\n",
    "GRPO_MODEL_PATH = \"models/qwen3-4b-grpo-final-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9RotUbF95JX"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = CHECKPOINT_PATH,\n",
    "    max_seq_length = 3072,\n",
    "    load_in_4bit = False,    # Must be False for merging\n",
    "    dtype = torch.float16,   # Standard 16-bit precision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1767785385537,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "wsr9yIBJ-LVa",
    "outputId": "61c00103-e019-4a2d-a29d-2b9988a6ba4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This physically modifies the weights: W_new = W_base + (A * B)\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72512,
     "status": "ok",
     "timestamp": 1767785490187,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "qDIF8Sic-OH2",
    "outputId": "82523488-b9c3-4cdd-a212-c1da8f5626b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 2 files from cache to `models/qwen3-4b-grpo-final-2-merged`: 100%|██████████| 2/2 [00:22<00:00, 11.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 2 files from cache to `models/qwen3-4b-grpo-final-2-merged`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 20410.24it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:46<00:00, 23.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-final-2-merged`\n"
     ]
    }
   ],
   "source": [
    "# Saves as a standard model (no adapters folder, just model.safetensors)\n",
    "model.save_pretrained_merged(\n",
    "    MERGED_PATH,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29710,
     "status": "ok",
     "timestamp": 1767846758896,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "CXSnbFawOMq2",
    "outputId": "26dadc04-7be5-40b7-d978-c1c5a2b32b3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0062)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft = safetensors.torch.load_file(\"outputs/checkpoint-90/adapter_model.safetensors\")\n",
    "grpo = safetensors.torch.load_file(\"outputs/checkpoint-188/adapter_model.safetensors\")\n",
    "\n",
    "# Pick any key\n",
    "k = list(sft.keys())[0]\n",
    "torch.norm(sft[k] - grpo[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1767846767624,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "09bjZwBjPm3t",
    "outputId": "69286aa0-356e-4df7-8d85-2b4f9135af7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0023)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(torch.norm(sft[k] - grpo[k]) for k in sft.keys()) / sum(torch.norm(sft[k]) for k in sft.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiFXlUhAWcJE"
   },
   "source": [
    "# Step 3: Merging at 32-Bit Precision (GRPO Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767932940866,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "PUe_Dr819SWZ"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = \"models/qwen3-4b-grpo-merged-f32-final\"\n",
    "SAVE_PATH_2 = \"models/qwen3-4b-grpo-merged-f16-final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAk3Xuh0WfoD"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = CHECKPOINT_PATH,\n",
    "    max_seq_length = 3072,\n",
    "    load_in_4bit = False,    # Must be False for merging\n",
    "    dtype = torch.float32,   # Standard 16-bit precision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1767933087918,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "nfE3B9kKWfji",
    "outputId": "f3bf314f-d557-4d73-9dfb-3d95ede6ed30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This physically modifies the weights: W_new = W_base + (A * B)\n",
    "# The addition now happens in 32-bit. The 0.002 signal is preserved.\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64858,
     "status": "ok",
     "timestamp": 1767933165554,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "agvN8ggd78HI",
    "outputId": "d422a0e1-bfaf-4b90-f13a-410a8e68cb4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/qwen3-4b-grpo-merged-f32-final/tokenizer_config.json',\n",
       " 'models/qwen3-4b-grpo-merged-f32-final/special_tokens_map.json',\n",
       " 'models/qwen3-4b-grpo-merged-f32-final/chat_template.jinja',\n",
       " 'models/qwen3-4b-grpo-merged-f32-final/vocab.json',\n",
       " 'models/qwen3-4b-grpo-merged-f32-final/merges.txt',\n",
       " 'models/qwen3-4b-grpo-merged-f32-final/added_tokens.json',\n",
       " 'models/qwen3-4b-grpo-merged-f32-final/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use HuggingFace's native save_pretrained on the underlying HF model to preserve exact weights\n",
    "# and file layout (avoid any adapter-wrapping behavior).\n",
    "# Avoid Unsloth's convenience save which can implicitly downcast tensors (e.g., to float16)\n",
    "# or alter serialization settings.\n",
    "# safe_serialization=True forces the zip-based, non-pickle format for portability and safety.\n",
    "# Save the tokenizer as well so the repository contains both model weights and tokenization files.\n",
    "model.model.save_pretrained(SAVE_PATH, safe_serialization=True)\n",
    "tokenizer.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91335,
     "status": "ok",
     "timestamp": 1767933420330,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "eQEkoQ8q9dOq",
    "outputId": "be9fcc6e-b281-4fe0-efec-94e0c006690c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 2 files from cache to `models/qwen3-4b-grpo-merged-f16-final`: 100%|██████████| 2/2 [00:45<00:00, 22.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 2 files from cache to `models/qwen3-4b-grpo-merged-f16-final`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 17403.75it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:43<00:00, 21.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-merged-f16-final`\n"
     ]
    }
   ],
   "source": [
    "# Saves as a standard model (no adapters folder, just model.safetensors)\n",
    "model.save_pretrained_merged(\n",
    "    SAVE_PATH_2,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1767933574215,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "84l0EqWIDS06",
    "outputId": "94764f5b-9dae-4948-c8ff-21c619855edb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nw5pbwUiF5Hm"
   },
   "source": [
    "# Step 4: Merging at BF16 Precision (GRPO Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767934748032,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "tDu6fGSPF52Y"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = \"models/qwen3-4b-grpo-merged-bf16-final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV_FnhJFF5me"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = GRPO_MODEL_PATH,\n",
    "    max_seq_length = 3072,\n",
    "    load_in_4bit = False,    # Must be False for merging\n",
    "    dtype = torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1767934807206,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Yam1yK2GGJsI",
    "outputId": "8335cd7d-276d-424a-8b27-7017c993791e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge LoRA adapter deltas into the base weights in-place:\n",
    "# W_new = W_base + (A @ B)  (LoRA rank-factor product)\n",
    "# Addition performed in 32-bit to avoid downcast/precision loss and preserve small update signals (~0.002)\n",
    "# This permanently applies the adapter and unloads adapter structures to free memory.\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33253,
     "status": "ok",
     "timestamp": 1767934842616,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "LnSzx9JJGRMX",
    "outputId": "471d39c6-fc66-4e58-dba5-6f1129d52ab5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/qwen3-4b-grpo-merged-bf16-final/tokenizer_config.json',\n",
       " 'models/qwen3-4b-grpo-merged-bf16-final/special_tokens_map.json',\n",
       " 'models/qwen3-4b-grpo-merged-bf16-final/chat_template.jinja',\n",
       " 'models/qwen3-4b-grpo-merged-bf16-final/vocab.json',\n",
       " 'models/qwen3-4b-grpo-merged-bf16-final/merges.txt',\n",
       " 'models/qwen3-4b-grpo-merged-bf16-final/added_tokens.json',\n",
       " 'models/qwen3-4b-grpo-merged-bf16-final/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.save_pretrained(SAVE_PATH, safe_serialization=True)\n",
    "tokenizer.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UepT38P60vxg"
   },
   "source": [
    "# Step 5: Merging SFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767829208464,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "ARF0DVCK1Fz4"
   },
   "outputs": [],
   "source": [
    "MERGED_PATH = \"models/qwen3-4b-sft-merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6UJTSch0vW2"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = SFT_MODEL_PATH,\n",
    "    max_seq_length = 3072,\n",
    "    load_in_4bit = False,    # Must be False for merging\n",
    "    dtype = torch.float16,   # Standard 16-bit precision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1767829343612,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "fdGpZcjl05ur",
    "outputId": "3a78a87d-f125-4a72-f6ef-74f93a1d0c78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This physically modifies the weights: W_new = W_base + (A * B)\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75989,
     "status": "ok",
     "timestamp": 1767829426457,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "I_ncifke0_v5",
    "outputId": "e2391590-d2d2-4b75-ef8d-52da039796c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 2 files from cache to `models/qwen3-4b-sft-merged`: 100%|██████████| 2/2 [00:22<00:00, 11.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 2 files from cache to `models/qwen3-4b-sft-merged`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 16980.99it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:50<00:00, 25.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-sft-merged`\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\n",
    "    MERGED_PATH,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEF60vh8DbD7"
   },
   "source": [
    "# Step 6: Merging at 32-Bit Precision (SFT Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1767933803088,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "slm57gZiDayh"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = \"models/qwen3-4b-sft-merged-f32\"\n",
    "SAVE_PATH_2 = \"models/qwen3-4b-sft-merged-f16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTPJGSFbDrpr"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = SFT_MODEL_PATH,\n",
    "    max_seq_length = 3072,\n",
    "    load_in_4bit = False,    # Must be False for merging\n",
    "    dtype = torch.float32,   # Standard 16-bit precision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4522,
     "status": "ok",
     "timestamp": 1767934012797,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "kNq_NybQDyuj",
    "outputId": "e281670a-4662-40f5-daf5-e630437b4a04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ghTVkdeEA_p"
   },
   "outputs": [],
   "source": [
    "model.model.save_pretrained(SAVE_PATH, safe_serialization=True)\n",
    "tokenizer.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91120,
     "status": "ok",
     "timestamp": 1767934175528,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "rS0op5KKECQY",
    "outputId": "8b5d6e4f-8f4d-4459-c10c-6030270b418c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 2 files from cache to `models/qwen3-4b-sft-merged-f16`: 100%|██████████| 2/2 [00:31<00:00, 15.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 2 files from cache to `models/qwen3-4b-sft-merged-f16`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 13530.01it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:56<00:00, 28.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-sft-merged-f16`\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\n",
    "    SAVE_PATH_2,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1767934214442,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Po7GL-hNFzeC",
    "outputId": "6c731cad-3ca6-4ca0-d6b3-3fbdf909b952"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRfYgDu72iCM"
   },
   "source": [
    "# Step 3: Merging Model at Checkpoint 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1767829728164,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zbZP2sPz1yc4"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"outputs/checkpoint-90\"\n",
    "MERGED_PATH = \"models/qwen3-4b-grpo-checkpoint90-merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5boh7xu2zFl"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = CHECKPOINT_PATH,\n",
    "    max_seq_length = 3072,\n",
    "    load_in_4bit = False,    # Must be False for merging\n",
    "    dtype = torch.float16,   # Standard 16-bit precision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1767829751279,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "YProXQh224yx",
    "outputId": "e37fe693-e4e4-4425-e540-5bf038af400d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131346,
     "status": "ok",
     "timestamp": 1767829920858,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Ym085f2n2-Bh",
    "outputId": "503865d1-99a0-4d63-c5f4-91742a1d1cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 2 files from cache to `models/qwen3-4b-grpo-checkpoint90-merged`: 100%|██████████| 2/2 [00:23<00:00, 11.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 2 files from cache to `models/qwen3-4b-grpo-checkpoint90-merged`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|██████████| 2/2 [00:00<00:00, 3818.21it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [01:44<00:00, 52.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-checkpoint90-merged`\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\n",
    "    MERGED_PATH,\n",
    "    tokenizer,\n",
    "    save_method = \"merged_16bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGKFxOEuSoRk"
   },
   "source": [
    "# Step 4: Checking Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1767935673359,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "5RC3-PGFTMqk"
   },
   "outputs": [],
   "source": [
    "paths_to_check = {\n",
    "    \"SFT Adapter (Manual Save)\": \"models/qwen3-4b-sft/adapter_model.safetensors\",\n",
    "    \"SFT Adapter (Merged, fp16)\": \"models/qwen3-4b-sft-merged-f16/model-00001-of-00002.safetensors\",\n",
    "    \"SFT Adapter (Merged, fp32)\": \"models/qwen3-4b-sft-merged-f32/model-00001-of-00004.safetensors\",\n",
    "    \"GRPO Checkpoint (Adapter, Auto Save)\": \"outputs/checkpoint-188/adapter_model.safetensors\",\n",
    "    \"GRPO Checkpoint (Adapter, save_lora method)\": \"models/qwen3-4b-grpo-final-2/adapter_model.safetensors\",\n",
    "    \"GRPO Checkpoint (Merged, bf16)\": \"models/qwen3-4b-grpo-merged-bf16-final/model-00001-of-00002.safetensors\",\n",
    "    \"GRPO Checkpoint (Merged, fp16)\": \"models/qwen3-4b-grpo-merged-f16-final/model-00001-of-00002.safetensors\",\n",
    "    \"GRPO Checkpoint (Merfed, fp32)\": \"models/qwen3-4b-grpo-merged-f32-final/model-00001-of-00004.safetensors\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6620,
     "status": "ok",
     "timestamp": 1767935680799,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "e4_IQKAJTUEs",
    "outputId": "3c3211e5-9132-449c-8d22-a6000b9e5712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE PATH                                          | SIZE (MB)  | PRECISION \n",
      "---------------------------------------------------------------------------\n",
      "SFT Adapter (Manual Save)                          | 252.1      | F32       \n",
      "SFT Adapter (Merged, fp16)                         | 4737.1     | BF16      \n",
      "SFT Adapter (Merged, fp32)                         | 4758.9     | F32       \n",
      "GRPO Checkpoint (Adapter, Auto Save)               | 252.1      | F32       \n",
      "GRPO Checkpoint (Adapter, save_lora method)        | 126.1      | BF16      \n",
      "GRPO Checkpoint (Merged, bf16)                     | 4737.1     | BF16      \n",
      "GRPO Checkpoint (Merged, fp16)                     | 4737.1     | BF16      \n",
      "GRPO Checkpoint (Merfed, fp32)                     | 4758.9     | F32       \n"
     ]
    }
   ],
   "source": [
    "# Print table header\n",
    "print(f\"{'FILE PATH':<50} | {'SIZE (MB)':<10} | {'PRECISION':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Iterate files to report existence, size and tensor precision\n",
    "for label, path in paths_to_check.items():\n",
    "    # Skip missing files with clear message\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"{label:<50} | FILE NOT FOUND\")\n",
    "        continue\n",
    "\n",
    "    # 1. Check File Size (MB)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "\n",
    "    # 2. Check Precision (Dtype)\n",
    "    # Open safetensors metadata-only (no full tensor load) and read dtype of first tensor\n",
    "    dtype_str = \"Unknown\"\n",
    "    try:\n",
    "        with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            # Inspect the first tensor entry's dtype via a lightweight slice\n",
    "            first_key = list(f.keys())[0]\n",
    "            tensor_slice = f.get_slice(first_key)\n",
    "            dtype_str = str(tensor_slice.get_dtype())\n",
    "    except Exception as e:\n",
    "        # Report errors reading tensor metadata\n",
    "        dtype_str = \"Error reading\"\n",
    "\n",
    "    # Print concise row: label, size and dtype\n",
    "    print(f\"{label:<50} | {size_mb:<10.1f} | {dtype_str:<10}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNVGT/nqhYHFPN/t4HZz9O4",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
