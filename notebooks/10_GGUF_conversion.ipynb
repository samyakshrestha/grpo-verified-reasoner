{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5Ufa_GykVef"
   },
   "source": [
    "# Step 0: Mounting Google Drive and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12064,
     "status": "ok",
     "timestamp": 1768187911101,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "YlZtp7Ve8wo8",
    "outputId": "5528f0a0-37e8-4d32-d5b8-606528ab4ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/grpo-verified-reasoner\n",
      "data\t\t\t      LICENSE\t outputs    unsloth_compiled_cache\n",
      "grpo_trainer_lora_model       models\t README.md  _unsloth_sentencepiece_temp\n",
      "huggingface_tokenizers_cache  notebooks  src\t    wandb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd /content/drive/MyDrive/grpo-verified-reasoner\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqxrMCBbLKWL"
   },
   "outputs": [],
   "source": [
    "!pip install -q unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1768183715172,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "MzbEM3xW54z5"
   },
   "outputs": [],
   "source": [
    "INPUT_MODEL_PATH = \"/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-merged-f32-final\"\n",
    "OUTPUT_GGUF_DIR = \"/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-f32-gguf\"\n",
    "os.makedirs(OUTPUT_GGUF_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43522,
     "status": "ok",
     "timestamp": 1768188106183,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "BCxUf-OtLH0c",
    "outputId": "e65f9fd8-715b-4fa5-fc4d-a87e54afe258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from google.colab import userdata\n",
    "from unsloth import FastLanguageModel\n",
    "from huggingface_hub import HfApi, create_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gl6qnQc7qvl"
   },
   "source": [
    "# Step 1: Setting up Environment on Local Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1768183716599,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "7V99St0l6HMB",
    "outputId": "ed55b860-519c-4e89-b6ae-1dfe5f37ecf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1768183740963,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "zNj58h1h7u1w"
   },
   "outputs": [],
   "source": [
    "!rm -rf llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11664,
     "status": "ok",
     "timestamp": 1768183753477,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "tY6rVzDA7vND",
    "outputId": "524490b0-1a6a-4d12-a227-37135c9c7bf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 75673, done.\u001b[K\n",
      "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
      "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
      "remote: Total 75673 (delta 70), reused 26 (delta 26), pack-reused 75550 (from 3)\u001b[K\n",
      "Receiving objects: 100% (75673/75673), 277.36 MiB | 43.51 MiB/s, done.\n",
      "Resolving deltas: 100% (54911/54911), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768183759919,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "6JtzgKpk7xGp",
    "outputId": "73bee56e-ce85-4f3c-a7fc-50088e7745c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 61622,
     "status": "ok",
     "timestamp": 1768183694793,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "l8j5T9Ly70Oy",
    "outputId": "02b2434c-fe34-4459-c91a-524880150b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\n",
      "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
      "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
      "Collecting numpy~=1.26.4 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 1))\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.57.1 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (4.57.3)\n",
      "Collecting gguf>=0.1.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 6))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 7))\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting torch~=2.6.0 (from -r ./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp312-cp312-linux_x86_64.whl.metadata (26 kB)\n",
      "Collecting aiohttp~=3.9.3 (from -r ./requirements/requirements-tool_bench.txt (line 1))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/aiohttp-3.9.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytest~=8.3.3 (from -r ./requirements/requirements-tool_bench.txt (line 2))\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 3)) (0.36.0)\n",
      "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 4)) (3.10.0)\n",
      "Collecting openai~=1.55.3 (from -r ./requirements/requirements-tool_bench.txt (line 6))\n",
      "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pandas~=2.2.3 (from -r ./requirements/requirements-tool_bench.txt (line 7))\n",
      "  Downloading https://download.pytorch.org/whl/nightly/pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m156.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting prometheus-client~=0.20.0 (from -r ./requirements/requirements-tool_bench.txt (line 8))\n",
      "  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 9)) (2.32.4)\n",
      "Collecting wget~=3.2 (from -r ./requirements/requirements-tool_bench.txt (line 10))\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting typer~=0.15.1 (from -r ./requirements/requirements-tool_bench.txt (line 11))\n",
      "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: seaborn~=0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 12)) (0.13.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (75.2.0)\n",
      "Collecting sympy==1.13.1 (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (6.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.12/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0,>=0.34.0->-r ./requirements/requirements-tool_bench.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2025.11.12)\n",
      "Collecting click<8.2,>=8.0.0 (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (13.9.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (2.19.2)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp312-cp312-linux_x86_64.whl (178.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m178.6/178.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=6f565536d6f1e48e3427fd928587126349c043ff031f01c15152b1a50430c4bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, sympy, pytest, protobuf, prometheus-client, numpy, click, torch, pandas, gguf, aiohttp, typer, openai\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 8.4.2\n",
      "    Uninstalling pytest-8.4.2:\n",
      "      Successfully uninstalled pytest-8.4.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "  Attempting uninstall: prometheus-client\n",
      "    Found existing installation: prometheus_client 0.23.1\n",
      "    Uninstalling prometheus_client-0.23.1:\n",
      "      Successfully uninstalled prometheus_client-0.23.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.3.1\n",
      "    Uninstalling click-8.3.1:\n",
      "      Successfully uninstalled click-8.3.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cpu\n",
      "    Uninstalling torch-2.9.0+cpu:\n",
      "      Successfully uninstalled torch-2.9.0+cpu\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.13.2\n",
      "    Uninstalling aiohttp-3.13.2:\n",
      "      Successfully uninstalled aiohttp-3.13.2\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.20.0\n",
      "    Uninstalling typer-0.20.0:\n",
      "      Successfully uninstalled typer-0.20.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 2.12.0\n",
      "    Uninstalling openai-2.12.0:\n",
      "      Successfully uninstalled openai-2.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "torchvision 0.24.0+cpu requires torch==2.9.0, but you have torch 2.6.0+cpu which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "torchaudio 2.9.0+cpu requires torch==2.9.0, but you have torch 2.6.0+cpu which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.9.5 click-8.1.8 gguf-0.17.1 numpy-1.26.4 openai-1.55.3 pandas-2.2.3 prometheus-client-0.20.0 protobuf-4.25.8 pytest-8.3.5 sympy-1.13.1 torch-2.6.0+cpu typer-0.15.4 wget-3.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "7d517cb991ff4d2f8121a3f98a444483",
       "pip_warning": {
        "packages": [
         "google",
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bFffDjG73QA"
   },
   "source": [
    "# Step 2: Compiling llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2327,
     "status": "ok",
     "timestamp": 1768185643952,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "vpsZapGaEZei",
    "outputId": "31ad6916-c2db-4b0b-ade0-90819e8d336d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "-- The ASM compiler identification is GNU\n",
      "-- Found assembler: /usr/bin/cc\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- ggml version: 0.9.5\n",
      "-- ggml commit:  0c3b7a9ef\n",
      "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
      "-- Generating embedded license file for target: common\n",
      "-- Configuring done (1.7s)\n",
      "-- Generating done (0.3s)\n",
      "-- Build files have been written to: /content/llama.cpp/build\n"
     ]
    }
   ],
   "source": [
    "# Configure with CMake (Disable CUDA if you are CPU-only, otherwise keep ON)\n",
    "!cmake -B build -DGGML_CUDA=OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167031,
     "status": "ok",
     "timestamp": 1768185828985,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "_lWXz0bg73BD",
    "outputId": "dc09bb56-6184-4ed8-b756-fc884596a1d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[  3%] Built target build_info\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
      "[  3%] Built target sha1\n",
      "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[  4%] Built target sha256\n",
      "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
      "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
      "[  5%] Built target llama-llava-cli\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
      "[  5%] Built target llama-minicpmv-cli\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  5%] Built target llama-gemma3-cli\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
      "[  5%] Built target llama-qwen2vl-cli\n",
      "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  6%] Built target xxhash\n",
      "[  6%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  6%] Built target ggml-base\n",
      "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[ 10%] Built target ggml-cpu\n",
      "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "[ 11%] Built target ggml\n",
      "[ 12%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
      "[ 13%] Built target llama-gguf\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
      "[ 14%] Built target llama-gguf-hash\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
      "[ 15%] Built target cpp-httplib\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "[ 44%] Built target llama\n",
      "[ 45%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
      "[ 47%] Built target test-c\n",
      "[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
      "[ 47%] Built target llama-simple\n",
      "[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
      "[ 47%] Built target llama-simple-chat\n",
      "[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/mobilenetv5.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/youtuvl.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
      "[ 55%] Built target mtmd\n",
      "[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/__/license.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 56%] Built target common\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
      "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "[ 58%] Built target test-sampling\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 58%] Built target test-tokenizer-0\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 60%] Built target test-grammar-parser\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
      "[ 61%] Built target test-llama-grammar\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
      "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
      "[ 62%] Built target test-tokenizer-1-bpe\n",
      "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 62%] Built target test-gbnf-validator\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n",
      "[ 63%] Built target test-tokenizer-1-spm\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
      "[ 64%] Built target test-quantize-stats\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
      "[ 64%] Built target test-chat-template\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
      "[ 65%] Built target test-log\n",
      "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
      "[ 66%] Built target test-json-partial\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
      "[ 66%] Built target test-grammar-integration\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
      "[ 66%] Built target test-json-schema-to-grammar\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
      "[ 66%] Built target test-chat-parser\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
      "[ 67%] Built target test-regex-partial\n",
      "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
      "[ 69%] Built target test-thread-safety\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
      "[ 69%] Built target test-arg-parser\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
      "[ 71%] Built target test-model-load-cancel\n",
      "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
      "[ 72%] Built target test-opt\n",
      "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/test-backend-sampler.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
      "[ 72%] Built target test-autorelease\n",
      "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/get-model.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
      "[ 73%] Built target test-gguf\n",
      "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
      "[ 73%] Built target test-barrier\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "[ 74%] Built target test-quantize-fns\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "[ 75%] Built target test-rope\n",
      "[ 76%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
      "[ 76%] Built target test-state-restore-fragmented\n",
      "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
      "[ 76%] Built target test-mtmd-c-api\n",
      "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-sampler\u001b[0m\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "[ 78%] Built target test-quantize-perf\n",
      "[ 78%] \u001b[32mBuilding CXX object examples/debug/CMakeFiles/llama-debug.dir/debug.cpp.o\u001b[0m\n",
      "[ 78%] Built target test-backend-sampler\n",
      "[ 78%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
      "[ 78%] Built target test-chat-peg-parser\n",
      "[ 78%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
      "[ 79%] Built target test-alloc\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
      "[ 81%] Built target test-peg-parser\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
      "[ 81%] Built target llama-batched\n",
      "[ 82%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
      "[ 83%] Built target llama-idle\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
      "[ 83%] Built target llama-eval-callback\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
      "[ 84%] Built target llama-embedding\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
      "[ 84%] Built target llama-lookup-merge\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
      "[ 85%] Built target llama-lookup\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
      "[ 85%] Built target llama-lookahead\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
      "[ 86%] Built target llama-lookup-create\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
      "[ 86%] Built target llama-lookup-stats\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
      "[ 86%] Built target llama-parallel\n",
      "[ 87%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
      "[ 87%] Built target llama-passkey\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
      "[ 88%] Built target llama-save-load-state\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
      "[ 88%] Built target llama-retrieval\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-debug\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
      "[ 88%] Built target llama-finetune\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
      "[ 88%] Built target llama-speculative-simple\n",
      "[ 88%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
      "[ 88%] Built target llama-debug\n",
      "[ 88%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "[ 88%] Built target llama-gen-docs\n",
      "[ 88%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "[ 88%] Built target llama-speculative\n",
      "[ 88%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
      "[ 88%] Built target llama-q8dot\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
      "[ 88%] Built target llama-vdot\n",
      "[ 88%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
      "[ 88%] Built target llama-gguf-split\n",
      "[ 88%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
      "[ 89%] Built target llama-convert-llama2c-to-ggml\n",
      "[ 90%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "[ 90%] Built target llama-batched-bench\n",
      "[ 91%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[ 91%] Built target llama-diffusion-cli\n",
      "[ 91%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "[ 91%] Built target llama-quantize\n",
      "[ 91%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
      "[ 92%] Built target llama-completion\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
      "[ 92%] Built target llama-tokenize\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
      "[ 92%] Built target llama-perplexity\n",
      "[ 93%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
      "[ 93%] Built target llama-mtmd-cli\n",
      "[ 93%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
      "[ 93%] Built target llama-imatrix\n",
      "[ 93%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
      "[ 94%] Built target llama-cvector-generator\n",
      "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "[ 95%] Built target llama-bench\n",
      "[ 95%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
      "[ 97%] Built target test-chat\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
      "[ 97%] Built target test-backend-ops\n",
      "[ 97%] Built target llama-fit-params\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
      "[ 97%] Built target llama-export-lora\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
      "[ 98%] Built target llama-tts\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\n",
      "[ 98%] Built target server-context\n",
      "[ 98%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
      "[ 99%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
      "[ 99%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\n",
      "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n",
      "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
      "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
      "[100%] Built target llama-cli\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
      "[100%] Built target llama-server\n"
     ]
    }
   ],
   "source": [
    "# Build the binaries\n",
    "!cmake --build build --config Release -j 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np9kFkRh79rS"
   },
   "source": [
    "# Step 3: Converting to Uncompressed GGUF (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561195,
     "status": "ok",
     "timestamp": 1768186406986,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "d0OsYZ5f770A",
    "outputId": "56cc3cdc-ac2f-45bd-916c-c4bb413f3664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: qwen3-4b-grpo-merged-f32-final\n",
      "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:heuristics unable to detect tensor dtype, defaulting to --outtype f16\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.float32 --> F16, shape = {2560, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.34.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.34.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.35.attn_k_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.35.attn_q_norm.weight, torch.float32 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2560\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 9728\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "The tokenizer you are loading from '/content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-merged-f32-final' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151643\n",
      "INFO:gguf.vocab:Setting special token type pad to 151654\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\n",
      "' + message['content'] + '<|im_end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% else %}{{ '<|im_start|>system\n",
      "' + message['content'] + '<|im_end|>\n",
      "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/temp_f16.gguf: n_tensors = 398, total_size = 8.0G\n",
      "Writing: 100% 8.05G/8.05G [08:59<00:00, 14.9Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/temp_f16.gguf\n"
     ]
    }
   ],
   "source": [
    "!python convert_hf_to_gguf.py {INPUT_MODEL_PATH} --outfile /content/temp_f16.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyvfNE-X8CqP"
   },
   "source": [
    "# Step 4: Quantizing to Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86245,
     "status": "ok",
     "timestamp": 1768186650765,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "V-I17tDG8AYn",
    "outputId": "b98013da-0983-4760-ebe6-8fa5b51e31ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 7708 (0c3b7a9ef)\n",
      "main: built with GNU 11.4.0 for Linux x86_64\n",
      "main: quantizing '/content/temp_f16.gguf' to '/content/qwen_q4_k_m.gguf' as Q4_K_M\n",
      "llama_model_loader: direct I/O is enabled, disabling mmap\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 398 tensors from /content/temp_f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen3 4b Grpo Merged F32 Final\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = grpo-merged-final\n",
      "llama_model_loader: - kv   4:                           general.basename str              = qwen3\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 4B\n",
      "llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\n",
      "llama_model_loader: - kv   7:                       qwen3.context_length u32              = 32768\n",
      "llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 9728\n",
      "llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - type  f32:  145 tensors\n",
      "llama_model_loader: - type  f16:  253 tensors\n",
      "[   1/ 398]                   output_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[   2/ 398]                    token_embd.weight - [ 2560, 151936,     1,     1], type =    f16, converting to q6_K .. size =   741.88 MiB ->   304.28 MiB\n",
      "[   3/ 398]                  blk.0.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[   4/ 398]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[   5/ 398]               blk.0.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[   6/ 398]             blk.0.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[   7/ 398]                  blk.0.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[   8/ 398]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[   9/ 398]                  blk.0.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[  10/ 398]                blk.0.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[  11/ 398]                blk.0.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  12/ 398]                blk.0.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  13/ 398]                  blk.0.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  14/ 398]                  blk.1.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  15/ 398]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  16/ 398]               blk.1.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  17/ 398]             blk.1.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  18/ 398]                  blk.1.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  19/ 398]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  20/ 398]                  blk.1.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[  21/ 398]                blk.1.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[  22/ 398]                blk.1.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  23/ 398]                blk.1.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  24/ 398]                  blk.1.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  25/ 398]                  blk.2.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  26/ 398]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  27/ 398]               blk.2.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  28/ 398]             blk.2.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  29/ 398]                  blk.2.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  30/ 398]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  31/ 398]                  blk.2.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[  32/ 398]                blk.2.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[  33/ 398]                blk.2.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  34/ 398]                blk.2.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  35/ 398]                  blk.2.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  36/ 398]                  blk.3.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  37/ 398]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  38/ 398]               blk.3.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  39/ 398]             blk.3.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  40/ 398]                  blk.3.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  41/ 398]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  42/ 398]                  blk.3.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[  43/ 398]                blk.3.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[  44/ 398]                blk.3.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  45/ 398]                blk.3.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  46/ 398]                  blk.3.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  47/ 398]                  blk.4.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  48/ 398]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  49/ 398]               blk.4.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  50/ 398]             blk.4.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  51/ 398]                  blk.4.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  52/ 398]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  53/ 398]                  blk.4.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  54/ 398]                blk.4.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  55/ 398]                blk.4.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  56/ 398]                blk.4.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  57/ 398]                  blk.4.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  58/ 398]                  blk.5.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  59/ 398]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  60/ 398]               blk.5.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  61/ 398]             blk.5.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  62/ 398]                  blk.5.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  63/ 398]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  64/ 398]                  blk.5.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  65/ 398]                blk.5.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  66/ 398]                blk.5.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  67/ 398]                blk.5.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  68/ 398]                  blk.5.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  69/ 398]                  blk.6.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  70/ 398]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  71/ 398]               blk.6.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  72/ 398]             blk.6.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  73/ 398]                  blk.6.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  74/ 398]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  75/ 398]                  blk.6.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[  76/ 398]                blk.6.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[  77/ 398]                blk.6.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  78/ 398]                blk.6.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  79/ 398]                  blk.6.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  80/ 398]                  blk.7.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  81/ 398]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  82/ 398]               blk.7.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  83/ 398]             blk.7.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  84/ 398]                  blk.7.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  85/ 398]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  86/ 398]                  blk.7.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  87/ 398]                blk.7.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  88/ 398]                blk.7.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  89/ 398]                blk.7.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  90/ 398]                  blk.7.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  91/ 398]                  blk.8.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  92/ 398]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  93/ 398]               blk.8.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[  94/ 398]             blk.8.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  95/ 398]                  blk.8.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[  96/ 398]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  97/ 398]                  blk.8.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[  98/ 398]                blk.8.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[  99/ 398]                blk.8.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 100/ 398]                blk.8.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 101/ 398]                  blk.8.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 102/ 398]                  blk.9.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 103/ 398]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 104/ 398]               blk.9.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 105/ 398]             blk.9.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 106/ 398]                  blk.9.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 107/ 398]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 108/ 398]                  blk.9.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 109/ 398]                blk.9.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 110/ 398]                blk.9.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 111/ 398]                blk.9.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 112/ 398]                  blk.9.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 113/ 398]                 blk.10.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 114/ 398]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 115/ 398]              blk.10.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 116/ 398]            blk.10.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 117/ 398]                 blk.10.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 118/ 398]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 119/ 398]                 blk.10.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 120/ 398]               blk.10.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 121/ 398]               blk.10.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 122/ 398]               blk.10.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 123/ 398]                 blk.10.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 124/ 398]                 blk.11.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 125/ 398]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 126/ 398]              blk.11.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 127/ 398]            blk.11.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 128/ 398]                 blk.11.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 129/ 398]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 130/ 398]                 blk.11.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 131/ 398]               blk.11.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 132/ 398]               blk.11.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 133/ 398]               blk.11.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 134/ 398]                 blk.11.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 135/ 398]                 blk.12.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 136/ 398]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 137/ 398]              blk.12.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 138/ 398]            blk.12.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 139/ 398]                 blk.12.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 140/ 398]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 141/ 398]                 blk.12.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 142/ 398]               blk.12.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 143/ 398]               blk.12.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 144/ 398]               blk.12.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 145/ 398]                 blk.12.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 146/ 398]                 blk.13.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 147/ 398]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 148/ 398]              blk.13.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 149/ 398]            blk.13.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 150/ 398]                 blk.13.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 151/ 398]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 152/ 398]                 blk.13.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 153/ 398]               blk.13.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 154/ 398]               blk.13.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 155/ 398]               blk.13.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 156/ 398]                 blk.13.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 157/ 398]                 blk.14.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 158/ 398]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 159/ 398]              blk.14.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 160/ 398]            blk.14.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 161/ 398]                 blk.14.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 162/ 398]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 163/ 398]                 blk.14.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 164/ 398]               blk.14.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 165/ 398]               blk.14.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 166/ 398]               blk.14.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 167/ 398]                 blk.14.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 168/ 398]                 blk.15.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 169/ 398]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 170/ 398]              blk.15.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 171/ 398]            blk.15.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 172/ 398]                 blk.15.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 173/ 398]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 174/ 398]                 blk.15.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 175/ 398]               blk.15.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 176/ 398]               blk.15.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 177/ 398]               blk.15.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 178/ 398]                 blk.15.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 179/ 398]                 blk.16.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 180/ 398]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 181/ 398]              blk.16.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 182/ 398]            blk.16.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 183/ 398]                 blk.16.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 184/ 398]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 185/ 398]                 blk.16.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 186/ 398]               blk.16.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 187/ 398]               blk.16.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 188/ 398]               blk.16.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 189/ 398]                 blk.16.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 190/ 398]                 blk.17.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 191/ 398]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 192/ 398]              blk.17.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 193/ 398]            blk.17.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 194/ 398]                 blk.17.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 195/ 398]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 196/ 398]                 blk.17.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 197/ 398]               blk.17.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 198/ 398]               blk.17.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 199/ 398]               blk.17.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 200/ 398]                 blk.17.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 201/ 398]                 blk.18.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 202/ 398]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 203/ 398]              blk.18.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 204/ 398]            blk.18.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 205/ 398]                 blk.18.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 206/ 398]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 207/ 398]                 blk.18.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 208/ 398]               blk.18.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 209/ 398]               blk.18.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 210/ 398]               blk.18.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 211/ 398]                 blk.18.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 212/ 398]                 blk.19.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 213/ 398]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 214/ 398]              blk.19.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 215/ 398]            blk.19.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 216/ 398]                 blk.19.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 217/ 398]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 218/ 398]                 blk.19.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 219/ 398]               blk.19.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 220/ 398]               blk.19.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 221/ 398]               blk.19.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 222/ 398]                 blk.19.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 223/ 398]                 blk.20.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 224/ 398]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 225/ 398]              blk.20.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 226/ 398]            blk.20.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 227/ 398]                 blk.20.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 228/ 398]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 229/ 398]                 blk.20.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 230/ 398]               blk.20.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 231/ 398]               blk.20.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 232/ 398]               blk.20.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 233/ 398]                 blk.20.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 234/ 398]                 blk.21.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 235/ 398]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 236/ 398]              blk.21.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 237/ 398]            blk.21.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 238/ 398]                 blk.21.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 239/ 398]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 240/ 398]                 blk.21.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 241/ 398]               blk.21.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 242/ 398]               blk.21.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 243/ 398]               blk.21.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 244/ 398]                 blk.21.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 245/ 398]                 blk.22.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 246/ 398]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 247/ 398]              blk.22.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 248/ 398]            blk.22.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 249/ 398]                 blk.22.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 250/ 398]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 251/ 398]                 blk.22.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 252/ 398]               blk.22.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 253/ 398]               blk.22.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 254/ 398]               blk.22.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 255/ 398]                 blk.22.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 256/ 398]                 blk.23.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 257/ 398]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 258/ 398]              blk.23.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 259/ 398]            blk.23.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 260/ 398]                 blk.23.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 261/ 398]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 262/ 398]                 blk.23.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 263/ 398]               blk.23.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 264/ 398]               blk.23.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 265/ 398]               blk.23.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 266/ 398]                 blk.23.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 267/ 398]                 blk.24.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 268/ 398]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 269/ 398]              blk.24.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 270/ 398]            blk.24.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 271/ 398]                 blk.24.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 272/ 398]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 273/ 398]                 blk.24.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 274/ 398]               blk.24.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 275/ 398]               blk.24.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 276/ 398]               blk.24.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 277/ 398]                 blk.24.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 278/ 398]                 blk.25.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 279/ 398]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 280/ 398]              blk.25.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 281/ 398]            blk.25.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 282/ 398]                 blk.25.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 283/ 398]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 284/ 398]                 blk.25.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 285/ 398]               blk.25.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 286/ 398]               blk.25.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 287/ 398]               blk.25.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 288/ 398]                 blk.25.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 289/ 398]                 blk.26.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 290/ 398]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 291/ 398]              blk.26.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 292/ 398]            blk.26.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 293/ 398]                 blk.26.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 294/ 398]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 295/ 398]                 blk.26.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 296/ 398]               blk.26.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 297/ 398]               blk.26.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 298/ 398]               blk.26.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 299/ 398]                 blk.26.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 300/ 398]                 blk.27.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 301/ 398]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 302/ 398]              blk.27.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 303/ 398]            blk.27.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 304/ 398]                 blk.27.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 305/ 398]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 306/ 398]                 blk.27.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 307/ 398]               blk.27.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 308/ 398]               blk.27.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 309/ 398]               blk.27.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 310/ 398]                 blk.27.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 311/ 398]                 blk.28.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 312/ 398]            blk.28.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 313/ 398]              blk.28.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 314/ 398]            blk.28.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 315/ 398]                 blk.28.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 316/ 398]            blk.28.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 317/ 398]                 blk.28.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 318/ 398]               blk.28.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 319/ 398]               blk.28.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 320/ 398]               blk.28.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 321/ 398]                 blk.28.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 322/ 398]                 blk.29.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 323/ 398]            blk.29.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 324/ 398]              blk.29.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 325/ 398]            blk.29.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 326/ 398]                 blk.29.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 327/ 398]            blk.29.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 328/ 398]                 blk.29.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 329/ 398]               blk.29.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 330/ 398]               blk.29.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 331/ 398]               blk.29.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 332/ 398]                 blk.29.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 333/ 398]                 blk.30.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 334/ 398]            blk.30.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 335/ 398]              blk.30.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 336/ 398]            blk.30.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 337/ 398]                 blk.30.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 338/ 398]            blk.30.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 339/ 398]                 blk.30.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 340/ 398]               blk.30.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 341/ 398]               blk.30.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 342/ 398]               blk.30.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 343/ 398]                 blk.30.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 344/ 398]                 blk.31.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 345/ 398]            blk.31.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 346/ 398]              blk.31.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 347/ 398]            blk.31.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 348/ 398]                 blk.31.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 349/ 398]            blk.31.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 350/ 398]                 blk.31.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 351/ 398]               blk.31.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 352/ 398]               blk.31.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 353/ 398]               blk.31.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 354/ 398]                 blk.31.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 355/ 398]                 blk.32.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 356/ 398]            blk.32.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 357/ 398]              blk.32.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 358/ 398]            blk.32.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 359/ 398]                 blk.32.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 360/ 398]            blk.32.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 361/ 398]                 blk.32.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 362/ 398]               blk.32.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 363/ 398]               blk.32.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 364/ 398]               blk.32.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 365/ 398]                 blk.32.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 366/ 398]                 blk.33.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 367/ 398]            blk.33.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 368/ 398]              blk.33.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 369/ 398]            blk.33.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 370/ 398]                 blk.33.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 371/ 398]            blk.33.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 372/ 398]                 blk.33.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 373/ 398]               blk.33.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 374/ 398]               blk.33.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 375/ 398]               blk.33.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 376/ 398]                 blk.33.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 377/ 398]                 blk.34.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 378/ 398]            blk.34.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 379/ 398]              blk.34.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 380/ 398]            blk.34.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 381/ 398]                 blk.34.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 382/ 398]            blk.34.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 383/ 398]                 blk.34.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 384/ 398]               blk.34.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 385/ 398]               blk.34.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 386/ 398]               blk.34.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 387/ 398]                 blk.34.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 388/ 398]                 blk.35.attn_k.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q4_K .. size =     5.00 MiB ->     1.41 MiB\n",
      "[ 389/ 398]            blk.35.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 390/ 398]              blk.35.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 391/ 398]            blk.35.attn_output.weight - [ 4096,  2560,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 392/ 398]                 blk.35.attn_q.weight - [ 2560,  4096,     1,     1], type =    f16, converting to q4_K .. size =    20.00 MiB ->     5.62 MiB\n",
      "[ 393/ 398]            blk.35.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 394/ 398]                 blk.35.attn_v.weight - [ 2560,  1024,     1,     1], type =    f16, converting to q6_K .. size =     5.00 MiB ->     2.05 MiB\n",
      "[ 395/ 398]               blk.35.ffn_down.weight - [ 9728,  2560,     1,     1], type =    f16, converting to q6_K .. size =    47.50 MiB ->    19.48 MiB\n",
      "[ 396/ 398]               blk.35.ffn_gate.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "[ 397/ 398]               blk.35.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
      "[ 398/ 398]                 blk.35.ffn_up.weight - [ 2560,  9728,     1,     1], type =    f16, converting to q4_K .. size =    47.50 MiB ->    13.36 MiB\n",
      "llama_model_quantize_impl: model size  =  7672.62 MiB\n",
      "llama_model_quantize_impl: quant size  =  2375.91 MiB\n",
      "\n",
      "main: quantize time = 86065.22 ms\n",
      "main:    total time = 86065.22 ms\n"
     ]
    }
   ],
   "source": [
    "!./build/bin/llama-quantize /content/temp_f16.gguf /content/qwen_q4_k_m.gguf q4_k_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM06C7WD8Gen"
   },
   "source": [
    "# Step 5: Moving Final File to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 21312,
     "status": "ok",
     "timestamp": 1768186709278,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "BGI8YQAD8HDQ"
   },
   "outputs": [],
   "source": [
    "!cp /content/qwen_q4_k_m.gguf {OUTPUT_GGUF_DIR}/qwen_q4_k_m.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1768186710675,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "epwcyRQO8KrI",
    "outputId": "e762bd5f-2700-4b82-b46a-51dfe0d0941d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> DONE. The GGUF file is at: /content/drive/MyDrive/grpo-verified-reasoner/models/qwen3-4b-grpo-f32-gguf/qwen_q4_k_m.gguf\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n>>> DONE. The GGUF file is at: {OUTPUT_GGUF_DIR}/qwen_q4_k_m.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxIGrGSCKlXk"
   },
   "source": [
    "# Step 6: Pushing to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1768188159141,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "utOTzCzvJAnM"
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1768188888995,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "Nogh_yxrMlAq"
   },
   "outputs": [],
   "source": [
    "local_model_path = \"models/qwen3-4b-grpo-merged-f32-final\"\n",
    "local_gguf_path = \"models/qwen3-4b-grpo-f32-gguf/qwen_q4_k_m.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151,
     "referenced_widgets": [
      "113be24f44b946a7a5a2ab9fbe2a128c",
      "4c8fd875b5ff4faebd19acb871cd98fc",
      "866b4c8ca2914b28b49ffb2080ce0554",
      "128260275dac4681adb6bd97e5668614",
      "c728c8072fb54ef9986e1e92587e3975",
      "29c458de606b40bb93f6bbf45400c5e9",
      "d2db0ee2e62d4a3886546ace632b971c",
      "4bdcf8c3f1224b0aae169d777824f2cb",
      "8446a41556994b738d7da75e51bffc3d",
      "c12e4c7b6c204b7aaf0dd2e7d87ee03d",
      "f4c88de39ca74dcf837d9e90b4affa83"
     ]
    },
    "executionInfo": {
     "elapsed": 311686,
     "status": "ok",
     "timestamp": 1768188478293,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "veSJloYAMnO3",
    "outputId": "70c65a46-5a39-423f-e10f-69525e02b079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113be24f44b946a7a5a2ab9fbe2a128c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    local_model_path,\n",
    "    dtype=torch.float16, # Downcast F32 -> F16 for bandwidth/storage efficiency\n",
    "    load_in_4bit=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "a50e3414d76b42e0a05bb81e4ad95249",
      "62ee7428f6894306a41d56f429e607d5",
      "7e7572e4711c44e9b906bd72a8e68281",
      "9b29e74418e546e190352662a2830329",
      "405c42b98b1347f0b9802b2b6f0ef035",
      "9b870e49ad64418299434d85d03fff92",
      "80b16be30c2c413abe7448343b2b7295",
      "e945843b5cd941c981eb355bf194f144",
      "3883d95ca8b248bf937c88ea0b5f1854",
      "803a93608b4f4044b5909517d5d3446c",
      "69ee0f7247ef44199c2e93e99bece713",
      "1ce241e8ccf94a2d8a99f35da4d1e1c8",
      "c6a34318293d4894969a9bc4401a9a3c",
      "21d10a51b01f48b1b8205eed1a463fb7",
      "78daea1ccfce45178acb81c2fec966c8",
      "148e110c3d3c489aa1a68a622b74bf1d",
      "7fcd5fc08f5448868c7e6cde5a4a390f",
      "7549d32d72c3404eb74da69641849129",
      "5745f4ff2f114e3099ab22b2754f4ded",
      "a85f136c6de847349a98193080203945",
      "a9a754aac9364fef978c6386f21b47ff",
      "2dfbc84d223a46e6b590fea21d86d68f",
      "7ba00a35b5ce4916a8554590e2a666c4",
      "450a9e09982d405f85326c105887adeb",
      "1994cb16cb754b86af64eb3791932d5d",
      "2dbb3578cb4e4a73b7dd58c8ddc1ff35",
      "acfb91b901ee4a0cb74c60ff898e38e6",
      "3add03340c8e417fb4f514466b63bd59",
      "cde7db133f554d8496b2e8cf1100c3f6",
      "872e2caa52a84d14aa1cb5f4d1986e88",
      "ee276512ba4c49b3a90f79eb51f9e294",
      "13b1cf1da8ae43f4b764845dceaaa874",
      "f106012e058d4dbe91e14d4915e06924",
      "0f241c7ebe0d45338a2062267318c599",
      "80b86b5222c24643a31b3fe971a0c0a4",
      "1d6d5971b45b4d9bae91fb8959b98e1e",
      "ebcc80df18574f478e6eb90f3f9d0602",
      "1da12a4e094c4172bf9dd914281d34ad",
      "c0ad22752559486abc9d14ecb44dc4ad",
      "62d305cb3e834b73847c3d499cd6118f",
      "8dc353835aaf4142b7cab4bb8f02d653",
      "3eabc46e2bad4285b901f8631b861d90",
      "8c2bf5a47327412e88bce89f109225e6",
      "f5ee38dd330b4993b93dc88434697918",
      "4877810b8c244462b3320467d00161af",
      "5a8a6ed6796842988a519f36fffa7dfc",
      "c9bf379875264ae480335b417aea345f",
      "b626a3e0702d48088717c382cdd762de",
      "74822aa8cc2849428426ae0e710c8bd2",
      "dbd5d4d06b7c4ea6bf5047b9208b0bdb",
      "948f6543c558433e9a104040fab84e68",
      "78f2bc95a1994def9dffc160c8ae33a9",
      "d4dcf42e44a048e6bd3136f7985ef7bb",
      "8386988a08ac473f92dd18d857989245",
      "aef84157d197491b9107c7bffd2c8475",
      "a643d6f10dcc4a81b7466c8f35438e1b",
      "fb6c76aef70c4ec6b4651163877e6c52",
      "6adbf1a0a3534cb39d99350bfcd8c320",
      "2562748a37bf4fdfb67f0c6af8ec1ed4",
      "8b3e3938a7054c35bb8a5e4a8c80f4fd",
      "1ad8a15ccf24438cb17c5b116f2b6a58",
      "a74ae650c9e74386973f911301d608f8",
      "f4511fb2ecf9464382f849fd1a3b8b76",
      "d766cf32ae964e31afc75ada52651816",
      "9d253fc92b2f4c51af30df55b9da7f27",
      "2c9a24ac0b634e9d94e8c488c508f686",
      "145ccd2d162441ea9008fac9ca91b0b5",
      "6bdff70a3657415c96f02c6e98d0a744",
      "07b3cc2cc219495e82e2dbccc65c0b44",
      "95e199bfaeec429094cf32fff39ce572",
      "909c774d9fb04f39910c31f7d3c9961c",
      "d6f3dd2ea1724481b4faa575ad09ffb6",
      "81d291a74adf4df9a0b4e213b7a0af1e",
      "f57fd281fce5419a9ef28f8aefe7bbe9",
      "860c014b8f114d2eab5d97d9f3aa5003",
      "49dc745abfdc44da985c5564c53e1455",
      "9c0eedea9e4d44e995c0c3eb9ef9f443",
      "50ed366bf9fc4f1ab93cc73149e25e76",
      "4a46ae7f7df94a4cb08f6a7b10fd25a4",
      "55b1358b27724303b1d6076e1342a187",
      "6fe2027d35b94efe9d229abe21821e55",
      "ba68aeb81ed74572a6f097a027b4e4bd",
      "db5fe307589447288998f080c1366500",
      "68ad2867795f46ff88e6801ed320df98",
      "b47bfab6754d4faea2489617e3f48e62",
      "4f72945624274c078322988faf5049c3",
      "334c97ed18674ac38499dca56cdaa546",
      "436ffb14340b47b3be5d0b7c5b8df3c3"
     ]
    },
    "executionInfo": {
     "elapsed": 132806,
     "status": "ok",
     "timestamp": 1768188627032,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "v-5sZFVvM8Nl",
    "outputId": "75788b56-a2b1-4647-946d-550fbb4eed8b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50e3414d76b42e0a05bb81e4ad95249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce241e8ccf94a2d8a99f35da4d1e1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba00a35b5ce4916a8554590e2a666c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f241c7ebe0d45338a2062267318c599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0002-of-00002.safetensors:   0%|          |  524kB / 3.08GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4877810b8c244462b3320467d00161af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...0001-of-00002.safetensors:   1%|          | 37.6MB / 4.97GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/samyakshrestha/qwen3-4b-grpo-fp16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a643d6f10dcc4a81b7466c8f35438e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145ccd2d162441ea9008fac9ca91b0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ed366bf9fc4f1ab93cc73149e25e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...mp34s36wun/tokenizer.json:  98%|#########8| 11.2MB / 11.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "repo_id_main = \"samyakshrestha/qwen3-4b-grpo-fp16\"\n",
    "model.push_to_hub(repo_id_main, token=hf_token)\n",
    "tokenizer.push_to_hub(repo_id_main, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768188761900,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "IkTjVih6NcSq"
   },
   "outputs": [],
   "source": [
    "repo_id_gguf = \"samyakshrestha/qwen3-4b-grpo-gguf\"\n",
    "api = HfApi(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1768188765353,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "_9FYL_XKQ1as",
    "outputId": "fc443648-3b78-45c1-98c9-4791b5a2ce24"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "RepoUrl('https://huggingface.co/samyakshrestha/qwen3-4b-grpo-gguf', endpoint='https://huggingface.co', repo_type='model', repo_id='samyakshrestha/qwen3-4b-grpo-gguf')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_repo(repo_id_gguf, repo_type=\"model\", exist_ok=True, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181,
     "referenced_widgets": [
      "11670d328d3640ba85ebede9d2acb153",
      "4703d164e46d4fc495452e09bc6a2715",
      "e3331a137c21414e9bd69b5f6a0f11f0",
      "ba6ea2c7f4594f8bb8cacfb4db1bbf69",
      "6a4867c926874282a41d282ef6e1cc78",
      "b358217afa6f4a7687cfdf264b936035",
      "d4f2c618a0ba48c790196f02720dae0f",
      "c5b84d17ace442cb92238ad6d5a08f65",
      "a121f73516f040f4b67881e5875d35f1",
      "d7725824ce514965884cf9296ad2d099",
      "b897a9d6bcc04143b639e2d870001f40",
      "5537cfaf6f464d36a290805799df7ec2",
      "4067ad12712940fb98baf90320637ae8",
      "225b7af4bbd346f7bed1dd21a39771f8",
      "745934ded34f464491205f25b15fc9b6",
      "2b1974510d464622b581d5822c0c9552",
      "7d82220563ca43a8b39d4fb26a7c30d7",
      "832238d9f74a4bcabac51e2234426462",
      "85d5ffb1dd614c2aa7d37dac2b2abee6",
      "d7c0f95d6c354c1a9c937608251937a5",
      "b45b7bcc93c74e818768f1b16293fefa",
      "e74f26fa331d43efb1221a6356860a38",
      "2500fcb7af7a4999beb38204a2dd49b8",
      "8877dbb6cae147d9807f068d5c4ec63d",
      "cd551db26d914b90a0cda40df165a9d4",
      "f1ab608834ac46eb89f92a525821ba3d",
      "aa8cd0b24eba46cab365cb75786c9ae9",
      "5c1b10150b6e454ea759a4502cd73ce2",
      "9366a3c59b6244558312976efdf525c2",
      "5d7dfde8980f49a28bedc27947756e2c",
      "2f53b6402b904fbb9ded7c37ba0d995b",
      "9fb9ad77cb644a3fa5102c72cc5a2dd3",
      "d4fc89af5ac745969c9bfc36daece677"
     ]
    },
    "executionInfo": {
     "elapsed": 54637,
     "status": "ok",
     "timestamp": 1768188977276,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "9j55LyGpQ2Gr",
    "outputId": "43556d2d-8346-4889-e65b-a69a4b67efe2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11670d328d3640ba85ebede9d2acb153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5537cfaf6f464d36a290805799df7ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2500fcb7af7a4999beb38204a2dd49b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...f32-gguf/qwen_q4_k_m.gguf:   1%|1         | 25.2MB / 2.50GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/samyakshrestha/qwen3-4b-grpo-gguf/commit/9af575c0f27eed2615912c46398dec8453a5f955', commit_message='Upload qwen3-4b-grpo-q4_k_m.gguf with huggingface_hub', commit_description='', oid='9af575c0f27eed2615912c46398dec8453a5f955', pr_url=None, repo_url=RepoUrl('https://huggingface.co/samyakshrestha/qwen3-4b-grpo-gguf', endpoint='https://huggingface.co', repo_type='model', repo_id='samyakshrestha/qwen3-4b-grpo-gguf'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=local_gguf_path,\n",
    "    path_in_repo=\"qwen3-4b-grpo-q4_k_m.gguf\", # Clean filename for the repo\n",
    "    repo_id=repo_id_gguf,\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64d0Y2sATIQB"
   },
   "source": [
    "# Step 7: Fixing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1284,
     "status": "ok",
     "timestamp": 1768189425931,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "jvorGviVTMKB"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19164,
     "status": "ok",
     "timestamp": 1768189445093,
     "user": {
      "displayName": "Samyak Shrestha",
      "userId": "13083503381857072620"
     },
     "user_tz": 360
    },
    "id": "dCEL6IbsTPf0",
    "outputId": "1a8fd2f1-54f7-40f7-b26c-1d825ab69c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKeMC1neTReT"
   },
   "outputs": [],
   "source": [
    "# List the notebook directory to confirm the file exists\n",
    "os.listdir(\"/content/drive/MyDrive/grpo-verified-reasoner/notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4jEVCHeTR4r"
   },
   "outputs": [],
   "source": [
    "notebook_path = \"/content/drive/MyDrive/grpo-verified-reasoner/notebooks/10_GGUF_conversion.ipynb\"\n",
    "\n",
    "with open(notebook_path, \"r\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "with open(notebook_path, \"w\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Notebook fixed and saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNagUU86QkQ3qit/EK0Zc6e",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
